{"nbformat":4,"nbformat_minor":0,"metadata":{"jupytext":{"split_at_heading":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"colab":{"name":"new_skr_03_data.core.ipynb","provenance":[],"collapsed_sections":["U532_enOVka5"]}},"cells":[{"cell_type":"code","metadata":{"id":"fhZXo5qRVkWP"},"source":["#default_exp data.core"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I_B9Ha6WVrVL","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601217658778,"user_tz":420,"elapsed":26076,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"f80d70d2-5c1f-4653-dfc9-876996cea810"},"source":["# make your Google drive accessible \n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","root_dir = \"/content/gdrive/My Drive/\"\n","base_dir = root_dir + 'fastai2_library/course-v4/'\n","\n","# navigate to the notebooks directory for dl2\n","import os\n","os.chdir(base_dir)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WduHJzeIVuqO","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601217666643,"user_tz":420,"elapsed":4735,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"c81ef098-b4b0-4d4d-bb54-bacb6ea90dbb"},"source":["!pwd\n","# cd to base_dir if above os.chdir does not work using below command\n","# %cd \"/content/gdrive/My Drive/fastai2_library/course-v4/\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/fastai2_library/course-v4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iId4u_5DVv25","colab":{"base_uri":"https://localhost:8080/","height":969},"executionInfo":{"status":"ok","timestamp":1601217680626,"user_tz":420,"elapsed":7896,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"a2d6f559-297d-4c72-dab6-3255d3caf6a4"},"source":["!pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n","!pip install fastai==2.0.9"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Requirement already satisfied: torch==1.6.0+cu101 in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n","Requirement already satisfied: torchvision==0.7.0+cu101 in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0+cu101) (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0+cu101) (1.18.5)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.7.0+cu101) (7.0.0)\n","Collecting fastai==2.0.9\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/c2/e0cad9379292182fe835792070b98d543d591f0e949dcbb620e5ec9a818b/fastai-2.0.9-py3-none-any.whl (354kB)\n","\u001b[K     |████████████████████████████████| 358kB 8.3MB/s \n","\u001b[?25hRequirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.9) (2.2.4)\n","Collecting fastcore>=1.0.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/03/04eb54f2d482e06375cbbd06fb9d71670a5607739ecfa18a4bd25bfbd9fa/fastcore-1.0.15-py3-none-any.whl (40kB)\n","\u001b[K     |████████████████████████████████| 40kB 5.8MB/s \n","\u001b[?25hRequirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.9) (1.0.0)\n","Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.9) (19.3.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.9) (3.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.9) (1.4.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.9) (20.4)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.9) (7.0.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.9) (0.22.2.post1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.9) (2.23.0)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.9) (1.6.0+cu101)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.9) (3.13)\n","Requirement already satisfied: torchvision>=0.7 in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.9) (0.7.0+cu101)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.9) (1.0.5)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai==2.0.9) (0.4.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai==2.0.9) (0.8.0)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai==2.0.9) (1.0.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai==2.0.9) (1.18.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai==2.0.9) (1.0.2)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai==2.0.9) (7.4.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai==2.0.9) (1.0.2)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai==2.0.9) (2.0.3)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai==2.0.9) (3.0.2)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai==2.0.9) (4.41.1)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai==2.0.9) (1.1.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->fastai==2.0.9) (50.3.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai==2.0.9) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai==2.0.9) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai==2.0.9) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai==2.0.9) (2.8.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->fastai==2.0.9) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->fastai==2.0.9) (0.16.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai==2.0.9) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai==2.0.9) (2020.6.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai==2.0.9) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai==2.0.9) (1.24.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->fastai==2.0.9) (0.16.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->fastai==2.0.9) (2018.9)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->fastai==2.0.9) (1.7.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->fastai==2.0.9) (3.1.0)\n","Installing collected packages: fastcore, fastai\n","  Found existing installation: fastai 1.0.61\n","    Uninstalling fastai-1.0.61:\n","      Successfully uninstalled fastai-1.0.61\n","Successfully installed fastai-2.0.9 fastcore-1.0.15\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3XnNdbL3XmYA","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601217687804,"user_tz":420,"elapsed":595,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"7abafc3a-2504-4849-9aee-22cf492a14d8"},"source":["%cd nbs"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/fastai2_library/course-v4/nbs\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UGJUr6BSVkWV"},"source":["#export\n","from fastai.torch_basics import *\n","from fastai.data.load import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EfOKeIz_VkWZ"},"source":["#hide\n","#from nbdev.showdoc import *"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"amFMI14FVkWc"},"source":["# Data core\n","\n","> Core functionality for gathering data"]},{"cell_type":"markdown","metadata":{"id":"j2guM_5RVkWd"},"source":["The classes here provide functionality for applying a list of transforms to a set of items (`TfmdLists`, `Datasets`) or a `DataLoader` (`TfmdDl`) as well as the base class used to gather the data for model training: `DataLoaders`."]},{"cell_type":"markdown","metadata":{"id":"VcACKC0PVkWe"},"source":["## TfmdDL -"]},{"cell_type":"code","metadata":{"id":"UkOxGwhHVkWe"},"source":["#export\n","@typedispatch\n","def show_batch(x, y, samples, ctxs=None, max_n=9, **kwargs):\n","    if ctxs is None: ctxs = Inf.nones\n","    if hasattr(samples[0], 'show'):\n","        ctxs = [s.show(ctx=c, **kwargs) for s,c,_ in zip(samples,ctxs,range(max_n))]\n","    else:\n","        for i in range_of(samples[0]):\n","            ctxs = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs,range(max_n))]\n","    return ctxs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OXZbiTPvVkWi"},"source":["`show_batch` is a type-dispatched function that is responsible for showing decoded `samples`. `x` and `y` are the input and the target in the batch to be shown, and are passed along to dispatch on their types. There is a different implementation of `show_batch` if `x` is a `TensorImage` or a `TensorText` for instance (see vision.core or text.data for more details). `ctxs` can be passed but the function is responsible to create them if necessary. `kwargs` depend on the specific implementation."]},{"cell_type":"code","metadata":{"id":"ttVgk_T-VkWj"},"source":["#export\n","@typedispatch\n","def show_results(x, y, samples, outs, ctxs=None, max_n=9, **kwargs):\n","    if ctxs is None: ctxs = Inf.nones\n","    for i in range(len(samples[0])):\n","        ctxs = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs,range(max_n))]\n","    for i in range(len(outs[0])):\n","        ctxs = [b.show(ctx=c, **kwargs) for b,c,_ in zip(outs.itemgot(i),ctxs,range(max_n))]\n","    return ctxs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TDv6u6sAVkWm"},"source":["`show_results` is a type-dispatched function that is responsible for showing decoded `samples` and their corresponding `outs`. Like in `show_batch`, `x` and `y` are the input and the target in the batch to be shown, and are passed along to dispatch on their types. `ctxs` can be passed but the function is responsible to create them if necessary. `kwargs` depend on the specific implementation."]},{"cell_type":"code","metadata":{"id":"1d-KfjmaVkWn"},"source":["#export\n","_all_ = [\"show_batch\", \"show_results\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nsL4XozOVkWs"},"source":["#export\n","_batch_tfms = ('after_item','before_batch','after_batch')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_WUuIyk6VkWv"},"source":["#export\n","@log_args(but_as=DataLoader.__init__)\n","@delegates()\n","class TfmdDL(DataLoader):\n","    \"Transformed `DataLoader`\"\n","    # Due to @delegates decorator the kwargs below passed to the superclass DataLoader in super().__init__\n","    # will be correctly documented and will TAB complete.\n","    def __init__(self, dataset, bs=64, shuffle=False, num_workers=None, verbose=False, do_setup=True, **kwargs):\n","        if num_workers is None: num_workers = min(16, defaults.cpus)\n","         # The specific list of callbacks in _batch_tfms list are turned into Transform Pipelines\n","        # so after you grab a single item in the dataset there will be a pipeline\n","        # After you get all the items but before you collate them, there will be the\n","        # before_batch callback and its transform pipeline,  \n","        # After things have been turned into a batch there will be after_batch callback\n","        # transform pipeline\n","        for nm in _batch_tfms: kwargs[nm] = Pipeline(kwargs.get(nm,None))\n","        super().__init__(dataset, bs=bs, shuffle=shuffle, num_workers=num_workers, **kwargs)\n","        # Calls do_setup if each of the pipline transforms has a setup.\n","        if do_setup:\n","            for nm in _batch_tfms:\n","                pv(f\"Setting up {nm}: {kwargs[nm]}\", verbose)\n","                kwargs[nm].setup(self)\n","\n","    def _one_pass(self):\n","        # do_batch on do_item to get b and\n","        # find type of its after batching b.\n","        b = self.do_batch([self.do_item(0)])\n","        if self.device is not None: b = to_device(b, self.device)\n","        its = self.after_batch(b)\n","        self._n_inp = 1 if not isinstance(its, (list,tuple)) or len(its)==1 else len(its)-1\n","        self._types = explode_types(its)\n","\n","    def _retain_dl(self,b):\n","        # A function to figure out types during decode\n","        # Also done in new below\n","        # do _one_pass see above\n","        if not getattr(self, '_types', None): self._one_pass()\n","        # retain_types function knows how to ensure that things\n","        # keep their types\n","        return retain_types(b, typs=self._types)\n","\n","    @delegates(DataLoader.new)\n","    def new(self, dataset=None, cls=None, **kwargs):\n","        res = super().new(dataset, cls, do_setup=False, **kwargs)\n","        if not hasattr(self, '_n_inp') or not hasattr(self, '_types'):\n","            try:\n","                self._one_pass()\n","                res._n_inp,res._types = self._n_inp,self._types\n","            except: print(\"Could not do one pass in your dataloader, there is something wrong in it\")\n","        else: res._n_inp,res._types = self._n_inp,self._types\n","        return res\n","\n","    def before_iter(self):\n","        super().before_iter()\n","        split_idx = getattr(self.dataset, 'split_idx', None)\n","        for nm in _batch_tfms:\n","            f = getattr(self,nm)\n","            if isinstance(f,Pipeline): f.split_idx=split_idx\n","    # when we execute decode we can put back the type using the _retail_dl\n","    def decode(self, b): return self.before_batch.decode(to_cpu(self.after_batch.decode(self._retain_dl(b))))\n","    def decode_batch(self, b, max_n=9, full=True): return self._decode_batch(self.decode(b), max_n, full)\n","\n","    def _decode_batch(self, b, max_n=9, full=True):\n","        f = self.after_item.decode\n","        f = compose(f, partial(getattr(self.dataset,'decode',noop), full = full))\n","        return L(batch_to_samples(b, max_n=max_n)).map(f)\n","\n","    # Used in show_batch but does the decode to recall/put back the correct\n","    # types and call the decode bits of all of our different pipelines and allow\n","    # to show. If we can show whole batch at once for things like tabular\n","    # most datasets we have to display each part of the tuple separately eg image\n","    # and label, which is handled here.\n","    def _pre_show_batch(self, b, max_n=9):\n","        \"Decode `b` to be ready for `show_batch`\"\n","        b = self.decode(b)\n","        if hasattr(b, 'show'): return b,None,None\n","        its = self._decode_batch(b, max_n, full=False)\n","        if not is_listy(b): b,its = [b],L((o,) for o in its)\n","        return detuplify(b[:self.n_inp]),detuplify(b[self.n_inp:]),its\n","    \n","    # When show_batch is executed it calls _pre_show_batch\n","    def show_batch(self, b=None, max_n=9, ctxs=None, show=True, unique=False, **kwargs):\n","        if unique:\n","            old_get_idxs = self.get_idxs\n","            self.get_idxs = lambda: Inf.zeros\n","        if b is None: b = self.one_batch()\n","        if not show: return self._pre_show_batch(b, max_n=max_n)\n","        show_batch(*self._pre_show_batch(b, max_n=max_n), ctxs=ctxs, max_n=max_n, **kwargs)\n","        if unique: self.get_idxs = old_get_idxs\n","\n","    def show_results(self, b, out, max_n=9, ctxs=None, show=True, **kwargs):\n","        x,y,its = self.show_batch(b, max_n=max_n, show=False)\n","        b_out = type(b)(b[:self.n_inp] + (tuple(out) if is_listy(out) else (out,)))\n","        x1,y1,outs = self.show_batch(b_out, max_n=max_n, show=False)\n","        res = (x,x1,None,None) if its is None else (x, y, its, outs.itemgot(slice(self.n_inp,None)))\n","        if not show: return res\n","        show_results(*res, ctxs=ctxs, max_n=max_n, **kwargs)\n","\n","    @property\n","    def n_inp(self):\n","        if hasattr(self.dataset, 'n_inp'): return self.dataset.n_inp\n","        if not hasattr(self, '_n_inp'): self._one_pass()\n","        return self._n_inp\n","\n","    def to(self, device):\n","        self.device = device\n","        for tfm in self.after_batch.fs:\n","            for a in L(getattr(tfm, 'parameters', None)): setattr(tfm, a, getattr(tfm, a).to(device))\n","        return self"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G8Ull36QVkWy"},"source":["A `TfmdDL` is a `DataLoader` that creates `Pipeline` from a list of `Transform`s for the callbacks `after_item`, `before_batch` and `after_batch`. As a result, it can decode or show a processed `batch`."]},{"cell_type":"code","metadata":{"id":"dRLP-wvxVkWz"},"source":["#export\n","add_docs(TfmdDL,\n","         decode=\"Decode `b` using `tfms`\",\n","         decode_batch=\"Decode `b` entirely\",\n","         new=\"Create a new version of self with a few changed attributes\",\n","         show_batch=\"Show `b` (defaults to `one_batch`), a list of lists of pipeline outputs (i.e. output of a `DataLoader`)\",\n","         show_results=\"Show each item of `b` and `out`\",\n","         before_iter=\"override\",\n","         to=\"Put self and its transforms state on `device`\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Esm-JXiRVkW3"},"source":["class _Category(int, ShowTitle): pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UQZeZgJ4T7Xn"},"source":["#Test retain type\n","class NegTfm(Transform):\n","    def encodes(self, x): return torch.neg(x)\n","    def decodes(self, x): return torch.neg(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2kjMxky6UBjm"},"source":["#Test retain type using None to allow for setting of type to Tensor instead\n","# of TensorImage\n","class NegTfmNone(Transform):\n","    def encodes(self, x)->None: return torch.neg(x)\n","    def decodes(self, x): return torch.neg(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aNnkYKtsVkW6","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1601217733210,"user_tz":420,"elapsed":649,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"9439ab06-0c6b-4a17-f1b7-672bd018e5c2"},"source":["# Only coz torch.neg does not care about length of x, we can put NegTfm() in\n","# after_batch callback NOT OW. IF we had added CUDA it could have been on GPU\n","# Since NO CUDA this is still on CPU - even though it is after_batch\n","tdl = TfmdDL([(TensorImage([1]),)] * 4, after_batch=NegTfm(), bs=4, num_workers=4)\n","b = tdl.one_batch()\n","print(b)\n","print()\n","print(b[0])\n","print()\n","# Check that after going through Transform Type remains same as input to dataloader\n","# This is so even though torch.neg generates a Tensor \n","# Now TensorImage is subclass of Tensor and the right subclass type is provided to \n","# b[0] ie it is of type TensorImage and NOT type Tensor after going through \n","# NegTfm().\n","\n","# SO TfmdDL and Dataset check that after going through Transforms/Pipelines\n","# that the type DOES NOT CHANGE. IF IT DOES CHANGE - it will change it BACK \n","# TO WHAT IT USED TO BE. \n","\n","# IF YOU DO NOT WANT THIS BEHAVIOR - OPT OUT by putting \n","# RETURN TYPE of None\n","#\n","test_eq(type(b[0]), TensorImage)\n","b = (tensor([1.,1.,1.,1.]),)\n","print(b[0][0])\n","# Key thing to note here that due to TypeDispatch the TensorImage type is \n","# attached to the data that comes from the data loader  \n","\n","# This is handy for Inference for in production your predictions can have\n","# proper types so you can call normal methods and Transforms on them\n","\n","# Also for applying stuff to the Test Set \n","# \n","test_eq(type(tdl.decode_batch(b)[0][0]), TensorImage)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(TensorImage([[-1],\n","        [-1],\n","        [-1],\n","        [-1]]),)\n","\n","TensorImage([[-1],\n","        [-1],\n","        [-1],\n","        [-1]])\n","\n","tensor(1.)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZI3fD8UZUnvl","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1601217736253,"user_tz":420,"elapsed":577,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"80fe45bf-6c94-4af3-aec1-761f7afe9909"},"source":["# Check that after going through Transform Type DOES NOT remain same as input \n","# to dataloader because NegTfmNone uses None for return allowing opting out\n","# of Transform's inherent ability to retain types going through Transform\n","\n","# So unlike for NegTfm, for NegTfmNone b is a tensor  \n","# \n","\n","# IF YOU DO NOT WANT THIS BEHAVIOR - OPT OUT by putting \n","# RETURN TYPE of None\n","#\n","tdl = TfmdDL([(TensorImage([1]),)] * 4, after_batch=NegTfmNone(), bs=4, num_workers=4)\n","b = tdl.one_batch()\n","print(b)\n","print()\n","print(b[0])\n","print()\n","\n","test_eq(type(b[0]), Tensor)\n","b = (tensor([1.,1.,1.,1.]),)\n","print(b[0][0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(tensor([[-1],\n","        [-1],\n","        [-1],\n","        [-1]]),)\n","\n","tensor([[-1],\n","        [-1],\n","        [-1],\n","        [-1]])\n","\n","tensor(1.)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Wh5paCW3VkW9"},"source":["class A(Transform): \n","    def encodes(self, x): return x \n","    def decodes(self, x): return TitledInt(x) \n","\n","# Note that there is no decode for this function\n","# so on decode it just returns its input\n","@Transform\n","def f(x)->None: return fastuple((x,x))\n","\n","start = torch.arange(50)\n","test_eq_type(f(2), fastuple((2,2)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GhSiLay8auTZ"},"source":["'''\n","Signature: tdl.one_batch()\n","Source:   \n","    def one_batch(self):\n","        if self.n is not None and len(self)==0: raise ValueError(f'This DataLoader does not contain any batches')\n","        with self.fake_l.no_multiproc(): res = first(self)\n","        if hasattr(self, 'it'): delattr(self, 'it')\n","        return res\n","File:      /usr/local/lib/python3.6/dist-packages/fastai2/data/load.py\n","Type:      method\n","'''\n","tdl.one_batch??"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ltiZlN6PawIA"},"source":["'''\n","Signature: first(x)\n","Source:   \n","def first(x):\n","    \"First element of `x`, or None if missing\"\n","    try: return next(iter(x))\n","    except StopIteration: return None\n","File:      /usr/local/lib/python3.6/dist-packages/fastcore/utils.py\n","'''\n","first??"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dWeXieF1VkXA","colab":{"base_uri":"https://localhost:8080/","height":153},"executionInfo":{"status":"ok","timestamp":1601217742486,"user_tz":420,"elapsed":747,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"c7ff84ea-d9ce-4c8d-d891-5793a4271e5d"},"source":["a = A()\n","tdl = TfmdDL(start, after_item=lambda x: (a(x), f(x)), bs=4)\n","# note that one_batch() is just next(iter(self)) of a tfmdDL\n","# alternately first(x) as shown above\n","x,y = tdl.one_batch()\n","print(x)\n","print()\n","print(y)\n","print()\n","test_eq(type(y), fastuple)\n","\n","s = tdl.decode_batch((x,y))\n","print(s)\n","print()\n","print(s[0][1])\n","test_eq(type(s[0][1]), fastuple)\n","#skr tests\n","print(s[0][0])\n","# Q - why is below a Tensor - would expect it to be TitledInt see A above \n","# Ans: NO - since default behavior of Transforms is to retain passed in type \n","# and the input to decodes is a Tensor the output will also be a Tensor even if\n","# decodes method return type is TitledInt - ofcourse unless behavior is opted\n","# OUT OF by providing None as output type of decodes\n","test_eq(type(s[0][0]), Tensor)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([0, 1, 2, 3])\n","\n","(tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3]))\n","\n","(#4) [(tensor(0), (tensor(0), tensor(0))),(tensor(1), (tensor(1), tensor(1))),(tensor(2), (tensor(2), tensor(2))),(tensor(3), (tensor(3), tensor(3)))]\n","\n","(tensor(0), tensor(0))\n","tensor(0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b8km8kguVkXE"},"source":["tdl = TfmdDL(torch.arange(0,50), after_item=A(), after_batch=NegTfm(), bs=4)\n","test_eq(tdl.dataset[0], start[0])\n","# see cells below for explanation\n","test_eq(len(tdl), (50-1)//4+1)\n","test_eq(tdl.bs, 4)\n","test_stdout(tdl.show_batch, '0\\n1\\n2\\n3')\n","test_stdout(partial(tdl.show_batch, unique=True), '0\\n0\\n0\\n0')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MPaQxe41bbv8","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601217747493,"user_tz":420,"elapsed":536,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"e3df0418-6faf-4dd7-f448-34a99163b485"},"source":["(50 - 1) // 4 "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"sCD8PeG9bbhb","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601217748975,"user_tz":420,"elapsed":771,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"6532293e-5433-4127-8f38-28427ec5cb4e"},"source":["# get the // 4 and + 1 but why 50 - 1??\n","(50 - 1) // 4 + 1"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["13"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"SYohiDHVbr4u","colab":{"base_uri":"https://localhost:8080/","height":187},"executionInfo":{"status":"ok","timestamp":1601217750973,"user_tz":420,"elapsed":516,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"299eab66-cda7-4075-c3ad-4e5fca736895"},"source":["batch_val = tdl.show_batch()\n","print(batch_val)\n","batch_val_unique = partial(tdl.show_batch, unique=True)()\n","print(batch_val_unique)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0\n","1\n","2\n","3\n","None\n","0\n","0\n","0\n","0\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NV4N_V1Rbrtu"},"source":["'''\n","Signature: detuplify(x)\n","Source:   \n","def detuplify(x):\n","    \"If `x` is a tuple with one thing, extract it\"\n","    return None if len(x)==0 else x[0] if len(x)==1 and getattr(x, 'ndim', 1)==1 else x\n","File:      /usr/local/lib/python3.6/dist-packages/fastcore/utils.py\n","'''\n","detuplify??"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MdFKHIYwbyjt","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1601217753766,"user_tz":420,"elapsed":554,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"c8bc6dd7-7335-45e5-8e24-b22f30e7d69d"},"source":["x = ((1),)\n","value1 = detuplify(x)\n","print(value1)\n","x = (())\n","value2 = detuplify(x)\n","print(value2)\n","x = ((1),)\n","# so this creates a dim attribute and assigns value of that to 1\n","# if that is true it returns x[0] else returns x \n","# so that getattr part is to distinguish between the two cases of \n","# x = (1,) in which case return x vs x = ((1),) in which case return x[0]\n","print(getattr(x, 'ndim', 1)) \n","print()\n","x = (1,)\n","value3 = detuplify(x)\n","print(value3)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1\n","None\n","1\n","\n","1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IdzbQ6tkVkXI"},"source":["class B(Transform):\n","    parameters = 'a'\n","    def __init__(self): self.a = torch.tensor(0.)\n","    def encodes(self, x): x\n","    \n","tdl = TfmdDL([(TensorImage([1]),)] * 4, after_batch=B(), bs=4)\n","test_eq(tdl.after_batch.fs[0].a.device, torch.device('cpu'))\n","tdl.to(default_device())\n","test_eq(tdl.after_batch.fs[0].a.device, default_device())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oxIU1CopVkXL"},"source":["### Methods"]},{"cell_type":"code","metadata":{"id":"26knsVBLVkXM","outputId":"6178d2be-983f-4af5-98df-d2f214f64891"},"source":["show_doc(TfmdDL.one_batch)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/markdown":"<h4 id=\"DataLoader.one_batch\" class=\"doc_header\"><code>DataLoader.one_batch</code><a href=\"https://github.com/fastai/fastai/tree/master/fastai/data/load.py#L130\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>DataLoader.one_batch</code>()\n\n","text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"HqSWUm1rVkXQ"},"source":["tfm = NegTfm()\n","tdl = TfmdDL(start, after_batch=tfm, bs=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3WeveNdYVkXV"},"source":["b = tdl.one_batch()\n","test_eq(tensor([0,-1,-2,-3]), b)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EHR0u9BgVkXZ","outputId":"15c7d91b-3025-4fe3-80f7-16ba95114084"},"source":["show_doc(TfmdDL.decode)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/markdown":"<h4 id=\"TfmdDL.decode\" class=\"doc_header\"><code>TfmdDL.decode</code><a href=\"__main__.py#L44\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>TfmdDL.decode</code>(**`b`**)\n\nDecode `b` using `tfms`","text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"qyrrG938VkXb"},"source":["# Note that this is just negTfm() decode which neg its input so gets back original\n","test_eq(tdl.decode(b), tensor(0,1,2,3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AwjNN3n0VkXe","outputId":"21c5a508-d642-45c4-c831-31326e9bf640"},"source":["show_doc(TfmdDL.decode_batch)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/markdown":"<h4 id=\"TfmdDL.decode_batch\" class=\"doc_header\"><code>TfmdDL.decode_batch</code><a href=\"__main__.py#L45\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>TfmdDL.decode_batch</code>(**`b`**, **`max_n`**=*`9`*, **`full`**=*`True`*)\n\nDecode `b` entirely","text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"bcxxvyGYVkXh"},"source":["test_eq(tdl.decode_batch(b), [0,1,2,3])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ASABt5pYVkXk","outputId":"4a6648b8-24bb-4e15-8e70-d4b1d5f9e14a"},"source":["show_doc(TfmdDL.show_batch)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/markdown":"<h4 id=\"TfmdDL.show_batch\" class=\"doc_header\"><code>TfmdDL.show_batch</code><a href=\"__main__.py#L60\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>TfmdDL.show_batch</code>(**`b`**=*`None`*, **`max_n`**=*`9`*, **`ctxs`**=*`None`*, **`show`**=*`True`*, **`unique`**=*`False`*, **\\*\\*`kwargs`**)\n\nShow `b` (defaults to `one_batch`), a list of lists of pipeline outputs (i.e. output of a [`DataLoader`](data.load#DataLoader))","text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"mK3ApLLuVkXn","outputId":"57986c2f-b232-4a32-b9aa-791ff157a03f"},"source":["show_doc(TfmdDL.to)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/markdown":"<h4 id=\"TfmdDL.to\" class=\"doc_header\"><code>TfmdDL.to</code><a href=\"__main__.py#L83\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>TfmdDL.to</code>(**`device`**)\n\nPut self and its transforms state on `device`","text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"tvkwkLGycNyF"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1hLAhoGrcPcx"},"source":["# What is covered at beginning of WT 3 Video"]},{"cell_type":"code","metadata":{"id":"mHnXccV8cW40"},"source":["''\n","Signature: to_device(b, device=None)\n","Source:   \n","def to_device(b, device=None):\n","    \"Recursively put `b` on `device`.\"\n","    if defaults.use_cuda==False: device='cpu'\n","    elif device is None: device=default_device()\n","    def _inner(o): return o.to(device, non_blocking=True) if isinstance(o,Tensor) else o.to_device(device) if hasattr(o, \"to_device\") else o\n","    # appply applies _inner recursively to any list, tuple, dict etc ie to all elems of those \n","    return apply(_inner, b)\n","File:      /usr/local/lib/python3.6/dist-packages/fastai2/torch_core.py\n","'''\n","to_device??"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kLrkDSgLcf68"},"source":["'''\n","Signature: default_device(use_cuda=-1)\n","Source:   \n","def default_device(use_cuda=-1):\n","    \"Return or set default device; `use_cuda`: None - CUDA if available; True - error if not availabe; False - CPU\"\n","    if use_cuda != -1: defaults.use_cuda=use_cuda\n","    use = defaults.use_cuda or (torch.cuda.is_available() and defaults.use_cuda is None)\n","    assert torch.cuda.is_available() or not use\n","    return torch.device(torch.cuda.current_device()) if use else torch.device('cpu')\n","File:      /usr/local/lib/python3.6/dist-packages/fastai2/torch_core.py\n","'''\n","default_device??"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d5I8R9gKchDz","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601217776124,"user_tz":420,"elapsed":518,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"0033fc5b-0bb7-4565-f6c2-02bf0034aa4d"},"source":["# Can use torch.cuda.set_device() to set set the device to be used \n","default_device()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"XRzdCnIZcp_9"},"source":["# Note below apply is fastai apply - rather than changing something it returns the result of applying\n","# and it is making every effort to keep all the types consistent as fastai does everywhere so if your listy thing\n","# is actually some subclass it will make sure that it keeps that subclass all the way through.\n","# "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bdhVZhiIcg77"},"source":["'''\n","Signature: apply(func, x, *args, **kwargs)\n","Source:   \n","def apply(func, x, *args, **kwargs):\n","    \"Apply `func` recursively to `x`, passing on args\"\n","    if is_listy(x): return type(x)([apply(func, o, *args, **kwargs) for o in x])\n","    if isinstance(x,dict):  return {k: apply(func, v, *args, **kwargs) for k,v in x.items()}\n","    res = func(x, *args, **kwargs)\n","    return res if x is None else retain_type(res, x)\n","File:      /usr/local/lib/python3.6/dist-packages/fastai2/torch_core.py\n","'''\n","apply??"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KJ7hndrxctUa"},"source":["'''\n","Signature: docs(cls)\n","Source:   \n","def docs(cls):\n","    \"Decorator version of `add_docs`, using `_docs` dict\"\n","    add_docs(cls, **cls._docs)\n","    return cls\n","File:      /usr/local/lib/python3.6/dist-packages/fastcore/foundation.py\n","'''\n","docs??"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ohCnEdgbctid"},"source":["'''\n","Signature: add_docs(cls, cls_doc=None, **docs)\n","Source:   \n","def add_docs(cls, cls_doc=None, **docs):\n","    \"Copy values from `docs` to `cls` docstrings, and confirm all public methods are documented\"\n","    if cls_doc is not None: cls.__doc__ = cls_doc\n","    for k,v in docs.items():\n","        f = getattr(cls,k)\n","        if hasattr(f,'__func__'): f = f.__func__ # required for class methods\n","        f.__doc__ = v\n","    # List of public callables without docstring\n","    nodoc = [c for n,c in vars(cls).items() if callable(c)\n","             and not n.startswith('_') and c.__doc__ is None]\n","    assert not nodoc, f\"Missing docs: {nodoc}\"\n","    assert cls.__doc__ is not None, f\"Missing class docs: {cls}\"\n","'''\n","add_docs??"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ckLcwTJGcr8y"},"source":["GetAttr??"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ud-2xiyYVkXq"},"source":["## DataLoaders -"]},{"cell_type":"markdown","metadata":{"id":"KmoRIbVDc-Fi"},"source":["Note that there is DataLoader class (note singular) which is different and is in 02_data.load notebook which also inherits from GetAttr - that DataLoader has the one_batch method."]},{"cell_type":"code","metadata":{"id":"Y5dwcx3VdCbR"},"source":["'''\n","Init signature: GetAttr(*args, **kwargs)\n","Source:        \n","class GetAttr:\n","    \"Inherit from this to have all attr accesses in `self._xtra` passed down to `self.default`\"\n","    _default='default'\n","    def _component_attr_filter(self,k):\n","        if k.startswith('__') or k in ('_xtra',self._default): return False\n","        xtra = getattr(self,'_xtra',None)\n","        return xtra is None or k in xtra\n","    def _dir(self): return [k for k in dir(getattr(self,self._default)) if self._component_attr_filter(k)]\n","    def __getattr__(self,k):\n","        if self._component_attr_filter(k):\n","            attr = getattr(self,self._default,None)\n","            if attr is not None: return getattr(attr,k)\n","        raise AttributeError(k)\n","    def __dir__(self): return custom_dir(self,self._dir())\n","#     def __getstate__(self): return self.__dict__\n","    def __setstate__(self,data): self.__dict__.update(data)\n","File:           /usr/local/lib/python3.6/dist-packages/fastcore/foundation.py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RyPmeyz4VkXr"},"source":["# export\n","@docs\n","class DataLoaders(GetAttr):\n","    \"Basic wrapper around several `DataLoader`s.\"\n","    _default='train'\n","    def __init__(self, *loaders, path='.', device=None):\n","        self.loaders,self.path = list(loaders),Path(path)\n","        if device is not None or hasattr(loaders[0],'to'): self.device = device\n","\n","    def __getitem__(self, i): return self.loaders[i]\n","    def new_empty(self):\n","        loaders = [dl.new(dl.dataset.new_empty()) for dl in self.loaders]\n","        return type(self)(*loaders, path=self.path, device=self.device)\n","\n","    def _set(i, self, v): self.loaders[i] = v\n","    train   ,valid    = add_props(lambda i,x: x[i], _set)\n","    train_ds,valid_ds = add_props(lambda i,x: x[i].dataset)\n","\n","    @property\n","    def device(self): return self._device\n","\n","    @device.setter\n","    def device(self, d):\n","        for dl in self.loaders: dl.to(d)\n","        self._device = d\n","\n","    def to(self, device):\n","        self.device = device\n","        return self\n","\n","    def cuda(self): return self.to(device=default_device())\n","    def cpu(self):  return self.to(device=torch.device('cpu'))\n","\n","    @classmethod\n","    def from_dsets(cls, *ds, path='.',  bs=64, device=None, dl_type=TfmdDL, **kwargs):\n","        default = (True,) + (False,) * (len(ds)-1)\n","        defaults = {'shuffle': default, 'drop_last': default}\n","        for nm in _batch_tfms:\n","            if nm in kwargs: kwargs[nm] = Pipeline(kwargs[nm])\n","        kwargs = merge(defaults, {k: tuplify(v, match=ds) for k,v in kwargs.items()})\n","        kwargs = [{k: v[i] for k,v in kwargs.items()} for i in range_of(ds)]\n","        return cls(*[dl_type(d, bs=bs, **k) for d,k in zip(ds, kwargs)], path=path, device=device)\n","\n","    @classmethod\n","    def from_dblock(cls, dblock, source, path='.',  bs=64, val_bs=None, shuffle_train=True, device=None, **kwargs):\n","        return dblock.dataloaders(source, path=path, bs=bs, val_bs=val_bs, shuffle_train=shuffle_train, device=device, **kwargs)\n","\n","    _docs=dict(__getitem__=\"Retrieve `DataLoader` at `i` (`0` is training, `1` is validation)\",\n","               train=\"Training `DataLoader`\",\n","               valid=\"Validation `DataLoader`\",\n","               train_ds=\"Training `Dataset`\",\n","               valid_ds=\"Validation `Dataset`\",\n","               to=\"Use `device`\",\n","               cuda=\"Use the gpu if available\",\n","               cpu=\"Use the cpu\",\n","               new_empty=\"Create a new empty version of `self` with the same transforms\",\n","               from_dblock=\"Create a dataloaders from a given `dblock`\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n8Xa_wyxdJ9p","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1601217786453,"user_tz":420,"elapsed":667,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"ed3942f3-d5c5-4fb4-e191-9fb471334817"},"source":["tdl.data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n","        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n","        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"o1RkFCdtdM7D","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601217790010,"user_tz":420,"elapsed":513,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"6daceef3-29b8-478f-f0de-37c6c8937faa"},"source":["b = tdl.one_batch()\n","b"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 0, -1, -2, -3])"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"CeVkz4XDdSsG"},"source":["'''\n","Signature: first(x)\n","Source:   \n","def first(x):\n","    \"First element of `x`, or None if missing\"\n","    try: return next(iter(x))\n","    except StopIteration: return None\n","File:      /usr/local/lib/python3.6/dist-packages/fastcore/utils.py\n","'''\n","first??"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SWeafpyUVkXu"},"source":["# DataLoaders just wrapper around multiple loaders\n","dls = DataLoaders(tdl,tdl)\n","# x is one batch of dls train dataloader \n","x = dls.train.one_batch()\n","# x2 is first (one_batch) of tdl - which ought to be same \n","# as dls is just a wrapper and dls.train is just first dataloader\n","x2 = first(tdl)\n","# Hence x and x2 are equal\n","test_eq(x,x2)\n","# Below test shows that dls.one_batch() and ds.train.one_batch() are the same\n","# This is because _default in dataloaders is 'train' (see above)\n","x2 = dls.one_batch()\n","test_eq(x,x2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"La_GUBKAd4Bq"},"source":["'''\n","Signature: add_props(f, g=None, n=2)\n","Source:   \n","def add_props(f, g=None, n=2):\n","    \"Create properties passing each of `range(n)` to f\"\n","    if g is None: return (property(partial(f,i)) for i in range(n))\n","    return (property(partial(f,i), partial(g,i)) for i in range(n))\n","File:      /usr/local/lib/python3.6/dist-packages/fastcore/utils.py\n","'''\n","add_props??"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GW1yJ6_7VkXw"},"source":["#hide\n","#test assignment works\n","dls.train = dls.train.new(bs=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gWRrBjJ8VkX2"},"source":["### Methods"]},{"cell_type":"code","metadata":{"id":"1CygdNb8VkX3","outputId":"815a2fb7-b657-44bb-d47e-d693cefd82e6"},"source":["show_doc(DataLoaders.__getitem__)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/markdown":"<h4 id=\"DataLoaders.__getitem__\" class=\"doc_header\"><code>DataLoaders.__getitem__</code><a href=\"__main__.py#L10\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>DataLoaders.__getitem__</code>(**`i`**)\n\nRetrieve [`DataLoader`](data.load#DataLoader) at `i` (`0` is training, `1` is validation)","text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"UMELvrYQVkX5"},"source":["x2 = dls[0].one_batch()\n","test_eq(x,x2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2kjq5f0lVkX-","outputId":"8cd1ced7-55ba-49e5-bc0e-ae935c140b31"},"source":["show_doc(DataLoaders.train, name=\"DataLoaders.train\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/markdown":"<h4 id=\"DataLoaders.train\" class=\"doc_header\"><code>DataLoaders.train</code><a href=\"__main__.py#L16\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\nTraining [`DataLoader`](data.load#DataLoader)","text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"KefZJR4wVkYA","outputId":"4e3ff740-33c1-4c65-92b2-6d7c77526c96"},"source":["show_doc(DataLoaders.valid, name=\"DataLoaders.valid\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/markdown":"<h4 id=\"DataLoaders.valid\" class=\"doc_header\"><code>DataLoaders.valid</code><a href=\"__main__.py#L16\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\nValidation [`DataLoader`](data.load#DataLoader)","text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"HRTCc885VkYF","outputId":"d0d0132e-d6f7-4a68-d016-9eb2e0110146"},"source":["show_doc(DataLoaders.train_ds, name=\"DataLoaders.train_ds\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/markdown":"<h4 id=\"DataLoaders.train_ds\" class=\"doc_header\"><code>DataLoaders.train_ds</code><a href=\"__main__.py#L17\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\nTraining `Dataset`","text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"SaL-2GhHVkYH","outputId":"2aba3ac1-ae43-49cb-cd2e-a5c67849dbcd"},"source":["show_doc(DataLoaders.valid_ds, name=\"DataLoaders.valid_ds\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/markdown":"<h4 id=\"DataLoaders.valid_ds\" class=\"doc_header\"><code>DataLoaders.valid_ds</code><a href=\"__main__.py#L17\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\nValidation `Dataset`","text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"uY6TuWc_VkYK"},"source":["## TfmdLists -"]},{"cell_type":"code","metadata":{"id":"OOFXwCd6VkYK"},"source":["#export\n","class FilteredBase:\n","    \"Base class for lists with subsets\"\n","    _dl_type,_dbunch_type = TfmdDL,DataLoaders\n","    def __init__(self, *args, dl_type=None, **kwargs):\n","        if dl_type is not None: self._dl_type = dl_type\n","        self.dataloaders = delegates(self._dl_type.__init__)(self.dataloaders)\n","        super().__init__(*args, **kwargs)\n","\n","    @property\n","    def n_subsets(self): return len(self.splits)\n","    def _new(self, items, **kwargs): return super()._new(items, splits=self.splits, **kwargs)\n","    def subset(self): raise NotImplemented\n","\n","    def dataloaders(self, bs=64, val_bs=None, shuffle_train=True, n=None, path='.', dl_type=None, dl_kwargs=None,\n","                    device=None, **kwargs):\n","        if device is None: device=default_device()\n","        if dl_kwargs is None: dl_kwargs = [{}] * self.n_subsets\n","        if dl_type is None: dl_type = self._dl_type\n","        drop_last = kwargs.pop('drop_last', shuffle_train)\n","        dl = dl_type(self.subset(0), bs=bs, shuffle=shuffle_train, drop_last=drop_last, n=n, device=device,\n","                     **merge(kwargs, dl_kwargs[0]))\n","        dls = [dl] + [dl.new(self.subset(i), bs=(bs if val_bs is None else val_bs), shuffle=False, drop_last=False,\n","                             n=None, **dl_kwargs[i]) for i in range(1, self.n_subsets)]\n","        return self._dbunch_type(*dls, path=path, device=device)\n","\n","FilteredBase.train,FilteredBase.valid = add_props(lambda i,x: x.subset(i))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GIIJmnJKeO1T"},"source":["TfmdLists: Pass in a list of items and a list of transforms, it creates a pipeline with those transforms, and due to inheriting from L and super().__init__ items items are in a L.\n","\n","self.splits becomes multiple L's if splits is not None\n","\n","if tfms are already TfmdLists then tfms = tfms.tfms\n","\n","if tfms are already Pipelines set their do_setup to False (do_setup indicates whether Pipeline.setup method should be called during initalization).\n","\n","if not make tfms Pipelines and set split_idx to passed in split_idx\n","\n","This is how (see example in tests below) you get to access TfmdLists as Ls, so we can subscript into it, and apply the Pipeline to it.\n","\n","So you begin to see some nice PyTorch dataset like behavior with TfmdLists. But keep in mind that a Dataset has an x and a y typically. Here we only have one thing in TfmdLists."]},{"cell_type":"markdown","metadata":{"id":"QUxSA5mmeTmb"},"source":["Aside: L is designed to let us create some more interesting type of collections. When you call \\_\\_getitem\\_\\_ in L, it actually calls self._get(idx) if idx is an iterator o.w. it calls L(self._get(idx), use_list=None). As long as i is not an iterator or slice, it just returns the i'th element. If an iterator or slice you can do interesting things like use this to access a pandas dataframe or an array or an iterator - see details in _get method of L."]},{"cell_type":"code","metadata":{"id":"-Y8w1phmVkYO"},"source":["#export\n","class TfmdLists(FilteredBase, L, GetAttr):\n","    \"A `Pipeline` of `tfms` applied to a collection of `items`\"\n","    _default='tfms'\n","    def __init__(self, items, tfms, use_list=None, do_setup=True, split_idx=None, train_setup=True,\n","                 splits=None, types=None, verbose=False, dl_type=None):\n","        super().__init__(items, use_list=use_list)\n","        if dl_type is not None: self._dl_type = dl_type\n","        self.splits = L([slice(None),[]] if splits is None else splits).map(mask2idxs)\n","        if isinstance(tfms,TfmdLists): tfms = tfms.tfms\n","        if isinstance(tfms,Pipeline): do_setup=False\n","        self.tfms = Pipeline(tfms, split_idx=split_idx)\n","        store_attr('types,split_idx')\n","        if do_setup:\n","            pv(f\"Setting up {self.tfms}\", verbose)\n","            self.setup(train_setup=train_setup)\n","\n","    def _new(self, items, split_idx=None, **kwargs):\n","        split_idx = ifnone(split_idx,self.split_idx)\n","        return super()._new(items, tfms=self.tfms, do_setup=False, types=self.types, split_idx=split_idx, **kwargs)\n","    def subset(self, i): return self._new(self._get(self.splits[i]), split_idx=i)\n","    def _after_item(self, o): return self.tfms(o)\n","    def __repr__(self): return f\"{self.__class__.__name__}: {self.items}\\ntfms - {self.tfms.fs}\"\n","    def __iter__(self): return (self[i] for i in range(len(self)))\n","    def show(self, o, **kwargs): return self.tfms.show(o, **kwargs)\n","    def decode(self, o, **kwargs): return self.tfms.decode(o, **kwargs)\n","    def __call__(self, o, **kwargs): return self.tfms.__call__(o, **kwargs)\n","    def overlapping_splits(self): return L(Counter(self.splits.concat()).values()).filter(gt(1))\n","    def new_empty(self): return self._new([])\n","\n","    def setup(self, train_setup=True):\n","        self.tfms.setup(self, train_setup)\n","        if len(self) != 0:\n","            x = super().__getitem__(0) if self.splits is None else super().__getitem__(self.splits[0])[0]\n","            self.types = []\n","            for f in self.tfms.fs:\n","                self.types.append(getattr(f, 'input_types', type(x)))\n","                x = f(x)\n","            self.types.append(type(x))\n","        types = L(t if is_listy(t) else [t] for t in self.types).concat().unique()\n","        self.pretty_types = '\\n'.join([f'  - {t}' for t in types])\n","\n","    def infer_idx(self, x):\n","        # TODO: check if we really need this, or can simplify\n","        idx = 0\n","        for t in self.types:\n","            if isinstance(x, t): break\n","            idx += 1\n","        types = L(t if is_listy(t) else [t] for t in self.types).concat().unique()\n","        pretty_types = '\\n'.join([f'  - {t}' for t in types])\n","        assert idx < len(self.types), f\"Expected an input of type in \\n{pretty_types}\\n but got {type(x)}\"\n","        return idx\n","\n","    def infer(self, x):\n","        return compose_tfms(x, tfms=self.tfms.fs[self.infer_idx(x):], split_idx=self.split_idx)\n","\n","    def __getitem__(self, idx):\n","        res = super().__getitem__(idx)\n","        if self._after_item is None: return res\n","        return self._after_item(res) if is_indexer(idx) else res.map(self._after_item)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8jyiUE1IVkYQ"},"source":["#export\n","add_docs(TfmdLists,\n","         setup=\"Transform setup with self\",\n","         decode=\"From `Pipeline`\",\n","         show=\"From `Pipeline`\",\n","         overlapping_splits=\"All splits that are in more than one split\",\n","         subset=\"New `TfmdLists` with same tfms that only includes items in `i`th split\",\n","         infer_idx=\"Finds the index where `self.tfms` can be applied to `x`, depending on the type of `x`\",\n","         infer=\"Apply `self.tfms` to `x` starting at the right tfm depending on the type of `x`\",\n","         new_empty=\"A new version of `self` but with no items\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tJP35yQzVkYT"},"source":["#exports\n","def decode_at(o, idx):\n","    \"Decoded item at `idx`\"\n","    return o.decode(o[idx])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zVkigT3xVkYV"},"source":["#exports\n","def show_at(o, idx, **kwargs):\n","    \"Show item at `idx`\",\n","    return o.show(o[idx], **kwargs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_EnO4ttrVkYY"},"source":["A `TfmdLists` combines a collection of object with a `Pipeline`. `tfms` can either be a `Pipeline` or a list of transforms, in which case, it will wrap them in a `Pipeline`. `use_list` is passed along to `L` with the `items` and `split_idx` are passed to each transform of the `Pipeline`. `do_setup` indicates if the `Pipeline.setup` method should be called during initialization."]},{"cell_type":"code","metadata":{"id":"TtuH5IV-VkYZ"},"source":["class _IntFloatTfm(Transform):\n","    def encodes(self, o):  return TitledInt(o)\n","    def decodes(self, o):  return TitledFloat(o)\n","int2f_tfm=_IntFloatTfm()\n","\n","def _neg(o): return -o\n","neg_tfm = Transform(_neg, _neg)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t36WtE1HVkYe","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1601217849336,"user_tz":420,"elapsed":538,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"d4cc7300-82bb-4f57-c93d-1bad4f72d728"},"source":["items = L([1.,2.,3.]); tfms = [neg_tfm, int2f_tfm]\n","tl = TfmdLists(items, tfms=tfms)\n","test_eq_type(tl[0], TitledInt(-1))\n","test_eq_type(tl[1], TitledInt(-2))\n","# explain next 2 lines why the diff?\n","test_eq_type(tl.decode(tl[2]), TitledFloat(3.))\n","test_stdout(lambda: show_at(tl, 2), '-3')\n","test_eq(tl.types, [float, float, TitledInt])\n","tl"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TfmdLists: [1.0, 2.0, 3.0]\n","tfms - (#2) [_neg:\n","encodes: (object,object) -> _negdecodes: (object,object) -> _neg,_IntFloatTfm:\n","encodes: (object,object) -> encodes\n","decodes: (object,object) -> decodes\n","]"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"Pu_D3ZeqVkYh"},"source":["# add splits to TfmdLists\n","splits = [[0,2],[1]]\n","tl = TfmdLists(items, tfms=tfms, splits=splits)\n","test_eq(tl.n_subsets, 2)\n","test_eq(tl.train, tl.subset(0))\n","test_eq(tl.valid, tl.subset(1))\n","test_eq(tl.train.items, items[splits[0]])\n","test_eq(tl.valid.items, items[splits[1]])\n","test_eq(tl.train.tfms.split_idx, 0)\n","test_eq(tl.valid.tfms.split_idx, 1)\n","test_eq(tl.train.new_empty().split_idx, 0)\n","test_eq(tl.valid.new_empty().split_idx, 1)\n","test_eq_type(tl.splits, L(splits))\n","assert not tl.overlapping_splits()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oayQGcL2VkYk"},"source":["df = pd.DataFrame(dict(a=[1,2,3],b=[2,3,4]))\n","tl = TfmdLists(df, lambda o: o.a+1, splits=[[0],[1,2]])\n","# coz o.a+1 and tl at index [1, 2] will be a+1 at index [1, 2]\n","# which are 2+1, 3+1 hence [3, 4]\n","test_eq(tl[1,2], [3,4])\n","# since subset 0 will be just index0 and index0 a is 1 so \n","# o.a+1 = 2 hence tr[:] is 2\n","tr = tl.subset(0)\n","test_eq(tr[:], [2])\n","# similar logic subset 1 has 2 values [3, 4]\n","val = tl.subset(1)\n","test_eq(val[:], [3,4])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZteexxY5VkYn","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601217855750,"user_tz":420,"elapsed":601,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"86d2bc53-fc60-419a-d94b-713a3ead62ed"},"source":["print(items)\n","class _B(Transform):\n","    def __init__(self): self.m = 0\n","    def encodes(self, o): return o+self.m\n","    def decodes(self, o): return o-self.m\n","    def setups(self, items): \n","        # print(items)\n","        self.m = tensor(items).float().mean().item()\n","\n","# test for setup, which updates `self.m`\n","tl = TfmdLists(items, _B())\n","test_eq(tl.m, 2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(#3) [1.0,2.0,3.0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fjTV5KNOVkYq"},"source":["Here's how we can use `TfmdLists.setup` to implement a simple category list, getting labels from a mock file list:"]},{"cell_type":"code","metadata":{"id":"29JqEuWGVkYr"},"source":["class _Cat(Transform):\n","    order = 1\n","    def encodes(self, o):    return int(self.o2i[o])\n","    def decodes(self, o):    return TitledStr(self.vocab[o])\n","    def setups(self, items): self.vocab,self.o2i = uniqueify(L(items), sort=True, bidir=True)\n","tcat = _Cat()\n","\n","def _lbl(o): return TitledStr(o.split('_')[0])\n","\n","# Check that tfms are sorted by `order` & `_lbl` is called first\n","fns = ['dog_0.jpg','cat_0.jpg','cat_2.jpg','cat_1.jpg','dog_1.jpg']\n","tl = TfmdLists(fns, [tcat,_lbl])\n","exp_voc = ['cat','dog']\n","test_eq(tcat.vocab, exp_voc)\n","test_eq(tl.tfms.vocab, exp_voc)\n","test_eq(tl.vocab, exp_voc)\n","test_eq(tl, (1,0,0,0,1))\n","test_eq([tl.decode(o) for o in tl], ('dog','cat','cat','cat','dog'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"deG1H8SzVkYt"},"source":["#Check only the training set is taken into account for setup\n","tl = TfmdLists(fns, [tcat,_lbl], splits=[[0,4], [1,2,3]])\n","test_eq(tcat.vocab, ['dog'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Brh5GYRbsW-"},"source":["#class A(Transform): \n","#    def encodes(self, x): return x \n","#    def decodes(self, x): return TitledInt(x) \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ROtE27pZe-Uu","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1601217861774,"user_tz":420,"elapsed":445,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"8533cfdc-6966-4a91-fe1b-62f7c4672e06"},"source":["print(A())\n","print(start)\n","print(NegTfm())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["A:\n","encodes: (object,object) -> encodes\n","decodes: (object,object) -> decodes\n","\n","tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n","        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n","        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])\n","NegTfm:\n","encodes: (object,object) -> encodes\n","decodes: (object,object) -> decodes\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xsLkOD7wVkYw"},"source":["# checking that split_idx of 1 does not work when TfmdLists has default split_idx of 0\n","tfm = NegTfm(split_idx=1)\n","tds = TfmdLists(start, A())\n","tdl = TfmdDL(tds, after_batch=tfm, bs=4)\n","x = tdl.one_batch()\n","# nothing happens below coz NegTfm is NOT executed\n","# since its split_idx of 1 is not a match\n","test_eq(x, torch.arange(4))\n","# make split idx match and now NegTfm is applied\n","tds.split_idx = 1\n","x = tdl.one_batch()\n","test_eq(x, -torch.arange(4))\n","# check again that with split_idx of - no Tfm applied\n","tds.split_idx = 0\n","x = tdl.one_batch()\n","test_eq(x, torch.arange(4))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_5gcqvq8VkYy"},"source":["tds = TfmdLists(start, A())\n","tdl = TfmdDL(tds, after_batch=NegTfm(), bs=4)\n","test_eq(tdl.dataset[0], start[0])\n","# note that len(tdl) is # of batches which is ((50)-1)//4 + 1 = 13\n","test_eq(len(tdl), (len(tds)-1)//4+1)\n","test_eq(tdl.bs, 4)\n","test_stdout(tdl.show_batch, '0\\n1\\n2\\n3')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-sDJit0SVkY0","outputId":"742a44f7-f0dd-4e99-d6f8-e0966c71e8ee"},"source":["show_doc(TfmdLists.subset)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/markdown":"<h4 id=\"TfmdLists.subset\" class=\"doc_header\"><code>TfmdLists.subset</code><a href=\"__main__.py#L21\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>TfmdLists.subset</code>(**`i`**)\n\nNew [`TfmdLists`](/data.core#TfmdLists) with same tfms that only includes items in `i`th split","text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"kuL8EQzVVkY3","outputId":"320f043e-963c-410f-d526-dc9dafae453f"},"source":["show_doc(TfmdLists.infer_idx)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/markdown":"<h4 id=\"TfmdLists.infer_idx\" class=\"doc_header\"><code>TfmdLists.infer_idx</code><a href=\"__main__.py#L43\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>TfmdLists.infer_idx</code>(**`x`**)\n\nFinds the index where `self.tfms` can be applied to `x`, depending on the type of `x`","text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"dy34UZIbVkY5","outputId":"ea0667e6-3862-4e78-99e1-018cd6193e28"},"source":["show_doc(TfmdLists.infer)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/markdown":"<h4 id=\"TfmdLists.infer\" class=\"doc_header\"><code>TfmdLists.infer</code><a href=\"__main__.py#L54\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>TfmdLists.infer</code>(**`x`**)\n\nApply `self.tfms` to `x` starting at the right tfm depending on the type of `x`","text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"vE9ac910fu8z"},"source":["def mult(x): return x*2\n","mult.order = 2\n","\n","fns = ['dog_0.jpg','cat_0.jpg','cat_2.jpg','cat_1.jpg','dog_1.jpg']\n","tl = TfmdLists(fns, [_lbl,_Cat(),mult])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pk8WDTrWf_5D","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1601217872175,"user_tz":420,"elapsed":808,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"3f9647fa-0193-4dfa-a7ad-5c9077ec667b"},"source":["# infer_idx finds index where self.tfms can be applied to x depending on type\n","# so type of x given is str so can apply from index 0\n","test_eq(tl.infer_idx('dog_45.jpg'), 0)\n","# Apply self.tfms starting at idx 0 implies apply _lbl get 'dog'\n","# Apply _Cat() get 1, then apply mult get 2.\n","test_eq(tl.infer('dog_45.jpg'), 2)\n","\n","print(tl.tfms)\n","print(tl.types)\n","# infex_idx of 4 is 2 coz type(4) is int which is at index 2\n","test_eq(tl.infer_idx(4), 2)\n","# infer says run tfms from that index forward to end on value x which is\n","# just mult 4 by 2 is 8\n","test_eq(tl.infer(4), 8)\n","\n","test_fail(lambda: tl.infer_idx(2.0))\n","test_fail(lambda: tl.infer(2.0))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Pipeline: _lbl -> _Cat -> mult\n","[<class 'str'>, <class 'fastai.torch_core.TitledStr'>, <class 'int'>, <class 'int'>]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XRc_O3pLgfnT"},"source":["'''\n","Init signature: TitledStr(*args, **kwargs)\n","Source:        \n","class TitledStr(Str, ShowTitle):\n","    _show_args = {'label': 'text'}\n","    def show(self, ctx=None, **kwargs):\n","        \"Show self\"\n","        return show_title(str(self), ctx=ctx, **merge(self._show_args, kwargs))\n","File:           /usr/local/lib/python3.6/dist-packages/fastai2/torch_core.py\n","Type:           type\n","'''\n","TitledStr??"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dg1AKJNxgi9c","colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"status":"ok","timestamp":1601217875292,"user_tz":420,"elapsed":561,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"d36cc4de-7b0c-4727-b120-8a8a41fefead"},"source":["# skr checks\n","print(fns)\n","print()\n","cat = _Cat()\n","tl1 = TfmdLists(fns, [cat])\n","print(tl1.types)\n","tl2 = TfmdLists(fns, [_lbl])\n","print(tl2.types)\n","tl3 = TfmdLists(fns, [mult]) \n","# whatever type(s) go(es) in come(s) out??\n","# but no output types considered per cat() example above??\n","print(tl3.types)\n","print()\n","tl4 = TfmdLists(fns, [_lbl, cat])\n","print(tl4.types)\n","tl5 = TfmdLists(fns, [_lbl, mult])\n","print(tl5.types)\n","tl6 = TfmdLists(fns, [cat, mult])\n","print(tl6.types)\n","print()\n","tl7 = TfmdLists(fns, [_lbl,cat,mult])\n","print(tl7.types)\n","print()\n","tl8 = TfmdLists(fns, [_lbl,cat,mult,mult])\n","print(tl8.types)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['dog_0.jpg', 'cat_0.jpg', 'cat_2.jpg', 'cat_1.jpg', 'dog_1.jpg']\n","\n","[<class 'str'>, <class 'int'>]\n","[<class 'str'>, <class 'fastai.torch_core.TitledStr'>]\n","[<class 'str'>, <class 'str'>]\n","\n","[<class 'str'>, <class 'fastai.torch_core.TitledStr'>, <class 'int'>]\n","[<class 'str'>, <class 'fastai.torch_core.TitledStr'>, <class 'fastai.torch_core.TitledStr'>]\n","[<class 'str'>, <class 'int'>, <class 'int'>]\n","\n","[<class 'str'>, <class 'fastai.torch_core.TitledStr'>, <class 'int'>, <class 'int'>]\n","\n","[<class 'str'>, <class 'fastai.torch_core.TitledStr'>, <class 'int'>, <class 'int'>, <class 'int'>]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TDWdNqF_gf3F","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601217878820,"user_tz":420,"elapsed":798,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"9f7c7dd3-f147-4a02-ca40-90490529b563"},"source":["#hide\n","#Test input_types works on a Transform\n","cat = _Cat()\n","cat.input_types = (str, float)\n","tl = TfmdLists(fns, [_lbl,cat,mult])\n","print(tl.types)\n","test_eq(tl.infer_idx(2.0), 1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[<class 'str'>, (<class 'str'>, <class 'float'>), <class 'int'>, <class 'int'>]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R8M9s2AngxeO","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601217880411,"user_tz":420,"elapsed":599,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"9b2fbc41-afc8-458f-a4e1-f5ae5c34b042"},"source":["#hide\n","#Test type annotations work on a function\n","def mult(x:(int,float)): return x*2\n","mult.order = 2\n","tl = TfmdLists(fns, [_lbl,_Cat(),mult])\n","print(tl.types)\n","test_eq(tl.infer_idx(2.0), 2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[<class 'str'>, <class 'fastai.torch_core.TitledStr'>, (<class 'int'>, <class 'float'>), <class 'int'>]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JK9ZoJ_4VkZD"},"source":["## Datasets -"]},{"cell_type":"code","metadata":{"id":"FJY1r0BzBfqT","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601219317457,"user_tz":420,"elapsed":540,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"f1e90f53-4923-4dd7-ead6-a8f73949fc23"},"source":["print(\"Start below\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Start below\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iLfzr5Rbg9aM"},"source":["'''\n","Signature: tuplify(o, use_list=False, match=None)\n","Source:   \n","def tuplify(o, use_list=False, match=None):\n","    \"Make `o` a tuple\"\n","    return tuple(L(o, use_list=use_list, match=match))\n","File:      /usr/local/lib/python3.6/dist-packages/fastcore/utils.py\n","Type:      function\n","'''\n","tuplify??"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XVDMtZhRVkZD"},"source":["#export\n","@docs\n","@delegates(TfmdLists)\n","class Datasets(FilteredBase):\n","    \"A dataset that creates a tuple from each `tfms`, passed through `item_tfms`\"\n","    def __init__(self, items=None, tfms=None, tls=None, n_inp=None, dl_type=None, **kwargs):\n","        super().__init__(dl_type=dl_type)\n","        # have a few TfmdLists, creates one TfmdLists for each list of tfms you pass in\n","        # so not one pipeline but n pipelines passed in, usually n is 2, a X (independent\n","        # variable) pipeline and a Y (dependent variable) pipeline. Same items but different\n","        # set of transforms.\n","        self.tls = L(tls if tls else [TfmdLists(items, t, **kwargs) for t in L(ifnone(tfms,[None]))])\n","        self.n_inp = ifnone(n_inp, max(1, len(self.tls)-1))\n","\n","    def __getitem__(self, it):\n","        res = tuple([tl[it] for tl in self.tls])\n","        return res if is_indexer(it) else list(zip(*res))\n","\n","    def __getattr__(self,k): return gather_attrs(self, k, 'tls')\n","    def __dir__(self): return super().__dir__() + gather_attr_names(self, 'tls')\n","    def __len__(self): return len(self.tls[0])\n","    def __iter__(self): return (self[i] for i in range(len(self)))\n","    def __repr__(self): return coll_repr(self)\n","    def decode(self, o, full=True): return tuple(tl.decode(o_, full=full) for o_,tl in zip(o,tuplify(self.tls, match=o)))\n","    def subset(self, i): return type(self)(tls=L(tl.subset(i) for tl in self.tls), n_inp=self.n_inp)\n","    def _new(self, items, *args, **kwargs): return super()._new(items, tfms=self.tfms, do_setup=False, **kwargs)\n","    def overlapping_splits(self): return self.tls[0].overlapping_splits()\n","    def new_empty(self): return type(self)(tls=[tl.new_empty() for tl in self.tls], n_inp=self.n_inp)\n","    @property\n","    def splits(self): return self.tls[0].splits\n","    @property\n","    def split_idx(self): return self.tls[0].tfms.split_idx\n","    @property\n","    def items(self): return self.tls[0].items\n","    @items.setter\n","    def items(self, v):\n","        for tl in self.tls: tl.items = v\n","\n","    def show(self, o, ctx=None, **kwargs):\n","        for o_,tl in zip(o,self.tls): ctx = tl.show(o_, ctx=ctx, **kwargs)\n","        return ctx\n","\n","    @contextmanager\n","    def set_split_idx(self, i):\n","        old_split_idx = self.split_idx\n","        for tl in self.tls: tl.tfms.split_idx = i\n","        try: yield self\n","        finally:\n","            for tl in self.tls: tl.tfms.split_idx = old_split_idx\n","\n","    _docs=dict(\n","        decode=\"Compose `decode` of all `tuple_tfms` then all `tfms` on `i`\",\n","        show=\"Show item `o` in `ctx`\",\n","        dataloaders=\"Get a `DataLoaders`\",\n","        overlapping_splits=\"All splits that are in more than one split\",\n","        subset=\"New `Datasets` that only includes subset `i`\",\n","        new_empty=\"Create a new empty version of the `self`, keeping only the transforms\",\n","        set_split_idx=\"Contextmanager to use the same `Datasets` with another `split_idx`\"\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FhXhxr_SVkZG"},"source":["A `Datasets` creates a tuple from `items` (typically input,target) by applying to them each list of `Transform` (or `Pipeline`) in `tfms`. Note that if `tfms` contains only one list of `tfms`, the items given by `Datasets` will be tuples of one element. \n","\n","`n_inp` is the number of elements in the tuples that should be considered part of the input and will default to 1 if `tfms` consists of one set of transforms, `len(tfms)-1` otherwise. In most cases, the number of elements in the tuples spit out by `Datasets` will be 2 (for input,target) but it can happen that there is 3 (Siamese networks or tabular data) in which case we need to be able to determine when the inputs end and the targets begin."]},{"cell_type":"code","metadata":{"id":"jr7whAxjVkZH","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601219921207,"user_tz":420,"elapsed":656,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"5576b00a-931d-4396-9632-15af2a818b08"},"source":["items = [1,2,3,4]\n","dsets = Datasets(items, [[neg_tfm,int2f_tfm], [add(1)]])\n","t = dsets[0] # t = 1\n","# Both pipes applied to t which is 1\n","test_eq(t, (-1,2))\n","test_eq(dsets[0,1,2], [(-1,2),(-2,3),(-3,4)])\n","test_eq(dsets.n_inp, 1)\n","# add has no decode so input is output so 2 \n","# and -1 goes through int2f_tfm decode so float -1.0 then neg_tfm\n","dsets.decode(t)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1.0, 2)"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"code","metadata":{"id":"IU4f7YcrVkZJ"},"source":["class Norm(Transform):\n","    def encodes(self, o): return (o-self.m)/self.s\n","    def decodes(self, o): return (o*self.s)+self.m\n","    def setups(self, items):\n","        its = tensor(items).float()\n","        self.m,self.s = its.mean(),its.std()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EsozxgJkVkZL","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1601219991617,"user_tz":420,"elapsed":816,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"70a67361-dfa7-4d9f-d7cb-a6d7a520fdb6"},"source":["items = [1,2,3,4]\n","nrm = Norm()\n","dsets = Datasets(items, [[neg_tfm,int2f_tfm], [neg_tfm,nrm]])\n","\n","print(nrm.m) # mean of [-1, -2, -3, -4] = tensor(-2.5)\n","print(nrm.s) # sd of [-1, -2, -3, -4] = tensor(1.29) \n","print()\n","\n","x,y = zip(*dsets)\n","print(x)\n","print(y)\n","\n","# since y is normed its mean should be close to 0, sd close to 1\n","test_close(tensor(y).mean(), 0)\n","test_close(tensor(y).std(), 1)\n","\n","# compare below to print value of x and print of nrm.m\n","test_eq(x, (-1,-2,-3,-4,))\n","test_eq(nrm.m, -2.5)\n","\n","# notice that y does not have any ability to show itself, only x does\n","# also if thing itself here for eg output of encodes is TitledInt is capable\n","# of showing itself it will show it; only if that is NOT possible, it will\n","# do the decode until you find something you can show approach. so here\n","# it ends up showing TitledInt of -2 = '-2\n","test_stdout(lambda:show_at(dsets, 1), '-2')\n","\n","# comparing attributes; also shows ways attributes can be accessed.\n","test_eq(dsets.m, nrm.m)\n","test_eq(dsets.norm.m, nrm.m)\n","test_eq(dsets.train.norm.m, nrm.m)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor(-2.5000)\n","tensor(1.2910)\n","\n","(-1, -2, -3, -4)\n","(tensor(1.1619), tensor(0.3873), tensor(-0.3873), tensor(-1.1619))\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vT5ACuJgVkZN"},"source":["#hide\n","#Check filtering is properly applied\n","class B(Transform):\n","    def encodes(self, x)->None:  return int(x+1)\n","    def decodes(self, x):        return TitledInt(x-1)\n","add1 = B(split_idx=1)\n","\n","# important to remember that splits are indices into the original items\n","dsets = Datasets(items, [neg_tfm, [neg_tfm,int2f_tfm,add1]], splits=[[3],[0,1,2]])\n","\n","# dsets index [1] implies value 2, add1 tfm will not match due to split_idx being 1 \n","# hence dsets[1] is [-2, -2]\n","test_eq(dsets[1], [-2,-2])\n","\n","# dsets.valid[1] is index 1 of dsets so -2 again but this index is match for add1\n","# hence -2, -1 result\n","test_eq(dsets.valid[1], [-2,-1])\n","test_eq(dsets.valid[[1,1]], [[-2,-1], [-2,-1]])\n","\n","# dsets.train[0] is dsets index 3, which is value 4.\n","test_eq(dsets.train[0], [-4,-4])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cHXN31MPVkZP"},"source":["test_fns = ['dog_0.jpg','cat_0.jpg','cat_2.jpg','cat_1.jpg','kid_1.jpg']\n","tcat = _Cat()\n","# note due to order that even though below is [tcat,_lbl] it will execute\n","# _lbl, tcat as _lbl order is 0 as it is default, while _Cat() order is 1 (speced above)\n","dsets = Datasets(test_fns, [[tcat,_lbl]], splits=[[0,1,2], [3,4]])\n","test_eq(tcat.vocab, ['cat','dog'])\n","test_eq(dsets.train, [(1,),(0,),(0,)])\n","test_eq(dsets.valid[0], (0,))\n","test_stdout(lambda: show_at(dsets.train, 0), \"dog\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YuP4Rxb9VkZW"},"source":["inp = [0,1,2,3,4]\n","dsets = Datasets(inp, tfms=[None])\n","\n","test_eq(*dsets[2], 2)          # Retrieve one item (subset 0 is the default)\n","test_eq(dsets[1,2], [(1,),(2,)])    # Retrieve two items by index\n","mask = [True,False,False,True,False]\n","test_eq(dsets[mask], [(0,),(3,)])   # Retrieve two items by mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pPg-gHMyVkZe"},"source":["inp = pd.DataFrame(dict(a=[5,1,2,3,4]))\n","dsets = Datasets(inp, tfms=attrgetter('a')).subset(0)\n","test_eq(*dsets[2], 2)          # Retrieve one item (subset 0 is the default)\n","test_eq(dsets[1,2], [(1,),(2,)])    # Retrieve two items by index\n","mask = [True,False,False,True,False]\n","test_eq(dsets[mask], [(5,),(3,)])   # Retrieve two items by mask"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cRN8MS_-ibnP"},"source":["Recall: n_inp is the number of elements in the tuples that should be considered part of the input and will default to 1 if tfms consists of one set of transforms, len(tfms)-1 otherwise"]},{"cell_type":"code","metadata":{"id":"GfABHH5IVkZh"},"source":["#test n_inp\n","inp = [0,1,2,3,4]\n","dsets = Datasets(inp, tfms=[None])\n","\n","# check default is 1\n","test_eq(dsets.n_inp, 1)\n","\n","# if not specified check n_inp is len(tfms)-1\n","dsets = Datasets(inp, tfms=[[None],[None],[None]])\n","test_eq(dsets.n_inp, 2)\n","\n","# if specified use n_imp and not len(tfms)-1\n","dsets = Datasets(inp, tfms=[[None],[None],[None]], n_inp=1)\n","test_eq(dsets.n_inp, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IdpoICWAVkZj","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601221227944,"user_tz":420,"elapsed":560,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"5408bc91-7afb-4bd8-ddbc-d104c683ddb6"},"source":["# splits can be tensors indices\n","dsets = Datasets(range(5), tfms=[None], splits=[tensor([0,2]), [1,3,4]])\n","\n","test_eq(dsets.subset(0), [(0,),(2,)])\n","test_eq(dsets.train, [(0,),(2,)])       # Subset 0 is aliased to `train`\n","test_eq(dsets.subset(1), [(1,),(3,),(4,)])\n","test_eq(dsets.valid, [(1,),(3,),(4,)])     # Subset 1 is aliased to `valid`\n","test_eq(*dsets.valid[2], 4)\n","#assert '[(1,),(3,),(4,)]' in str(dsets) and '[(0,),(2,)]' in str(dsets)\n","dsets"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(#5) [(0,),(1,),(2,),(3,),(4,)]"]},"metadata":{"tags":[]},"execution_count":78}]},{"cell_type":"code","metadata":{"id":"9vt6ewLfVkZm"},"source":["# splits can be boolean masks (they don't have to cover all items, but must be disjoint)\n","# Seems to imply mask cannot be True in both (ensuring disjoint) \n","# but can be False in both (not covering all items) - see index 3 False in both\n","splits = [[False,True,True,False,True], [True,False,False,False,False]]\n","dsets = Datasets(range(5), tfms=[None], splits=splits)\n","\n","test_eq(dsets.train, [(1,),(2,),(4,)])\n","test_eq(dsets.valid, [(0,)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rRt6MQpLVkZq"},"source":["# apply transforms to all items\n","tfm = [[lambda x: x*2,lambda x: x+1]]\n","splits = [[1,2],[0,3,4]]\n","dsets = Datasets(range(5), tfm, splits=splits)\n","# so below is values at index 1 and 2 which are 1,2\n","# 1*2+1, 2*2+1 hence 3,5\n","test_eq(dsets.train,[(3,),(5,)])\n","\n","# similar to above for val at indices 0,3,4\n","test_eq(dsets.valid,[(1,),(7,),(9,)])\n","\n","# use mask\n","test_eq(dsets.train[False,True], [(5,)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fCWZ7B2TVkZs"},"source":["# only transform subset 1\n","class _Tfm(Transform):\n","    split_idx=1\n","    def encodes(self, x): return x*2\n","    def decodes(self, x): return TitledStr(x//2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OtfchtkFVkZu","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601222000970,"user_tz":420,"elapsed":530,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"61d06021-1d06-466f-b00c-4970e57154bf"},"source":["dsets = Datasets(range(5), [_Tfm()], splits=[[1,2],[0,3,4]])\n","\n","# since split_idx is 1 for _Tfm() no match on training any index\n","# hence below returns orig values of dsets.train at indices 1,2\n","test_eq(dsets.train,[(1,),(2,)])\n","\n","# since split_idx is matched values of dsets.valid at indices\n","# 0,3,4 which are values 0,3,4 are *2 and hence\n","test_eq(dsets.valid,[(0,),(6,),(8,)])\n","\n","# mask on no returned values still works\n","test_eq(dsets.train[False,True], [(2,)])\n","dsets"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(#5) [(0,),(1,),(2,),(3,),(4,)]"]},"metadata":{"tags":[]},"execution_count":82}]},{"cell_type":"code","metadata":{"id":"lXTRC-ESVkZw"},"source":["#A context manager to change the split_idx and apply the validation transform on the training set\n","# So can use context mgr to change split_idx within context\n","# Notice that only within context mgr split_idx of ds.train is set to 1 not outside - so within context mgr\n","# since split_idx matches so dsets.train has\n","ds = dsets.train\n","with ds.set_split_idx(1):\n","    test_eq(ds,[(2,),(4,)])\n","test_eq(dsets.train,[(1,),(2,)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uP-RerubVkZz"},"source":["#hide\n","#Test Datasets pickles\n","dsrc1 = pickle.loads(pickle.dumps(dsets))\n","test_eq(dsets.train, dsrc1.train)\n","test_eq(dsets.valid, dsrc1.valid)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CIWgPt4DVkZ8"},"source":["dsets = Datasets(range(5), [_Tfm(),noop], splits=[[1,2],[0,3,4]])\n","# _Tfm() only if idx matches which it does not but noop on indices 1 and 2 returning tuples\n","test_eq(dsets.train,[(1,1),(2,2)])\n","\n","# below seems like you passed in two pipelines one of _Tfm() and another noop ???\n","# see skr test below\n","test_eq(dsets.valid,[(0,0),(6,3),(8,4)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mSV6sQUpkCsQ"},"source":["# skr test\n","dsets1 = Datasets(range(5), [[_Tfm()], [noop]], splits=[[1,2],[0,3,4]])\n","test_eq(dsets.train,[(1,1),(2,2)])\n","test_eq(dsets1.valid,[(0,0),(6,3),(8,4)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kt61ZzYy11bP"},"source":["#class A(Transform): \n","#    def encodes(self, x): return x \n","#    def decodes(self, x): return TitledInt(x) \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bvtAH1GQVkaC","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1601223404989,"user_tz":420,"elapsed":539,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"ebf6114a-dc87-440a-a4df-a7e447803515"},"source":["start = torch.arange(0,50)\n","tds = Datasets(start, [A()])\n","tdl = TfmdDL(tds, after_item=NegTfm(), bs=4)\n","print(L(tdl))\n","print(b)\n","b = tdl.one_batch()\n","# Since neg negates on encodes and also on encodes returns same value after decodes\n","test_eq(tdl.decode_batch(b), ((0,),(1,),(2,),(3,)))\n","test_stdout(tdl.show_batch, \"0\\n1\\n2\\n3\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(#13) [(tensor([ 0, -1, -2, -3]),),(tensor([-4, -5, -6, -7]),),(tensor([ -8,  -9, -10, -11]),),(tensor([-12, -13, -14, -15]),),(tensor([-16, -17, -18, -19]),),(tensor([-20, -21, -22, -23]),),(tensor([-24, -25, -26, -27]),),(tensor([-28, -29, -30, -31]),),(tensor([-32, -33, -34, -35]),),(tensor([-36, -37, -38, -39]),)...]\n","(tensor([ 0, -1, -2, -3]),)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hsamnKaJVkaI"},"source":["# only transform subset 1\n","class _Tfm(Transform):\n","    split_idx=1\n","    def encodes(self, x): return x*2\n","\n","dsets = Datasets(range(8), [None], splits=[[1,2,5,7],[0,3,4,6]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jRuoOHMVVkaK"},"source":["# only transform subset 1\n","class _Tfm(Transform):\n","    split_idx=1\n","    def encodes(self, x): return x*2\n","\n","dsets = Datasets(range(8), [None], splits=[[1,2,5,7],[0,3,4,6]])\n","dls = dsets.dataloaders(bs=4, after_batch=_Tfm(), shuffle_train=False, device=torch.device('cpu'))\n","test_eq(dls.train, [(tensor([1,2,5, 7]),)])\n","test_eq(dls.valid, [(tensor([0,6,8,12]),)])\n","test_eq(dls.n_inp, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lnBxHACwVkaN"},"source":["### Methods"]},{"cell_type":"code","metadata":{"id":"PpDUMBguVkaN"},"source":["items = [1,2,3,4]\n","dsets = Datasets(items, [[neg_tfm,int2f_tfm]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I1m3w-nlVkaR","colab":{"base_uri":"https://localhost:8080/","height":197},"executionInfo":{"status":"error","timestamp":1601224164909,"user_tz":420,"elapsed":7798,"user":{"displayName":"Srinivas Raman","photoUrl":"","userId":"10288249463255174028"}},"outputId":"1d53cd56-48e4-47f5-8dd1-1ed5e43ef547"},"source":["#hide_input\n","_dsrc = Datasets([1,2])\n","show_doc(_dsrc.dataloaders, name=\"Datasets.dataloaders\")"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-98-cb314977cca1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#hide_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0m_dsrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mshow_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_dsrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Datasets.dataloaders\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'show_doc' is not defined"]}]},{"cell_type":"code","metadata":{"id":"GYdkVqm_VkaT","outputId":"7f1755fd-f016-430a-8c82-1f0d2f0acad5"},"source":["show_doc(Datasets.decode)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/markdown":"<h4 id=\"Datasets.decode\" class=\"doc_header\"><code>Datasets.decode</code><a href=\"__main__.py#L20\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>Datasets.decode</code>(**`o`**, **`full`**=*`True`*)\n\nCompose `decode` of all `tuple_tfms` then all `tfms` on `i`","text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"oq-7ZEGuVkaV"},"source":["test_eq(*dsets[0], -1)\n","test_eq(*dsets.decode((-1,)), 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"11ekiXhvVkaX","outputId":"602d9cf8-27a4-4149-e659-125167b77e05"},"source":["show_doc(Datasets.show)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/markdown":"<h4 id=\"Datasets.show\" class=\"doc_header\"><code>Datasets.show</code><a href=\"__main__.py#L35\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>Datasets.show</code>(**`o`**, **`ctx`**=*`None`*, **\\*\\*`kwargs`**)\n\nShow item `o` in `ctx`","text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"Z8HsekSfVkaa"},"source":["test_stdout(lambda:dsets.show(dsets[1]), '-2')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BUyZViINVkab","outputId":"e7bbdbfb-0293-43f3-94bc-c378de71146c"},"source":["show_doc(Datasets.new_empty)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/markdown":"<h4 id=\"Datasets.new_empty\" class=\"doc_header\"><code>Datasets.new_empty</code><a href=\"__main__.py#L24\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>Datasets.new_empty</code>()\n\nCreate a new empty version of the `self`, keeping only the transforms","text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"QOZCvZ7KVkad"},"source":["items = [1,2,3,4]\n","nrm = Norm()\n","dsets = Datasets(items, [[neg_tfm,int2f_tfm], [neg_tfm]])\n","empty = dsets.new_empty()\n","test_eq(empty.items, [])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lEEo96ctVkah"},"source":["#hide\n","#test it works for dataframes too\n","df = pd.DataFrame({'a':[1,2,3,4,5], 'b':[6,7,8,9,10]})\n","dsets = Datasets(df, [[attrgetter('a')], [attrgetter('b')]])\n","empty = dsets.new_empty()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yHVsNeonVkaj"},"source":["## Add test set for inference"]},{"cell_type":"code","metadata":{"id":"-7yG8tnd3UuJ"},"source":["# only transform subset 1\n","#class _Tfm(Transform):\n","#    split_idx=1\n","#    def encodes(self, x): return x*2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X6EJyBF9Vkaj"},"source":["# only transform subset 0\n","class _Tfm1(Transform):\n","    split_idx=0\n","    def encodes(self, x): return x*3\n","\n","dsets = Datasets(range(8), [[_Tfm(),_Tfm1()]], splits=[[1,2,5,7],[0,3,4,6]])\n","\n","# execute appropriate transforms from pipeline where split_idx matches\n","# so _Tmf1() for training set since split_idx=0 matches training set idx which is 0\n","test_eq(dsets.train, [(3,),(6,),(15,),(21,)])\n","\n","# so _Tfm() for valid set since split_idx=1 matches valid set idx which is 1\n","test_eq(dsets.valid, [(0,),(6,),(8,),(12,)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ngY29IyCVkal"},"source":["#export\n","def test_set(dsets, test_items, rm_tfms=None, with_labels=False):\n","    \"Create a test set from `test_items` using validation transforms of `dsets`\"\n","    if isinstance(dsets, Datasets):\n","        tls = dsets.tls if with_labels else dsets.tls[:dsets.n_inp]\n","        test_tls = [tl._new(test_items, split_idx=1) for tl in tls]\n","        if rm_tfms is None: rm_tfms = [tl.infer_idx(get_first(test_items)) for tl in test_tls]\n","        else:               rm_tfms = tuplify(rm_tfms, match=test_tls)\n","        for i,j in enumerate(rm_tfms): test_tls[i].tfms.fs = test_tls[i].tfms.fs[j:]\n","        return Datasets(tls=test_tls)\n","    elif isinstance(dsets, TfmdLists):\n","        test_tl = dsets._new(test_items, split_idx=1)\n","        if rm_tfms is None: rm_tfms = dsets.infer_idx(get_first(test_items))\n","        test_tl.tfms.fs = test_tl.tfms.fs[rm_tfms:]\n","        return test_tl\n","    else: raise Exception(f\"This method requires using the fastai library to assemble your data. Expected a `Datasets` or a `TfmdLists` but got {dsets.__class__.__name__}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gxCuEqVOVkan"},"source":["class _Tfm1(Transform):\n","    split_idx=0\n","    def encodes(self, x): return x*3\n","\n","dsets = Datasets(range(8), [[_Tfm(),_Tfm1()]], splits=[[1,2,5,7],[0,3,4,6]])\n","test_eq(dsets.train, [(3,),(6,),(15,),(21,)])\n","test_eq(dsets.valid, [(0,),(6,),(8,),(12,)])\n","\n","#Tranform of the validation set are applied and you have to provide test_items\n","tst = test_set(dsets, [9,10,11])\n","test_eq(tst, [(18,),(20,),(22,)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xo-6SBr9Vkas"},"source":["#hide\n","#Test with different types\n","tfm = _Tfm1()\n","tfm.split_idx,tfm.order = None,2\n","dsets = Datasets(['dog', 'cat', 'cat', 'dog'], [[_Cat(),tfm]])\n","\n","#With strings\n","# 3*1 for dog, 3*0 for cat\n","test_eq(test_set(dsets, ['dog', 'cat', 'cat']), [(3,), (0,), (0,)])\n","#With ints\n","test_eq(test_set(dsets, [1,2]), [(3,), (6,)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zk60CuUIVkav"},"source":["#hide\n","#Test with various input lengths\n","dsets = Datasets(range(8), [[_Tfm(),_Tfm1()],[_Tfm(),_Tfm1()],[_Tfm(),_Tfm1()]], splits=[[1,2,5,7],[0,3,4,6]])\n","tst = test_set(dsets, [1,2,3])\n","# Here you get a tuple coz without n_inp defined it is now len(tfms)-1 = 2 hence a tuple with 2 values\n","test_eq(tst, [(2,2),(4,4),(6,6)])\n","\n","dsets = Datasets(range(8), [[_Tfm(),_Tfm1()],[_Tfm(),_Tfm1()],[_Tfm(),_Tfm1()]], splits=[[1,2,5,7],[0,3,4,6]], n_inp=1)\n","tst = test_set(dsets, [1,2,3])\n","# Here n_inp is explicitly one so you only get a single valued tuple as output\n","test_eq(tst, [(2,),(4,),(6,)])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eCv2_F43lEg5"},"source":["Note: rm_tfms is defined with test_set. See above."]},{"cell_type":"code","metadata":{"id":"zKE2W2y2Vkax"},"source":["#hide\n","#Test with rm_tfms\n","dsets = Datasets(range(8), [[_Tfm(),_Tfm()]], splits=[[1,2,5,7],[0,3,4,6]])\n","tst = test_set(dsets, [1,2,3])\n","test_eq(tst, [(4,),(8,),(12,)])\n","\n","dsets = Datasets(range(8), [[_Tfm(),_Tfm()]], splits=[[1,2,5,7],[0,3,4,6]])\n","\n","# seeing code for rm_tfms in test_set seems remove tfms starting at index specified\n","# in rm_tfms if multiple pipeline specified in tuple\n","tst = test_set(dsets, [1,2,3], rm_tfms=1)\n","test_eq(tst, [(2,),(4,),(6,)])\n","\n","dsets = Datasets(range(8), [[_Tfm(),_Tfm()], [_Tfm(),_Tfm()]], splits=[[1,2,5,7],[0,3,4,6]], n_inp=2)\n","\n","# use of rm_tfms tuples says remove tfms starting at index 1 in 1st pipeline and 0 imply no changes to second pipeline.\n","tst = test_set(dsets, [1,2,3], rm_tfms=(1,0))\n","test_eq(tst, [(2,4),(4,8),(6,12)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j2dRtXB9Vkaz"},"source":["#export\n","@delegates(TfmdDL.__init__)\n","@patch\n","def test_dl(self:DataLoaders, test_items, rm_type_tfms=None, with_labels=False, **kwargs):\n","    \"Create a test dataloader from `test_items` using validation transforms of `dls`\"\n","    test_ds = test_set(self.valid_ds, test_items, rm_tfms=rm_type_tfms, with_labels=with_labels\n","                      ) if isinstance(self.valid_ds, (Datasets, TfmdLists)) else test_items\n","    return self.valid.new(test_ds, **kwargs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sQXfo5vtVka1"},"source":["dsets = Datasets(range(8), [[_Tfm(),_Tfm1()]], splits=[[1,2,5,7],[0,3,4,6]])\n","dls = dsets.dataloaders(bs=4, device=torch.device('cpu'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Btdy1B6Vka3"},"source":["dsets = Datasets(range(8), [[_Tfm(),_Tfm1()]], splits=[[1,2,5,7],[0,3,4,6]])\n","dls = dsets.dataloaders(bs=4, device=torch.device('cpu'))\n","tst_dl = dls.test_dl([2,3,4,5])\n","test_eq(tst_dl._n_inp, 1)\n","test_eq(list(tst_dl), [(tensor([ 4,  6,  8, 10]),)])\n","#Test you can change transforms\n","tst_dl = dls.test_dl([2,3,4,5], after_item=add1)\n","test_eq(list(tst_dl), [(tensor([ 5,  7,  9, 11]),)])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U532_enOVka5"},"source":["## Export -"]},{"cell_type":"code","metadata":{"id":"T6xrxph7Vka5","outputId":"fec711b3-ea10-4441-8c00-4b0bad4ea3ba"},"source":["#hide\n","from nbdev.export import notebook2script\n","notebook2script()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Converted 00_torch_core.ipynb.\n","Converted 01_layers.ipynb.\n","Converted 02_data.load.ipynb.\n","Converted 03_data.core.ipynb.\n","Converted 04_data.external.ipynb.\n","Converted 05_data.transforms.ipynb.\n","Converted 06_data.block.ipynb.\n","Converted 07_vision.core.ipynb.\n","Converted 08_vision.data.ipynb.\n","Converted 09_vision.augment.ipynb.\n","Converted 09b_vision.utils.ipynb.\n","Converted 09c_vision.widgets.ipynb.\n","Converted 10_tutorial.pets.ipynb.\n","Converted 11_vision.models.xresnet.ipynb.\n","Converted 12_optimizer.ipynb.\n","Converted 13_callback.core.ipynb.\n","Converted 13a_learner.ipynb.\n","Converted 13b_metrics.ipynb.\n","Converted 14_callback.schedule.ipynb.\n","Converted 14a_callback.data.ipynb.\n","Converted 15_callback.hook.ipynb.\n","Converted 15a_vision.models.unet.ipynb.\n","Converted 16_callback.progress.ipynb.\n","Converted 17_callback.tracker.ipynb.\n","Converted 18_callback.fp16.ipynb.\n","Converted 18a_callback.training.ipynb.\n","Converted 19_callback.mixup.ipynb.\n","Converted 20_interpret.ipynb.\n","Converted 20a_distributed.ipynb.\n","Converted 21_vision.learner.ipynb.\n","Converted 22_tutorial.imagenette.ipynb.\n","Converted 23_tutorial.vision.ipynb.\n","Converted 24_tutorial.siamese.ipynb.\n","Converted 24_vision.gan.ipynb.\n","Converted 30_text.core.ipynb.\n","Converted 31_text.data.ipynb.\n","Converted 32_text.models.awdlstm.ipynb.\n","Converted 33_text.models.core.ipynb.\n","Converted 34_callback.rnn.ipynb.\n","Converted 35_tutorial.wikitext.ipynb.\n","Converted 36_text.models.qrnn.ipynb.\n","Converted 37_text.learner.ipynb.\n","Converted 38_tutorial.text.ipynb.\n","Converted 39_tutorial.transformers.ipynb.\n","Converted 40_tabular.core.ipynb.\n","Converted 41_tabular.data.ipynb.\n","Converted 42_tabular.model.ipynb.\n","Converted 43_tabular.learner.ipynb.\n","Converted 44_tutorial.tabular.ipynb.\n","Converted 45_collab.ipynb.\n","Converted 46_tutorial.collab.ipynb.\n","Converted 50_tutorial.datablock.ipynb.\n","Converted 60_medical.imaging.ipynb.\n","Converted 61_tutorial.medical_imaging.ipynb.\n","Converted 65_medical.text.ipynb.\n","Converted 70_callback.wandb.ipynb.\n","Converted 71_callback.tensorboard.ipynb.\n","Converted 72_callback.neptune.ipynb.\n","Converted 73_callback.captum.ipynb.\n","Converted 74_callback.cutmix.ipynb.\n","Converted 97_test_utils.ipynb.\n","Converted 99_pytorch_doc.ipynb.\n","Converted index.ipynb.\n","Converted tutorial.ipynb.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WuK0kF_MVka7"},"source":[""],"execution_count":null,"outputs":[]}]}