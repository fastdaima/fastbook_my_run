{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "jupytext": {
      "split_at_heading": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "walk_03_data.core.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgOAH7O8aZWJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c0f654f0-5b3a-4bfb-dac2-dd933396c8d7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0omRi-SaxFc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "21ac3079-8739-4b21-ad80-09ab1138d564"
      },
      "source": [
        "%cd drive/'My Drive'/'Colab Notebooks'/\n",
        "# Only once clone fastai2 repository in this directory\n",
        "# git clone https://github.com/fastai/fastai2.git\n",
        "!wget -c https://repo.continuum.io/archive/Anaconda3-5.1.0-Linux-x86_64.sh\n",
        "!chmod +x Anaconda3-5.1.0-Linux-x86_64.sh\n",
        "!bash ./Anaconda3-5.1.0-Linux-x86_64.sh -b -f -p /usr/local\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.6/site-packages')\n",
        "%cd fastai2\n",
        "!conda env create -f environment.yml\n",
        "!source activate fastai2\n",
        "!pip install -e \".[dev]\"\n",
        "!pip install nbdev"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n",
            "--2020-07-12 12:38:22--  https://repo.continuum.io/archive/Anaconda3-5.1.0-Linux-x86_64.sh\n",
            "Resolving repo.continuum.io (repo.continuum.io)... 104.18.200.79, 104.18.201.79, 2606:4700::6812:c94f, ...\n",
            "Connecting to repo.continuum.io (repo.continuum.io)|104.18.200.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://repo.anaconda.com/archive/Anaconda3-5.1.0-Linux-x86_64.sh [following]\n",
            "--2020-07-12 12:38:22--  https://repo.anaconda.com/archive/Anaconda3-5.1.0-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8303, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "PREFIX=/usr/local\n",
            "installing: python-3.6.4-hc3d631a_1 ...\n",
            "Python 3.6.4 :: Anaconda, Inc.\n",
            "installing: ca-certificates-2017.08.26-h1d4fec5_0 ...\n",
            "installing: conda-env-2.6.0-h36134e3_1 ...\n",
            "installing: intel-openmp-2018.0.0-hc7b2577_8 ...\n",
            "installing: libgcc-ng-7.2.0-h7cc24e2_2 ...\n",
            "installing: libgfortran-ng-7.2.0-h9f7466a_2 ...\n",
            "installing: libstdcxx-ng-7.2.0-h7a57d05_2 ...\n",
            "installing: bzip2-1.0.6-h9a117a8_4 ...\n",
            "installing: expat-2.2.5-he0dffb1_0 ...\n",
            "installing: gmp-6.1.2-h6c8ec71_1 ...\n",
            "installing: graphite2-1.3.10-hf63cedd_1 ...\n",
            "installing: icu-58.2-h9c2bf20_1 ...\n",
            "installing: jbig-2.1-hdba287a_0 ...\n",
            "installing: jpeg-9b-h024ee3a_2 ...\n",
            "installing: libffi-3.2.1-hd88cf55_4 ...\n",
            "installing: libsodium-1.0.15-hf101ebd_0 ...\n",
            "installing: libtool-2.4.6-h544aabb_3 ...\n",
            "installing: libxcb-1.12-hcd93eb1_4 ...\n",
            "installing: lzo-2.10-h49e0be7_2 ...\n",
            "installing: mkl-2018.0.1-h19d6760_4 ...\n",
            "installing: ncurses-6.0-h9df7e31_2 ...\n",
            "installing: openssl-1.0.2n-hb7f436b_0 ...\n",
            "installing: patchelf-0.9-hf79760b_2 ...\n",
            "installing: pcre-8.41-hc27e229_1 ...\n",
            "installing: pixman-0.34.0-hceecf20_3 ...\n",
            "installing: tk-8.6.7-hc745277_3 ...\n",
            "installing: unixodbc-2.3.4-hc36303a_1 ...\n",
            "installing: xz-5.2.3-h55aa19d_2 ...\n",
            "installing: yaml-0.1.7-had09818_2 ...\n",
            "installing: zlib-1.2.11-ha838bed_2 ...\n",
            "installing: glib-2.53.6-h5d9569c_2 ...\n",
            "installing: hdf5-1.10.1-h9caa474_1 ...\n",
            "installing: libedit-3.1-heed3624_0 ...\n",
            "installing: libpng-1.6.34-hb9fc6fc_0 ...\n",
            "installing: libssh2-1.8.0-h9cfc8f7_4 ...\n",
            "installing: libtiff-4.0.9-h28f6b97_0 ...\n",
            "installing: libxml2-2.9.7-h26e45fe_0 ...\n",
            "installing: mpfr-3.1.5-h11a74b3_2 ...\n",
            "installing: pandoc-1.19.2.1-hea2e7c5_1 ...\n",
            "installing: readline-7.0-ha6073c6_4 ...\n",
            "installing: zeromq-4.2.2-hbedb6e5_2 ...\n",
            "installing: dbus-1.12.2-hc3f9b76_1 ...\n",
            "installing: freetype-2.8-hab7d2ae_1 ...\n",
            "installing: gstreamer-1.12.4-hb53b477_0 ...\n",
            "installing: libcurl-7.58.0-h1ad7b7a_0 ...\n",
            "installing: libxslt-1.1.32-h1312cb7_0 ...\n",
            "installing: mpc-1.0.3-hec55b23_5 ...\n",
            "installing: sqlite-3.22.0-h1bed415_0 ...\n",
            "installing: curl-7.58.0-h84994c4_0 ...\n",
            "installing: fontconfig-2.12.4-h88586e7_1 ...\n",
            "installing: gst-plugins-base-1.12.4-h33fb286_0 ...\n",
            "installing: alabaster-0.7.10-py36h306e16b_0 ...\n",
            "installing: asn1crypto-0.24.0-py36_0 ...\n",
            "installing: attrs-17.4.0-py36_0 ...\n",
            "installing: backports-1.0-py36hfa02d7e_1 ...\n",
            "installing: beautifulsoup4-4.6.0-py36h49b8c8c_1 ...\n",
            "installing: bitarray-0.8.1-py36h14c3975_1 ...\n",
            "installing: boto-2.48.0-py36h6e4cd66_1 ...\n",
            "installing: cairo-1.14.12-h77bcde2_0 ...\n",
            "installing: certifi-2018.1.18-py36_0 ...\n",
            "installing: chardet-3.0.4-py36h0f667ec_1 ...\n",
            "installing: click-6.7-py36h5253387_0 ...\n",
            "installing: cloudpickle-0.5.2-py36_1 ...\n",
            "installing: colorama-0.3.9-py36h489cec4_0 ...\n",
            "installing: contextlib2-0.5.5-py36h6c84a62_0 ...\n",
            "installing: dask-core-0.16.1-py36_0 ...\n",
            "installing: decorator-4.2.1-py36_0 ...\n",
            "installing: docutils-0.14-py36hb0f60f5_0 ...\n",
            "installing: entrypoints-0.2.3-py36h1aec115_2 ...\n",
            "installing: et_xmlfile-1.0.1-py36hd6bccc3_0 ...\n",
            "installing: fastcache-1.0.2-py36h14c3975_2 ...\n",
            "installing: filelock-2.0.13-py36h646ffb5_0 ...\n",
            "installing: glob2-0.6-py36he249c77_0 ...\n",
            "installing: gmpy2-2.0.8-py36hc8893dd_2 ...\n",
            "installing: greenlet-0.4.12-py36h2d503a6_0 ...\n",
            "installing: heapdict-1.0.0-py36_2 ...\n",
            "installing: idna-2.6-py36h82fb2a8_1 ...\n",
            "installing: imagesize-0.7.1-py36h52d8127_0 ...\n",
            "installing: ipython_genutils-0.2.0-py36hb52b0d5_0 ...\n",
            "installing: itsdangerous-0.24-py36h93cc618_1 ...\n",
            "installing: jdcal-1.3-py36h4c697fb_0 ...\n",
            "installing: lazy-object-proxy-1.3.1-py36h10fcdad_0 ...\n",
            "installing: llvmlite-0.21.0-py36ha241eea_0 ...\n",
            "installing: locket-0.2.0-py36h787c0ad_1 ...\n",
            "installing: lxml-4.1.1-py36hf71bdeb_1 ...\n",
            "installing: markupsafe-1.0-py36hd9260cd_1 ...\n",
            "installing: mccabe-0.6.1-py36h5ad9710_1 ...\n",
            "installing: mistune-0.8.3-py36_0 ...\n",
            "installing: mkl-service-1.1.2-py36h17a0993_4 ...\n",
            "installing: mpmath-1.0.0-py36hfeacd6b_2 ...\n",
            "installing: msgpack-python-0.5.1-py36h6bb024c_0 ...\n",
            "installing: multipledispatch-0.4.9-py36h41da3fb_0 ...\n",
            "installing: numpy-1.14.0-py36h3dfced4_1 ...\n",
            "installing: olefile-0.45.1-py36_0 ...\n",
            "installing: pandocfilters-1.4.2-py36ha6701b7_1 ...\n",
            "installing: parso-0.1.1-py36h35f843b_0 ...\n",
            "installing: path.py-10.5-py36h55ceabb_0 ...\n",
            "installing: pep8-1.7.1-py36_0 ...\n",
            "installing: pickleshare-0.7.4-py36h63277f8_0 ...\n",
            "installing: pkginfo-1.4.1-py36h215d178_1 ...\n",
            "installing: pluggy-0.6.0-py36hb689045_0 ...\n",
            "installing: ply-3.10-py36hed35086_0 ...\n",
            "installing: psutil-5.4.3-py36h14c3975_0 ...\n",
            "installing: ptyprocess-0.5.2-py36h69acd42_0 ...\n",
            "installing: py-1.5.2-py36h29bf505_0 ...\n",
            "installing: pycodestyle-2.3.1-py36hf609f19_0 ...\n",
            "installing: pycosat-0.6.3-py36h0a5515d_0 ...\n",
            "installing: pycparser-2.18-py36hf9f622e_1 ...\n",
            "installing: pycrypto-2.6.1-py36h14c3975_7 ...\n",
            "installing: pycurl-7.43.0.1-py36hb7f436b_0 ...\n",
            "installing: pyodbc-4.0.22-py36hf484d3e_0 ...\n",
            "installing: pyparsing-2.2.0-py36hee85983_1 ...\n",
            "installing: pysocks-1.6.7-py36hd97a5b1_1 ...\n",
            "installing: pytz-2017.3-py36h63b9c63_0 ...\n",
            "installing: pyyaml-3.12-py36hafb9ca4_1 ...\n",
            "installing: pyzmq-16.0.3-py36he2533c7_0 ...\n",
            "installing: qt-5.6.2-h974d657_12 ...\n",
            "installing: qtpy-1.3.1-py36h3691cc8_0 ...\n",
            "installing: rope-0.10.7-py36h147e2ec_0 ...\n",
            "installing: ruamel_yaml-0.15.35-py36h14c3975_1 ...\n",
            "installing: send2trash-1.4.2-py36_0 ...\n",
            "installing: simplegeneric-0.8.1-py36_2 ...\n",
            "installing: sip-4.18.1-py36h51ed4ed_2 ...\n",
            "installing: six-1.11.0-py36h372c433_1 ...\n",
            "installing: snowballstemmer-1.2.1-py36h6febd40_0 ...\n",
            "installing: sortedcontainers-1.5.9-py36_0 ...\n",
            "installing: sphinxcontrib-1.0-py36h6d0f590_1 ...\n",
            "installing: sqlalchemy-1.2.1-py36h14c3975_0 ...\n",
            "installing: tblib-1.3.2-py36h34cf8b6_0 ...\n",
            "installing: testpath-0.3.1-py36h8cadb63_0 ...\n",
            "installing: toolz-0.9.0-py36_0 ...\n",
            "installing: tornado-4.5.3-py36_0 ...\n",
            "installing: typing-3.6.2-py36h7da032a_0 ...\n",
            "installing: unicodecsv-0.14.1-py36ha668878_0 ...\n",
            "installing: wcwidth-0.1.7-py36hdf4376a_0 ...\n",
            "installing: webencodings-0.5.1-py36h800622e_1 ...\n",
            "installing: werkzeug-0.14.1-py36_0 ...\n",
            "installing: wrapt-1.10.11-py36h28b7045_0 ...\n",
            "installing: xlrd-1.1.0-py36h1db9f0c_1 ...\n",
            "installing: xlsxwriter-1.0.2-py36h3de1aca_0 ...\n",
            "installing: xlwt-1.3.0-py36h7b00a1f_0 ...\n",
            "installing: babel-2.5.3-py36_0 ...\n",
            "installing: backports.shutil_get_terminal_size-1.0.0-py36hfea85ff_2 ...\n",
            "installing: bottleneck-1.2.1-py36haac1ea0_0 ...\n",
            "installing: cffi-1.11.4-py36h9745a5d_0 ...\n",
            "installing: conda-verify-2.0.0-py36h98955d8_0 ...\n",
            "installing: cycler-0.10.0-py36h93f1223_0 ...\n",
            "installing: cytoolz-0.9.0-py36h14c3975_0 ...\n",
            "installing: h5py-2.7.1-py36h3585f63_0 ...\n",
            "installing: harfbuzz-1.7.4-hc5b324e_0 ...\n",
            "installing: html5lib-1.0.1-py36h2f9c1c0_0 ...\n",
            "installing: jedi-0.11.1-py36_0 ...\n",
            "installing: networkx-2.1-py36_0 ...\n",
            "installing: nltk-3.2.5-py36h7532b22_0 ...\n",
            "installing: numba-0.36.2-np114py36hc6662d5_0 ...\n",
            "installing: numexpr-2.6.4-py36hc4a3f9a_0 ...\n",
            "installing: openpyxl-2.4.10-py36_0 ...\n",
            "installing: packaging-16.8-py36ha668100_1 ...\n",
            "installing: partd-0.3.8-py36h36fd896_0 ...\n",
            "installing: pathlib2-2.3.0-py36h49efa8e_0 ...\n",
            "installing: pexpect-4.3.1-py36_0 ...\n",
            "installing: pillow-5.0.0-py36h3deb7b8_0 ...\n",
            "installing: pyqt-5.6.0-py36h0386399_5 ...\n",
            "installing: python-dateutil-2.6.1-py36h88d3b88_1 ...\n",
            "installing: pywavelets-0.5.2-py36he602eb0_0 ...\n",
            "installing: qtawesome-0.4.4-py36h609ed8c_0 ...\n",
            "installing: scipy-1.0.0-py36hbf646e7_0 ...\n",
            "installing: setuptools-38.4.0-py36_0 ...\n",
            "installing: singledispatch-3.4.0.3-py36h7a266c3_0 ...\n",
            "installing: sortedcollections-0.5.3-py36h3c761f9_0 ...\n",
            "installing: sphinxcontrib-websupport-1.0.1-py36hb5cb234_1 ...\n",
            "installing: sympy-1.1.1-py36hc6d1c1c_0 ...\n",
            "installing: terminado-0.8.1-py36_1 ...\n",
            "installing: traitlets-4.3.2-py36h674d592_0 ...\n",
            "installing: zict-0.1.3-py36h3a3bf81_0 ...\n",
            "installing: astroid-1.6.1-py36_0 ...\n",
            "installing: bleach-2.1.2-py36_0 ...\n",
            "installing: clyent-1.2.2-py36h7e57e65_1 ...\n",
            "installing: cryptography-2.1.4-py36hd09be54_0 ...\n",
            "installing: cython-0.27.3-py36h1860423_0 ...\n",
            "installing: datashape-0.5.4-py36h3ad6b5c_0 ...\n",
            "installing: distributed-1.20.2-py36_0 ...\n",
            "installing: get_terminal_size-1.0.0-haa9412d_0 ...\n",
            "installing: gevent-1.2.2-py36h2fe25dc_0 ...\n",
            "installing: imageio-2.2.0-py36he555465_0 ...\n",
            "installing: isort-4.2.15-py36had401c0_0 ...\n",
            "installing: jinja2-2.10-py36ha16c418_0 ...\n",
            "installing: jsonschema-2.6.0-py36h006f8b5_0 ...\n",
            "installing: jupyter_core-4.4.0-py36h7c827e3_0 ...\n",
            "installing: matplotlib-2.1.2-py36h0e671d2_0 ...\n",
            "installing: navigator-updater-0.1.0-py36h14770f7_0 ...\n",
            "installing: nose-1.3.7-py36hcdf7029_2 ...\n",
            "installing: pandas-0.22.0-py36hf484d3e_0 ...\n",
            "installing: pango-1.41.0-hd475d92_0 ...\n",
            "installing: patsy-0.5.0-py36_0 ...\n",
            "installing: pyflakes-1.6.0-py36h7bd6a15_0 ...\n",
            "installing: pygments-2.2.0-py36h0d3125c_0 ...\n",
            "installing: pytables-3.4.2-py36h3b5282a_2 ...\n",
            "installing: pytest-3.3.2-py36_0 ...\n",
            "installing: scikit-learn-0.19.1-py36h7aa7ec6_0 ...\n",
            "installing: wheel-0.30.0-py36hfd4bba0_1 ...\n",
            "installing: astropy-2.0.3-py36h14c3975_0 ...\n",
            "installing: bkcharts-0.2-py36h735825a_0 ...\n",
            "installing: bokeh-0.12.13-py36h2f9c1c0_0 ...\n",
            "installing: flask-0.12.2-py36hb24657c_0 ...\n",
            "installing: jupyter_client-5.2.2-py36_0 ...\n",
            "installing: nbformat-4.4.0-py36h31c9010_0 ...\n",
            "installing: pip-9.0.1-py36h6c6f9ce_4 ...\n",
            "installing: prompt_toolkit-1.0.15-py36h17d85b1_0 ...\n",
            "installing: pylint-1.8.2-py36_0 ...\n",
            "installing: pyopenssl-17.5.0-py36h20ba746_0 ...\n",
            "installing: statsmodels-0.8.0-py36h8533d0b_0 ...\n",
            "installing: dask-0.16.1-py36_0 ...\n",
            "installing: flask-cors-3.0.3-py36h2d857d3_0 ...\n",
            "installing: ipython-6.2.1-py36h88c514a_1 ...\n",
            "installing: nbconvert-5.3.1-py36hb41ffb7_0 ...\n",
            "installing: seaborn-0.8.1-py36hfad7ec4_0 ...\n",
            "installing: urllib3-1.22-py36hbe7ace6_0 ...\n",
            "installing: ipykernel-4.8.0-py36_0 ...\n",
            "installing: odo-0.5.1-py36h90ed295_0 ...\n",
            "installing: requests-2.18.4-py36he2e5f8d_1 ...\n",
            "installing: scikit-image-0.13.1-py36h14c3975_1 ...\n",
            "installing: anaconda-client-1.6.9-py36_0 ...\n",
            "installing: blaze-0.11.3-py36h4e06776_0 ...\n",
            "installing: jupyter_console-5.2.0-py36he59e554_1 ...\n",
            "installing: notebook-5.4.0-py36_0 ...\n",
            "installing: qtconsole-4.3.1-py36h8f73b5b_0 ...\n",
            "installing: sphinx-1.6.6-py36_0 ...\n",
            "installing: anaconda-project-0.8.2-py36h44fb852_0 ...\n",
            "installing: jupyterlab_launcher-0.10.2-py36_0 ...\n",
            "installing: numpydoc-0.7.0-py36h18f165f_0 ...\n",
            "installing: widgetsnbextension-3.1.0-py36_0 ...\n",
            "installing: anaconda-navigator-1.7.0-py36_0 ...\n",
            "installing: ipywidgets-7.1.1-py36_0 ...\n",
            "installing: jupyterlab-0.31.5-py36_0 ...\n",
            "installing: spyder-3.2.6-py36_0 ...\n",
            "installing: _ipyw_jlab_nb_ext_conf-0.1.0-py36he11e457_0 ...\n",
            "installing: jupyter-1.0.0-py36_4 ...\n",
            "installing: anaconda-5.1.0-py36_2 ...\n",
            "installing: conda-4.4.10-py36_0 ...\n",
            "installing: conda-build-3.4.1-py36_0 ...\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Anaconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Anaconda3: /usr/local\n",
            "/content/drive/My Drive/Colab Notebooks/fastai2\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "  current version: 4.4.10\n",
            "  latest version: 4.8.3\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base conda\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "threadpoolctl 2.1.0: 100% 1.0/1 [00:00<00:00, 19.26it/s]\n",
            "openssl 1.1.1g: 100% 1.0/1 [00:01<00:00,  1.23s/it]               \n",
            "glib 2.65.0: 100% 1.0/1 [00:01<00:00,  1.41s/it]   \n",
            "jedi 0.17.1: 100% 1.0/1 [00:00<00:00,  1.07it/s]               \n",
            "pyqt 5.9.2: 100% 1.0/1 [00:02<00:00,  2.33s/it]               \n",
            "scikit-learn 0.23.1: 100% 1.0/1 [00:02<00:00,  2.89s/it]               \n",
            "cymem 2.0.2: 100% 1.0/1 [00:02<00:00,  2.13s/it] \n",
            "send2trash 1.5.0: 100% 1.0/1 [00:00<00:00, 27.45it/s]\n",
            "ipython_genutils 0.2.0: 100% 1.0/1 [00:00<00:00,  2.18it/s]                \n",
            "tqdm 4.47.0: 100% 1.0/1 [00:00<00:00, 18.44it/s]\n",
            "setuptools 47.3.1: 100% 1.0/1 [00:00<00:00,  3.01it/s]               \n",
            "libxcb 1.14: 100% 1.0/1 [00:00<00:00,  2.98it/s]               \n",
            "fontconfig 2.13.0: 100% 1.0/1 [00:00<00:00,  6.35it/s]               \n",
            "lcms2 2.11: 100% 1.0/1 [00:01<00:00,  1.06s/it]               \n",
            "wasabi 0.2.2: 100% 1.0/1 [00:02<00:00,  2.12s/it] \n",
            "mkl-service 2.3.0: 100% 1.0/1 [00:00<00:00,  9.39it/s] \n",
            "zeromq 4.3.2: 100% 1.0/1 [00:00<00:00,  3.06it/s]               \n",
            "zlib 1.2.11: 100% 1.0/1 [00:00<00:00, 15.68it/s]\n",
            "numpy-base 1.18.5: 100% 1.0/1 [00:02<00:00,  2.13s/it]               \n",
            "pexpect 4.8.0: 100% 1.0/1 [00:00<00:00, 13.68it/s]\n",
            "zstd 1.4.4: 100% 1.0/1 [00:00<00:00,  2.94it/s]              \n",
            "jupyter_core 4.6.3: 100% 1.0/1 [00:00<00:00, 12.86it/s]\n",
            "mkl_random 1.1.1: 100% 1.0/1 [00:00<00:00,  6.07it/s]               \n",
            "dbus 1.13.16: 100% 1.0/1 [00:00<00:00,  4.11it/s]               \n",
            "sqlite 3.32.3: 100% 1.0/1 [00:00<00:00,  1.47it/s] \n",
            "chardet 3.0.4: 100% 1.0/1 [00:00<00:00,  2.44it/s]               \n",
            "jupyter_console 6.1.0: 100% 1.0/1 [00:00<00:00, 21.71it/s]\n",
            "pygments 2.6.1: 100% 1.0/1 [00:00<00:00,  2.74it/s]               \n",
            "zipp 3.1.0: 100% 1.0/1 [00:00<00:00, 27.51it/s]\n",
            "wheel 0.34.2: 100% 1.0/1 [00:00<00:00, 18.46it/s]\n",
            "expat 2.2.9: 100% 1.0/1 [00:00<00:00, 10.54it/s]\n",
            "terminado 0.8.3: 100% 1.0/1 [00:00<00:00, 23.31it/s]\n",
            "idna 2.10: 100% 1.0/1 [00:00<00:00, 14.77it/s]\n",
            "widgetsnbextension 3.5.1: 100% 1.0/1 [00:00<00:00,  1.31it/s]               \n",
            "readline 8.0: 100% 1.0/1 [00:00<00:00,  4.95it/s] \n",
            "importlib-metadata 1.7.0: 100% 1.0/1 [00:00<00:00, 17.06it/s]\n",
            "matplotlib-base 3.2.2: 100% 1.0/1 [00:02<00:00,  2.46s/it]               \n",
            "ipywidgets 7.5.1: 100% 1.0/1 [00:00<00:00, 12.78it/s]\n",
            "prompt_toolkit 3.0.5: 100% 1.0/1 [00:00<00:00, 31.99it/s]\n",
            "torchvision 0.6.1: 100% 1.0/1 [00:06<00:00,  6.72s/it]               \n",
            "libuuid 1.0.3: 100% 1.0/1 [00:00<00:00, 26.70it/s]\n",
            "cryptography 2.9.2: 100% 1.0/1 [00:00<00:00,  2.80it/s]               \n",
            "parso 0.7.0: 100% 1.0/1 [00:00<00:00, 14.88it/s]\n",
            "traitlets 4.3.3: 100% 1.0/1 [00:00<00:00,  2.80it/s]              \n",
            "freetype 2.10.2: 100% 1.0/1 [00:00<00:00,  3.10it/s]               \n",
            "pandas 1.0.5: 100% 1.0/1 [00:04<00:00,  4.21s/it]              \n",
            "prometheus_client 0.8.0: 100% 1.0/1 [00:00<00:00, 10.41it/s]\n",
            "nbformat 5.0.7: 100% 1.0/1 [00:00<00:00, 10.30it/s]\n",
            "wcwidth 0.2.5: 100% 1.0/1 [00:00<00:00, 17.79it/s]\n",
            "pytz 2020.1: 100% 1.0/1 [00:00<00:00,  3.68it/s]               \n",
            "tornado 6.0.4: 100% 1.0/1 [00:00<00:00,  3.00it/s]               \n",
            "libpng 1.6.37: 100% 1.0/1 [00:00<00:00,  6.88it/s]               \n",
            "certifi 2020.6.20: 100% 1.0/1 [00:00<00:00, 13.07it/s]\n",
            "pip 20.1.1: 100% 1.0/1 [00:00<00:00,  1.03it/s]               \n",
            "pycparser 2.20: 100% 1.0/1 [00:00<00:00, 14.23it/s]\n",
            "thinc 7.0.8: 100% 1.0/1 [00:04<00:00,  4.18s/it]               \n",
            "prompt-toolkit 3.0.5: 100% 1.0/1 [00:00<00:00,  5.85it/s]               \n",
            "jupyter 1.0.0: 100% 1.0/1 [00:00<00:00,  3.65it/s] \n",
            "pyyaml 5.3.1: 100% 1.0/1 [00:00<00:00,  2.86it/s]                \n",
            "python 3.7.7: 100% 1.0/1 [00:12<00:00, 12.97s/it]               \n",
            "kiwisolver 1.2.0: 100% 1.0/1 [00:00<00:00, 17.93it/s]\n",
            "ipykernel 5.3.0: 100% 1.0/1 [00:00<00:00,  2.68it/s]               \n",
            "decorator 4.4.2: 100% 1.0/1 [00:00<00:00, 31.57it/s]\n",
            "cffi 1.14.0: 100% 1.0/1 [00:00<00:00,  2.92it/s]                 \n",
            "ptyprocess 0.6.0: 100% 1.0/1 [00:00<00:00,  3.42it/s]               \n",
            "webencodings 0.5.1: 100% 1.0/1 [00:00<00:00, 24.49it/s]\n",
            "notebook 6.0.3: 100% 1.0/1 [00:02<00:00,  2.76s/it]              \n",
            "markupsafe 1.1.1: 100% 1.0/1 [00:00<00:00, 22.90it/s]\n",
            "bleach 3.1.5: 100% 1.0/1 [00:00<00:00, 11.38it/s]\n",
            "cython-blis 0.2.4: 100% 1.0/1 [00:04<00:00,  4.37s/it]               \n",
            "matplotlib 3.2.2: 100% 1.0/1 [00:00<00:00, 27.84it/s]\n",
            "six 1.15.0: 100% 1.0/1 [00:00<00:00, 28.44it/s]\n",
            "olefile 0.46: 100% 1.0/1 [00:00<00:00, 27.78it/s]\n",
            "ca-certificates 2020.6.24: 100% 1.0/1 [00:00<00:00, 16.75it/s]\n",
            "pyzmq 19.0.1: 100% 1.0/1 [00:00<00:00,  1.70it/s]              \n",
            "pyrsistent 0.16.0: 100% 1.0/1 [00:00<00:00,  3.10it/s]                \n",
            "_libgcc_mutex 0.1: 100% 1.0/1 [00:00<00:00, 37.73it/s]\n",
            "plac 0.9.6: 100% 1.0/1 [00:00<00:00, 25.12it/s]\n",
            "packaging 20.4: 100% 1.0/1 [00:00<00:00, 20.65it/s]\n",
            "gstreamer 1.14.0: 100% 1.0/1 [00:01<00:00,  1.16s/it]               \n",
            "python-dateutil 2.8.1: 100% 1.0/1 [00:00<00:00, 12.10it/s]\n",
            "attrs 19.3.0: 100% 1.0/1 [00:00<00:00, 23.64it/s]\n",
            "entrypoints 0.3: 100% 1.0/1 [00:00<00:00, 35.22it/s]\n",
            "spacy 2.1.8: 100% 1.0/1 [00:33<00:00, 33.23s/it]               \n",
            "yaml 0.2.5: 100% 1.0/1 [00:00<00:00, 15.98it/s]\n",
            "mkl 2020.1: 100% 1.0/1 [01:07<00:00, 67.73s/it]                \n",
            "libedit 3.1.20191231: 100% 1.0/1 [00:00<00:00,  8.91it/s]               \n",
            "requests 2.24.0: 100% 1.0/1 [00:00<00:00, 18.18it/s]\n",
            "libgfortran-ng 7.3.0: 100% 1.0/1 [00:00<00:00,  2.11it/s]               \n",
            "scipy 1.5.0: 100% 1.0/1 [00:06<00:00,  6.30s/it]               \n",
            "lz4-c 1.9.2: 100% 1.0/1 [00:00<00:00,  7.97it/s]               \n",
            "pickleshare 0.7.5: 100% 1.0/1 [00:00<00:00, 28.85it/s]\n",
            "libxml2 2.9.10: 100% 1.0/1 [00:00<00:00,  1.49it/s]               \n",
            "pyopenssl 19.1.0: 100% 1.0/1 [00:00<00:00, 15.25it/s]\n",
            "pandocfilters 1.4.2: 100% 1.0/1 [00:00<00:00, 31.57it/s]\n",
            "ld_impl_linux-64 2.33.1: 100% 1.0/1 [00:00<00:00,  3.53it/s] \n",
            "cudatoolkit 10.2.89: 100% 1.0/1 [02:15<00:00, 135.95s/it]                \n",
            "testpath 0.4.4: 100% 1.0/1 [00:00<00:00, 14.48it/s]\n",
            "qt 5.9.7: 100% 1.0/1 [00:28<00:00, 28.85s/it]                \n",
            "murmurhash 1.0.2: 100% 1.0/1 [00:00<00:00, 25.34it/s]\n",
            "numpy 1.18.5: 100% 1.0/1 [00:00<00:00,  3.93it/s] \n",
            "ninja 1.9.0: 100% 1.0/1 [00:00<00:00,  1.98it/s]               \n",
            "sip 4.19.8: 100% 1.0/1 [00:00<00:00,  5.82it/s] \n",
            "intel-openmp 2020.1: 100% 1.0/1 [00:00<00:00,  3.24it/s]               \n",
            "libtiff 4.1.0: 100% 1.0/1 [00:00<00:00,  3.86it/s]               \n",
            "gst-plugins-base 1.14.0: 100% 1.0/1 [00:01<00:00,  1.88s/it]               \n",
            "xz 5.2.5: 100% 1.0/1 [00:00<00:00,  5.30it/s]               \n",
            "mkl_fft 1.1.0: 100% 1.0/1 [00:00<00:00, 12.26it/s]\n",
            "fastprogress 0.2.2: 100% 1.0/1 [00:00<00:00,  1.13it/s] \n",
            "libstdcxx-ng 9.1.0: 100% 1.0/1 [00:01<00:00,  1.24s/it]               \n",
            "qtconsole 4.7.5: 100% 1.0/1 [00:00<00:00, 12.96it/s]\n",
            "defusedxml 0.6.0: 100% 1.0/1 [00:00<00:00, 21.88it/s]\n",
            "pandoc 2.9.2.1: 100% 1.0/1 [00:08<00:00,  8.09s/it]               \n",
            "brotlipy 0.7.0: 100% 1.0/1 [00:00<00:00,  8.02it/s] \n",
            "nbconvert 5.6.1: 100% 1.0/1 [00:00<00:00,  1.63it/s]               \n",
            "pytorch 1.5.1: 100% 1.0/1 [02:24<00:00, 144.69s/it]      \n",
            "backcall 0.2.0: 100% 1.0/1 [00:00<00:00, 17.95it/s]\n",
            "srsly 0.1.0: 100% 1.0/1 [00:02<00:00,  2.54s/it] \n",
            "jupyter_client 6.1.5: 100% 1.0/1 [00:00<00:00, 14.73it/s]\n",
            "libffi 3.3: 100% 1.0/1 [00:00<00:00, 18.99it/s]\n",
            "pillow 7.2.0: 100% 1.0/1 [00:00<00:00,  3.40it/s]               \n",
            "libsodium 1.0.18: 100% 1.0/1 [00:00<00:00,  6.19it/s]               \n",
            "qtpy 1.9.0: 100% 1.0/1 [00:00<00:00, 18.41it/s]\n",
            "ncurses 6.2: 100% 1.0/1 [00:00<00:00,  1.01it/s]               \n",
            "joblib 0.16.0: 100% 1.0/1 [00:00<00:00,  6.49it/s]               \n",
            "importlib_metadata 1.7.0: 100% 1.0/1 [00:00<00:00, 30.22it/s]\n",
            "urllib3 1.25.9: 100% 1.0/1 [00:00<00:00, 13.04it/s]\n",
            "blas 1.0: 100% 1.0/1 [00:00<00:00, 31.49it/s]\n",
            "icu 58.2: 100% 1.0/1 [00:06<00:00,  6.73s/it]               \n",
            "pysocks 1.7.1: 100% 1.0/1 [00:00<00:00, 24.31it/s]\n",
            "tk 8.6.10: 100% 1.0/1 [00:01<00:00,  1.17s/it]               \n",
            "pcre 8.44: 100% 1.0/1 [00:00<00:00,  7.06it/s]               \n",
            "cycler 0.10.0: 100% 1.0/1 [00:00<00:00,  2.67it/s] \n",
            "pyparsing 2.4.7: 100% 1.0/1 [00:00<00:00, 18.68it/s]\n",
            "mistune 0.8.4: 100% 1.0/1 [00:00<00:00, 14.45it/s]\n",
            "jinja2 2.11.2: 100% 1.0/1 [00:00<00:00, 14.41it/s]\n",
            "preshed 2.0.1: 100% 1.0/1 [00:00<00:00, 16.02it/s]\n",
            "jsonschema 3.2.0: 100% 1.0/1 [00:00<00:00, 12.57it/s]\n",
            "ipython 7.16.1: 100% 1.0/1 [00:00<00:00,  1.09it/s]               \n",
            "libgcc-ng 9.1.0: 100% 1.0/1 [00:02<00:00,  2.48s/it]               \n",
            "Preparing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Verifying transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "#\n",
            "# To activate this environment, use:\n",
            "# > source activate fastai2\n",
            "#\n",
            "# To deactivate an active environment, use:\n",
            "# > source deactivate\n",
            "#\n",
            "\n",
            "Obtaining file:///content/drive/My%20Drive/Colab%20Notebooks/fastai2\n",
            "Collecting fastcore (from fastai2==0.0.18)\n",
            "  Downloading https://files.pythonhosted.org/packages/e2/6e/a18c0ff6cdca36915e65cf1690137134241a33d74ceef7882f4a63a6af55/fastcore-0.1.18-py3-none-any.whl\n",
            "Collecting torch>=1.3.0 (from fastai2==0.0.18)\n",
            "  Downloading https://files.pythonhosted.org/packages/62/01/457b49d790b6c4b9720e6f9dbbb617692f6ce8afdaadf425c055c41a7416/torch-1.5.1-cp36-cp36m-manylinux1_x86_64.whl (753.2MB)\n",
            "\u001b[K    99% |████████████████████████████████| 753.2MB 59.3MB/s eta 0:00:01tcmalloc: large alloc 1506312192 bytes == 0x5572811ee000 @  0x7f761f1991e7 0x557247bfb804 0x557247c40eb4 0x557247cf0ae2 0x557247c41999 0x7f761befdf98 0x557247c22060 0x557247c4ff24 0x557247cafbec 0x557247cd4eb1 0x557247ca89a6 0x557247ca9a11 0x557247cafcc5 0x557247cd419a 0x557247ca97db 0x557247cafcc5 0x557247cd419a 0x557247ca89a6 0x557247ca9a11 0x557247cafcc5 0x557247cd4eb1 0x557247ca89a6 0x557247ca9eee 0x557247c2239f 0x557247c26ff3 0x557247c21dde 0x557247d17095 0x557247c221bb 0x557247cafd3e 0x557247cd419a 0x557247ca97db\n",
            "\u001b[K    100% |████████████████████████████████| 753.2MB 1.7kB/s \n",
            "\u001b[?25hCollecting torchvision>=0.5 (from fastai2==0.0.18)\n",
            "\u001b[33m  Retrying (Retry(total=4, connect=None, read=None, redirect=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /packages/9a/f1/535a407b4a265adf2dd7c2c2458217e37c5fe83ec97234e66c564592a9a0/torchvision-0.6.1-cp36-cp36m-manylinux1_x86_64.whl\u001b[0m\n",
            "  Downloading https://files.pythonhosted.org/packages/9a/f1/535a407b4a265adf2dd7c2c2458217e37c5fe83ec97234e66c564592a9a0/torchvision-0.6.1-cp36-cp36m-manylinux1_x86_64.whl (6.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 6.6MB 175kB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/site-packages (from fastai2==0.0.18)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/site-packages (from fastai2==0.0.18)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/site-packages (from fastai2==0.0.18)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/site-packages (from fastai2==0.0.18)\n",
            "Collecting fastprogress>=0.1.22 (from fastai2==0.0.18)\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/da/ffd8fe0daf7e679804a32a1e8654ac2988e2ef85937fc1d223e98eee736e/fastprogress-0.2.3-py3-none-any.whl\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/site-packages (from fastai2==0.0.18)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/site-packages (from fastai2==0.0.18)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/site-packages (from fastai2==0.0.18)\n",
            "Collecting spacy (from fastai2==0.0.18)\n",
            "  Downloading https://files.pythonhosted.org/packages/f3/80/c3c0d15cc3ea97c1fd578c39489ef6c360ec0fedfbf15cb29fd89dcf3271/spacy-2.3.1-cp36-cp36m-manylinux1_x86_64.whl (9.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 9.9MB 121kB/s \n",
            "\u001b[?25hCollecting nbdev (from fastai2==0.0.18)\n",
            "  Downloading https://files.pythonhosted.org/packages/19/54/f39f9050f0e1610c4c5f764872812ef72615dac70ea7f1c9bc20948acb04/nbdev-0.2.18-py3-none-any.whl (45kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 10.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from fastcore->fastai2==0.0.18)\n",
            "Collecting dataclasses>='0.7'; python_version < \"3.7\" (from fastcore->fastai2==0.0.18)\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/d2/6f02df2616fd4016075f60157c7a0452b38d8f7938ae94343911e0fb0b09/dataclasses-0.7-py3-none-any.whl\n",
            "Collecting future (from torch>=1.3.0->fastai2==0.0.18)\n",
            "  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K    100% |████████████████████████████████| 829kB 825kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/site-packages (from matplotlib->fastai2==0.0.18)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/site-packages (from matplotlib->fastai2==0.0.18)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/site-packages (from matplotlib->fastai2==0.0.18)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/site-packages (from matplotlib->fastai2==0.0.18)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/site-packages (from matplotlib->fastai2==0.0.18)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests->fastai2==0.0.18)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests->fastai2==0.0.18)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests->fastai2==0.0.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests->fastai2==0.0.18)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy->fastai2==0.0.18)\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/b5/3e1714ebda8fd7c5859f9b216e381adc0a38b962f071568fd00d67e1b1ca/cymem-2.0.3-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting wasabi<1.1.0,>=0.4.0 (from spacy->fastai2==0.0.18)\n",
            "  Downloading https://files.pythonhosted.org/packages/04/e5/aa1892776a8ed6f6d552ba1be0640e6403f07e850d36e79f475f1e605aa9/wasabi-0.7.0.tar.gz\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy->fastai2==0.0.18)\n",
            "  Downloading https://files.pythonhosted.org/packages/a6/e6/63f160a4fdf0e875d16b28f972083606d8d54f56cd30cb8929f9a1ee700e/murmurhash-1.0.2-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting tqdm<5.0.0,>=4.38.0 (from spacy->fastai2==0.0.18)\n",
            "  Downloading https://files.pythonhosted.org/packages/46/62/7663894f67ac5a41a0d8812d78d9d2a9404124051885af9d77dc526fb399/tqdm-4.47.0-py2.py3-none-any.whl (66kB)\n",
            "\u001b[K    100% |████████████████████████████████| 71kB 10.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/site-packages (from spacy->fastai2==0.0.18)\n",
            "Collecting srsly<1.1.0,>=1.0.2 (from spacy->fastai2==0.0.18)\n",
            "  Downloading https://files.pythonhosted.org/packages/0e/9a/70bd934dd4d25545c9aa6c8cd4edbac2a33ba9c915439a9209b69f0ec0ad/srsly-1.0.2-cp36-cp36m-manylinux1_x86_64.whl (185kB)\n",
            "\u001b[K    100% |████████████████████████████████| 194kB 2.6MB/s \n",
            "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2 (from spacy->fastai2==0.0.18)\n",
            "  Downloading https://files.pythonhosted.org/packages/db/6b/e07fad36913879757c90ba03d6fb7f406f7279e11dcefc105ee562de63ea/preshed-3.0.2-cp36-cp36m-manylinux1_x86_64.whl (119kB)\n",
            "\u001b[K    100% |████████████████████████████████| 122kB 1.6MB/s \n",
            "\u001b[?25hCollecting thinc==7.4.1 (from spacy->fastai2==0.0.18)\n",
            "  Downloading https://files.pythonhosted.org/packages/10/ae/ef3ae5e93639c0ef8e3eb32e3c18341e511b3c515fcfc603f4b808087651/thinc-7.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.1MB 449kB/s \n",
            "\u001b[?25hCollecting plac<1.2.0,>=0.9.6 (from spacy->fastai2==0.0.18)\n",
            "  Downloading https://files.pythonhosted.org/packages/86/85/40b8f66c2dd8f4fd9f09d59b22720cffecf1331e788b8a0cab5bafb353d1/plac-1.1.3-py2.py3-none-any.whl\n",
            "Collecting blis<0.5.0,>=0.4.0 (from spacy->fastai2==0.0.18)\n",
            "  Downloading https://files.pythonhosted.org/packages/41/19/f95c75562d18eb27219df3a3590b911e78d131b68466ad79fdf5847eaac4/blis-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.7MB 279kB/s \n",
            "\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7 (from spacy->fastai2==0.0.18)\n",
            "  Downloading https://files.pythonhosted.org/packages/6c/f9/9a5658e2f56932e41eb264941f9a2cb7f3ce41a80cb36b2af6ab78e2f8af/catalogue-1.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: nbformat>=4.4.0 in /usr/local/lib/python3.6/site-packages (from nbdev->fastai2==0.0.18)\n",
            "Collecting fastscript (from nbdev->fastai2==0.0.18)\n",
            "  Downloading https://files.pythonhosted.org/packages/55/0e/ecdc0213646bc82986884121109a38b50bbc2cd2c491bbbfdc7ae39228e3/fastscript-0.1.4-py3-none-any.whl\n",
            "Collecting nbconvert>=5.6.1 (from nbdev->fastai2==0.0.18)\n",
            "  Downloading https://files.pythonhosted.org/packages/79/6c/05a569e9f703d18aacb89b7ad6075b404e8a4afde2c26b73ca77bb644b14/nbconvert-5.6.1-py2.py3-none-any.whl (455kB)\n",
            "\u001b[K    100% |████████████████████████████████| 460kB 790kB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/site-packages (from nbdev->fastai2==0.0.18)\n",
            "Collecting importlib-metadata>=0.20; python_version < \"3.8\" (from catalogue<1.1.0,>=0.0.7->spacy->fastai2==0.0.18)\n",
            "  Downloading https://files.pythonhosted.org/packages/8e/58/cdea07eb51fc2b906db0968a94700866fc46249bdc75cac23f9d13168929/importlib_metadata-1.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: ipython_genutils in /usr/local/lib/python3.6/site-packages (from nbformat>=4.4.0->nbdev->fastai2==0.0.18)\n",
            "Requirement already satisfied: traitlets>=4.1 in /usr/local/lib/python3.6/site-packages (from nbformat>=4.4.0->nbdev->fastai2==0.0.18)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/site-packages (from nbformat>=4.4.0->nbdev->fastai2==0.0.18)\n",
            "Requirement already satisfied: jupyter_core in /usr/local/lib/python3.6/site-packages (from nbformat>=4.4.0->nbdev->fastai2==0.0.18)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/site-packages (from nbconvert>=5.6.1->nbdev->fastai2==0.0.18)\n",
            "Collecting defusedxml (from nbconvert>=5.6.1->nbdev->fastai2==0.0.18)\n",
            "  Downloading https://files.pythonhosted.org/packages/06/74/9b387472866358ebc08732de3da6dc48e44b0aacd2ddaa5cb85ab7e986a2/defusedxml-0.6.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/site-packages (from nbconvert>=5.6.1->nbdev->fastai2==0.0.18)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/site-packages (from nbconvert>=5.6.1->nbdev->fastai2==0.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/site-packages (from nbconvert>=5.6.1->nbdev->fastai2==0.0.18)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/site-packages (from nbconvert>=5.6.1->nbdev->fastai2==0.0.18)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/site-packages (from nbconvert>=5.6.1->nbdev->fastai2==0.0.18)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/site-packages (from nbconvert>=5.6.1->nbdev->fastai2==0.0.18)\n",
            "Collecting zipp>=0.5 (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->fastai2==0.0.18)\n",
            "  Downloading https://files.pythonhosted.org/packages/b2/34/bfcb43cc0ba81f527bc4f40ef41ba2ff4080e047acb0586b56b3d017ace4/zipp-3.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/site-packages (from traitlets>=4.1->nbformat>=4.4.0->nbdev->fastai2==0.0.18)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/site-packages (from jinja2>=2.4->nbconvert>=5.6.1->nbdev->fastai2==0.0.18)\n",
            "Requirement already satisfied: html5lib!=1.0b1,!=1.0b2,!=1.0b3,!=1.0b4,!=1.0b5,!=1.0b6,!=1.0b7,!=1.0b8,>=0.99999999pre in /usr/local/lib/python3.6/site-packages (from bleach->nbconvert>=5.6.1->nbdev->fastai2==0.0.18)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/site-packages (from html5lib!=1.0b1,!=1.0b2,!=1.0b3,!=1.0b4,!=1.0b5,!=1.0b6,!=1.0b7,!=1.0b8,>=0.99999999pre->bleach->nbconvert>=5.6.1->nbdev->fastai2==0.0.18)\n",
            "Building wheels for collected packages: future, wasabi\n",
            "  Running setup.py bdist_wheel for future ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Running setup.py bdist_wheel for wasabi ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/33/96/74/01741d5dde3d866a4461a05b3fc6aa43bd7ece8729a7264bf7\n",
            "Successfully built future wasabi\n",
            "Installing collected packages: dataclasses, fastcore, future, torch, torchvision, fastprogress, cymem, wasabi, murmurhash, tqdm, srsly, preshed, plac, blis, zipp, importlib-metadata, catalogue, thinc, spacy, fastscript, defusedxml, nbconvert, nbdev, fastai2\n",
            "  Found existing installation: nbconvert 5.3.1\n",
            "    Uninstalling nbconvert-5.3.1:\n",
            "      Successfully uninstalled nbconvert-5.3.1\n",
            "  Running setup.py develop for fastai2\n",
            "Successfully installed blis-0.4.1 catalogue-1.0.0 cymem-2.0.3 dataclasses-0.7 defusedxml-0.6.0 fastai2 fastcore-0.1.18 fastprogress-0.2.3 fastscript-0.1.4 future-0.18.2 importlib-metadata-1.7.0 murmurhash-1.0.2 nbconvert-5.6.1 nbdev-0.2.18 plac-1.1.3 preshed-3.0.2 spacy-2.3.1 srsly-1.0.2 thinc-7.4.1 torch-1.5.1 torchvision-0.6.1 tqdm-4.47.0 wasabi-0.7.0 zipp-3.1.0\n",
            "Requirement already satisfied: nbdev in /usr/local/lib/python3.6/site-packages\n",
            "Requirement already satisfied: nbformat>=4.4.0 in /usr/local/lib/python3.6/site-packages (from nbdev)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/site-packages (from nbdev)\n",
            "Requirement already satisfied: fastscript in /usr/local/lib/python3.6/site-packages (from nbdev)\n",
            "Requirement already satisfied: nbconvert>=5.6.1 in /usr/local/lib/python3.6/site-packages (from nbdev)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/site-packages (from nbdev)\n",
            "Requirement already satisfied: ipython_genutils in /usr/local/lib/python3.6/site-packages (from nbformat>=4.4.0->nbdev)\n",
            "Requirement already satisfied: traitlets>=4.1 in /usr/local/lib/python3.6/site-packages (from nbformat>=4.4.0->nbdev)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/site-packages (from nbformat>=4.4.0->nbdev)\n",
            "Requirement already satisfied: jupyter_core in /usr/local/lib/python3.6/site-packages (from nbformat>=4.4.0->nbdev)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/site-packages (from nbconvert>=5.6.1->nbdev)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/site-packages (from nbconvert>=5.6.1->nbdev)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/site-packages (from nbconvert>=5.6.1->nbdev)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/site-packages (from nbconvert>=5.6.1->nbdev)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/site-packages (from nbconvert>=5.6.1->nbdev)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/site-packages (from nbconvert>=5.6.1->nbdev)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/site-packages (from nbconvert>=5.6.1->nbdev)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/site-packages (from nbconvert>=5.6.1->nbdev)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/site-packages (from packaging->nbdev)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from packaging->nbdev)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/site-packages (from traitlets>=4.1->nbformat>=4.4.0->nbdev)\n",
            "Requirement already satisfied: html5lib!=1.0b1,!=1.0b2,!=1.0b3,!=1.0b4,!=1.0b5,!=1.0b6,!=1.0b7,!=1.0b8,>=0.99999999pre in /usr/local/lib/python3.6/site-packages (from bleach->nbconvert>=5.6.1->nbdev)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/site-packages (from jinja2>=2.4->nbconvert>=5.6.1->nbdev)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/site-packages (from html5lib!=1.0b1,!=1.0b2,!=1.0b3,!=1.0b4,!=1.0b5,!=1.0b6,!=1.0b7,!=1.0b8,>=0.99999999pre->bleach->nbconvert>=5.6.1->nbdev)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMNTXHEEZxuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#default_exp data.core"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdyQDTfXZxuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "from fastai2.torch_basics import *\n",
        "from fastai2.data.load import *"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_OzxVAFZxua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nbdev.showdoc import *"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PexvOK16Zxuc",
        "colab_type": "text"
      },
      "source": [
        "# Data core\n",
        "\n",
        "> Core functionality for gathering data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFrOW0EIZxud",
        "colab_type": "text"
      },
      "source": [
        "The classes here provide functionality for applying a list of transforms to a set of items (`TfmdLists`, `Datasets`) or a `DataLoader` (`TfmdDl`) as well as the base class used to gather the data for model training: `DataLoaders`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoAF632oZxud",
        "colab_type": "text"
      },
      "source": [
        "## TfmdDL -"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owblst7leglT",
        "colab_type": "text"
      },
      "source": [
        "###**walk-note** [TypeDispatch](https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/#transform) \n",
        "\" when you call a function or class with some argument x, look at type of argument x and find appropriate function or method to call based on type of argument \"\n",
        "\n",
        "* The fastai type dispatch system is like the [functools.singledispatch](https://www.blog.pythonlibrary.org/2016/02/23/python-3-function-overloading-with-singledispatch/) system provided in the Python standard library while supporting multiple dispatch over two parameters (input and target).\n",
        "\n",
        "Here is an example of creating two different methods which dispatch based on parameter types:\n",
        "```\n",
        "@typedispatch\n",
        "def f_td_test(x:numbers.Integral, y): return x+1\n",
        "\n",
        "@typedispatch\n",
        "def f_td_test(x:int, y:float): return x+y\n",
        "```\n",
        "Here f_td_test has a generic implementation for x of numeric types and all ys, then a specialized implementation when x is an int and y is a float.\n",
        "\n",
        "[Implemented in fastcore](https://fastcore.fast.ai/dispatch#TypeDispatch)\n",
        "\n",
        "**Object-oriented semantic tensors**\n",
        "\n",
        "* fastai provides a new tensor base class, which can be easily instantiated and subclass. \n",
        "* fastai also patches PyTorch’s tensor class to attempt to maintain subclass information through operations wherever possible (not always possible)\n",
        "* all fastai Transform automatically maintain subclass types appropriately (see [fastcore](https://github.com/fastai/fastcore/blob/master/fastcore/dispatch.py#L153)).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMP4P0XrZxue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "@typedispatch\n",
        "def show_batch(x, y, samples, ctxs=None, max_n=9, **kwargs):\n",
        "    if ctxs is None: ctxs = Inf.nones\n",
        "    if hasattr(samples[0], 'show'):\n",
        "        ctxs = [s.show(ctx=c, **kwargs) for s,c,_ in zip(samples,ctxs,range(max_n))]\n",
        "    else:\n",
        "        for i in range_of(samples[0]):\n",
        "            ctxs = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs,range(max_n))]\n",
        "    return ctxs"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhOxn6laZxug",
        "colab_type": "text"
      },
      "source": [
        "`show_batch` is a type-dispatched function that is responsible for showing decoded `samples`. `x` and `y` are the input and the target in the batch to be shown, and are passed along to dispatch on their types. There is a different implementation of `show_batch` if `x` is a `TensorImage` or a `TensorText` for instance (see vision.core or text.data for more details). `ctxs` can be passed but the function is responsible to create them if necessary. `kwargs` depend on the specific implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJODE3sIZxug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "@typedispatch\n",
        "def show_results(x, y, samples, outs, ctxs=None, max_n=9, **kwargs):\n",
        "    if ctxs is None: ctxs = Inf.nones\n",
        "    for i in range(len(samples[0])):\n",
        "        ctxs = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs,range(max_n))]\n",
        "    for i in range(len(outs[0])):\n",
        "        ctxs = [b.show(ctx=c, **kwargs) for b,c,_ in zip(outs.itemgot(i),ctxs,range(max_n))]\n",
        "    return ctxs"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHUM1OfRZxui",
        "colab_type": "text"
      },
      "source": [
        "`show_results` is a type-dispatched function that is responsible for showing decoded `samples` and their corresponding `outs`. Like in `show_batch`, `x` and `y` are the input and the target in the batch to be shown, and are passed along to dispatch on their types. `ctxs` can be passed but the function is responsible to create them if necessary. `kwargs` depend on the specific implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSTIB5ThZxui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "_all_ = [\"show_batch\", \"show_results\"]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI_zJg9KZxul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "_batch_tfms = ('after_item','before_batch','after_batch')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W55CE3Ua5YO8",
        "colab_type": "text"
      },
      "source": [
        "###**walk-note** [**kwargs and @delegates](https://www.fast.ai/2019/08/06/delegation/)\n",
        "\n",
        "\n",
        "**\\*\\*kwargs**\n",
        "*   We can use the special syntax of \\*args and \\*\\*kwargs within a function definition in order to pass a variable number of arguments to the function.  \n",
        "\\*\\*kwargs in a parameter means “put any additional keyword arguments into a dict called kwarg.\n",
        "\n",
        "\n",
        "```\n",
        "class ProductPage(WebPage):\n",
        "    def __init__(self, title, price, cost, **kwargs):\n",
        "        super().__init__(title, **kwargs)\n",
        "        ...\n",
        "p = ProductPage('Soap', 15.0, 10.50, category='Bathroom', author=\"Sylvain\")\n",
        "```\n",
        "*  problem: the environment doesn’t know what parameters are available, so things like tab-completion of parameter names and popup lists of signatures won’t work.\n",
        "\n",
        "**delegated inheritance** [implemented in fastcore](https://fastcore.fast.ai/foundation#delegates)\n",
        "\n",
        "The solution from fastai2 is to create a decorator:\n",
        "```\n",
        "@delegates()\n",
        "class ProductPage(WebPage):\n",
        "    def __init__(self, title, price, cost, **kwargs):\n",
        "        super().__init__(title, **kwargs)\n",
        "        ...\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djm2GrhV9bNC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d70bfddb-717f-484f-c925-dbd120266c9c"
      },
      "source": [
        "# walk-note\n",
        "# execute this cell AFTER the next cell\n",
        "print(inspect.signature(TfmdDL))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(dataset, bs=64, shuffle=False, num_workers=None, verbose=False, do_setup=True, pin_memory=False, timeout=0, batch_size=None, drop_last=False, indexed=None, n=None, device=None, *, wif=None, before_iter=None, after_item=None, before_batch=None, after_batch=None, after_iter=None, create_batches=None, create_item=None, create_batch=None, retain=None, get_idxs=None, sample=None, shuffle_fn=None, do_batch=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_R-QStqh9vZ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "23156ed8-357d-427d-f29d-f2ff0b0af8a5"
      },
      "source": [
        "show_doc(DataLoader)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<h2 id=\"DataLoader\" class=\"doc_header\"><code>class</code> <code>DataLoader</code><a href=\"https://github.com/fastai/fastai2/tree/master/fastai2/data/load.py#L61\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n\n> <code>DataLoader</code>(**`dataset`**=*`None`*, **`bs`**=*`None`*, **`num_workers`**=*`0`*, **`pin_memory`**=*`False`*, **`timeout`**=*`0`*, **`batch_size`**=*`None`*, **`shuffle`**=*`False`*, **`drop_last`**=*`False`*, **`indexed`**=*`None`*, **`n`**=*`None`*, **`device`**=*`None`*, **`wif`**=*`None`*, **`before_iter`**=*`None`*, **`after_item`**=*`None`*, **`before_batch`**=*`None`*, **`after_batch`**=*`None`*, **`after_iter`**=*`None`*, **`create_batches`**=*`None`*, **`create_item`**=*`None`*, **`create_batch`**=*`None`*, **`retain`**=*`None`*, **`get_idxs`**=*`None`*, **`sample`**=*`None`*, **`shuffle_fn`**=*`None`*, **`do_batch`**=*`None`*) :: `GetAttr`\n\nInherit from this to have all attr accesses in `self._xtra` passed down to `self.default`",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT2qqzH4Zxun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "@log_args(but_as=DataLoader.__init__)\n",
        "@delegates()\n",
        "class TfmdDL(DataLoader):\n",
        "    \"Transformed `DataLoader`\"\n",
        "    def __init__(self, dataset, bs=64, shuffle=False, num_workers=None, verbose=False, do_setup=True, **kwargs):\n",
        "        if num_workers is None: num_workers = min(16, defaults.cpus)\n",
        "        for nm in _batch_tfms: kwargs[nm] = Pipeline(kwargs.get(nm,None))\n",
        "        super().__init__(dataset, bs=bs, shuffle=shuffle, num_workers=num_workers, **kwargs)\n",
        "        if do_setup:\n",
        "            for nm in _batch_tfms: \n",
        "                pv(f\"Setting up {nm}: {kwargs[nm]}\", verbose)\n",
        "                kwargs[nm].setup(self)\n",
        "\n",
        "    def _one_pass(self):\n",
        "        b = self.do_batch([self.do_item(0)])\n",
        "        if self.device is not None: b = to_device(b, self.device)\n",
        "        its = self.after_batch(b)\n",
        "        self._n_inp = 1 if not isinstance(its, (list,tuple)) or len(its)==1 else len(its)-1\n",
        "        self._types = explode_types(its)\n",
        "\n",
        "    def _retain_dl(self,b):\n",
        "        if not getattr(self, '_types', None): self._one_pass()\n",
        "        return retain_types(b, typs=self._types)\n",
        "\n",
        "    @delegates(DataLoader.new)\n",
        "    def new(self, dataset=None, cls=None, **kwargs):\n",
        "        res = super().new(dataset, cls, do_setup=False, **kwargs)\n",
        "        if not hasattr(self, '_n_inp') or not hasattr(self, '_types'):\n",
        "            try: \n",
        "                self._one_pass()\n",
        "                res._n_inp,res._types = self._n_inp,self._types\n",
        "            except: print(\"Could not do one pass in your dataloader, there is something wrong in it\")\n",
        "        else: res._n_inp,res._types = self._n_inp,self._types\n",
        "        return res\n",
        "\n",
        "    def before_iter(self):\n",
        "        super().before_iter()\n",
        "        split_idx = getattr(self.dataset, 'split_idx', None)\n",
        "        for nm in _batch_tfms:\n",
        "            f = getattr(self,nm)\n",
        "            if isinstance(f,Pipeline): f.split_idx=split_idx\n",
        "\n",
        "    def decode(self, b): return self.before_batch.decode(to_cpu(self.after_batch.decode(self._retain_dl(b))))\n",
        "    def decode_batch(self, b, max_n=9, full=True): return self._decode_batch(self.decode(b), max_n, full)\n",
        "\n",
        "    def _decode_batch(self, b, max_n=9, full=True):\n",
        "        f = self.after_item.decode\n",
        "        f = compose(f, partial(getattr(self.dataset,'decode',noop), full = full))\n",
        "        return L(batch_to_samples(b, max_n=max_n)).map(f)\n",
        "\n",
        "    def _pre_show_batch(self, b, max_n=9):\n",
        "        \"Decode `b` to be ready for `show_batch`\"\n",
        "        b = self.decode(b)\n",
        "        if hasattr(b, 'show'): return b,None,None\n",
        "        its = self._decode_batch(b, max_n, full=False)\n",
        "        if not is_listy(b): b,its = [b],L((o,) for o in its)\n",
        "        return detuplify(b[:self.n_inp]),detuplify(b[self.n_inp:]),its\n",
        "\n",
        "    def show_batch(self, b=None, max_n=9, ctxs=None, show=True, unique=False, **kwargs):\n",
        "        if unique:\n",
        "            old_get_idxs = self.get_idxs\n",
        "            self.get_idxs = lambda: Inf.zeros\n",
        "        if b is None: b = self.one_batch()\n",
        "        if not show: return self._pre_show_batch(b, max_n=max_n)\n",
        "        show_batch(*self._pre_show_batch(b, max_n=max_n), ctxs=ctxs, max_n=max_n, **kwargs)\n",
        "        if unique: self.get_idxs = old_get_idxs\n",
        "\n",
        "    def show_results(self, b, out, max_n=9, ctxs=None, show=True, **kwargs):\n",
        "        x,y,its = self.show_batch(b, max_n=max_n, show=False)\n",
        "        b_out = type(b)(b[:self.n_inp] + (tuple(out) if is_listy(out) else (out,))) \n",
        "        x1,y1,outs = self.show_batch(b_out, max_n=max_n, show=False)\n",
        "        res = (x,x1,None,None) if its is None else (x, y, its, outs.itemgot(slice(self.n_inp,None)))\n",
        "        if not show: return res\n",
        "        show_results(*res, ctxs=ctxs, max_n=max_n, **kwargs)\n",
        "\n",
        "    @property\n",
        "    def n_inp(self):\n",
        "        if hasattr(self.dataset, 'n_inp'): return self.dataset.n_inp\n",
        "        if not hasattr(self, '_n_inp'): self._one_pass()\n",
        "        return self._n_inp\n",
        "    \n",
        "    def to(self, device):\n",
        "        self.device = device\n",
        "        for tfm in self.after_batch.fs:\n",
        "            for a in L(getattr(tfm, 'parameters', None)): setattr(tfm, a, getattr(tfm, a).to(device))\n",
        "        return self"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P84vZVp3Zxup",
        "colab_type": "text"
      },
      "source": [
        "A `TfmdDL` is a `DataLoader` that creates `Pipeline` from a list of `Transform`s for the callbacks `after_item`, `before_batch` and `after_batch`. As a result, it can decode or show a processed `batch`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTxvF-SBZxuq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "add_docs(TfmdDL,\n",
        "         decode=\"Decode `b` using `tfms`\",\n",
        "         decode_batch=\"Decode `b` entirely\",\n",
        "         new=\"Create a new version of self with a few changed attributes\",\n",
        "         show_batch=\"Show `b` (defaults to `one_batch`), a list of lists of pipeline outputs (i.e. output of a `DataLoader`)\",\n",
        "         show_results=\"Show each item of `b` and `out`\",\n",
        "         before_iter=\"override\",\n",
        "         to=\"Put self and its transforms state on `device`\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS6EH99YZxus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class _Category(int, ShowTitle): pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUWU0RKbLHAL",
        "colab_type": "text"
      },
      "source": [
        "### **walk-note** \n",
        "all fastai Transform automatically maintain subclass types appropriately (see [fastcore](https://github.com/fastai/fastcore/blob/master/fastcore/dispatch.py#L153)).\n",
        "\n",
        "\n",
        "```\n",
        "\"Cast `new` to type of `old` or `typ` if it's a superclass\"\n",
        "```\n",
        "In this case, cast torch.Tensor to TensorImage, since Tensor is a superclass\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIt6sZ4YMrkL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "0154af30-c521-4d1d-9699-c5421feb1446"
      },
      "source": [
        "show_doc(TensorImage)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<h2 id=\"TensorImage\" class=\"doc_header\"><code>class</code> <code>TensorImage</code><a href=\"https://github.com/fastai/fastai2/tree/master/fastai2/torch_core.py#L301\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n\n> <code>TensorImage</code>(**`x`**, **\\*\\*`kwargs`**) :: [`TensorImageBase`](/torch_core#TensorImageBase)\n\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrQG5MhYNdtt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "099cf815-2be2-4e2a-8fb9-8ae429cddce5"
      },
      "source": [
        "show_doc(TensorBase)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<h2 id=\"TensorBase\" class=\"doc_header\"><code>class</code> <code>TensorBase</code><a href=\"https://github.com/fastai/fastai2/tree/master/fastai2/torch_core.py#L241\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n\n> <code>TensorBase</code>(**`x`**, **\\*\\*`kwargs`**) :: `Tensor`\n\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0tWCycnZxuu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0812b210-81a0-4dc7-c4cd-3195c7f96c46"
      },
      "source": [
        "#Test retain type\n",
        "class NegTfm(Transform):\n",
        "    def encodes(self, x): return torch.neg(x)\n",
        "    def decodes(self, x): return torch.neg(x)\n",
        "    \n",
        "tdl = TfmdDL([(TensorImage([1]),)] * 4, after_batch=NegTfm(), bs=4, num_workers=4)\n",
        "b = tdl.one_batch()\n",
        "print(type(b[0]))\n",
        "test_eq(type(b[0]), TensorImage)\n",
        "b = (tensor([1.,1.,1.,1.]),)\n",
        "test_eq(type(tdl.decode_batch(b)[0][0]), TensorImage)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'fastai2.torch_core.TensorImage'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXa-wAFSKbCY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c650f44f-7e19-4f13-ab54-06015a5d5fa5"
      },
      "source": [
        "# walk-note This would fail, because the output of torch.neg is a tensor\n",
        "my_x = torch.neg(TensorImage([1]))\n",
        "type(my_x)\n",
        "# test_eq(type(my_x), TensorImage)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zlm8mLesLgLu",
        "colab_type": "text"
      },
      "source": [
        "### **walk-note** \n",
        "No cast needed per \n",
        "\n",
        "``` ->None ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4_x7lFjZxuw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class A(Transform): \n",
        "    def encodes(self, x): return x \n",
        "    def decodes(self, x): return TitledInt(x) \n",
        "\n",
        "@Transform\n",
        "def f(x)->None: return Tuple((x,x))\n",
        "\n",
        "start = torch.arange(50)\n",
        "test_eq_type(f(2), Tuple((2,2)))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwqGb-uHI1TO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "d09e33f5-249a-4cbc-df27-2ffee8365dcb"
      },
      "source": [
        "show_doc(TitledInt)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<h2 id=\"TitledInt\" class=\"doc_header\"><code>class</code> <code>TitledInt</code><a href=\"https://github.com/fastai/fastai2/tree/master/fastai2/torch_core.py#L397\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n\n> <code>TitledInt</code>() :: `Int`\n\nAn `int` with `show`",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNZpT2opZxuy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = A()\n",
        "tdl = TfmdDL(start, after_item=lambda x: (a(x), f(x)), bs=4)\n",
        "x,y = tdl.one_batch()\n",
        "test_eq(type(y), Tuple)\n",
        "\n",
        "s = tdl.decode_batch((x,y))\n",
        "test_eq(type(s[0][1]), Tuple)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRnBHstgOZhX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e7a9cf90-12b3-4360-93c6-e85064e1b796"
      },
      "source": [
        "# walk-note\n",
        "print(y)\n",
        "print(type(x),type(y))\n",
        "print(s[0])\n",
        "print(type(s[0][1]))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3]))\n",
            "<class 'torch.Tensor'> <class 'fastcore.utils.Tuple'>\n",
            "(tensor(0), (tensor(0), tensor(0)))\n",
            "<class 'fastcore.utils.Tuple'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93bXILN0Q1nc",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Zhyz5CGP8aq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "6b5870ca-113e-4782-ba51-9d244c158481"
      },
      "source": [
        "show_doc(test_stdout)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<h4 id=\"test_stdout\" class=\"doc_header\"><code>test_stdout</code><a href=\"https://github.com/fastai/fastcore/tree/master/fastcore/test.py#L72\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>test_stdout</code>(**`f`**, **`exp`**, **`regex`**=*`False`*)\n\nTest that `f` prints `exp` to stdout, optionally checking as `regex`",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uJCQYutQ7nd",
        "colab_type": "text"
      },
      "source": [
        "### **walk-note** [test-stdout](https://github.com/fastai/fastcore/blob/master/fastcore/test.py#L72)\n",
        "\n",
        "What is the *unique* parameter in show_batch?\n",
        "see [code](https://github.com/fastai/fastai2/blob/master/fastai2/data/core.py#L93)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Doxra_jZxuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tdl = TfmdDL(torch.arange(0,50), after_item=A(), after_batch=NegTfm(), bs=4)\n",
        "test_eq(tdl.dataset[0], start[0])\n",
        "test_eq(len(tdl), (50-1)//4+1)\n",
        "test_eq(tdl.bs, 4)\n",
        "test_stdout(tdl.show_batch, '0\\n1\\n2\\n3')\n",
        "test_stdout(partial(tdl.show_batch, unique=True), '0\\n0\\n0\\n0')"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isntmdvD4IIt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "154c1a4b-8ec9-43dc-91c4-da549815d63a"
      },
      "source": [
        "tdl.show_batch()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qlJGyWi3oVp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "10b6753d-913b-4ddc-bf66-67f197af222e"
      },
      "source": [
        "tdl.show_batch(unique=True)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr5RFGyzRgAK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a5d181a4-e82d-4d66-8c10-862a71c71f70"
      },
      "source": [
        "# walk-note ????\n",
        "x = partial(tdl.show_batch, unique=True)\n",
        "x()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjOTRDLiSsu4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "outputId": "3209100b-6680-4acf-b045-a702cd6c85ff"
      },
      "source": [
        "show_doc(tdl.show_batch)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<h4 id=\"TfmdDL.show_batch\" class=\"doc_header\"><code>TfmdDL.show_batch</code><a href=\"__main__.py#L60\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>TfmdDL.show_batch</code>(**`b`**=*`None`*, **`max_n`**=*`9`*, **`ctxs`**=*`None`*, **`show`**=*`True`*, **`unique`**=*`False`*, **\\*\\*`kwargs`**)\n\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvKIEpMiPrex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "95837d5c-35e8-42d1-b5b9-9498294345b3"
      },
      "source": [
        "tdl.show_batch()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHiqJyrrUCM_",
        "colab_type": "text"
      },
      "source": [
        "### **walk-note**\n",
        "\n",
        "The transform parameter is stored in the cpu until is sent to default device\n",
        "\n",
        "* after_batch typically you want it in the gpu\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm2UlUATZxu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class B(Transform):\n",
        "    parameters = 'a'\n",
        "    def __init__(self): self.a = torch.tensor(0.)\n",
        "    def encodes(self, x): x\n",
        "    \n",
        "tdl = TfmdDL([(TensorImage([1]),)] * 4, after_batch=B(), bs=4)\n",
        "test_eq(tdl.after_batch.fs[0].a.device, torch.device('cpu'))\n",
        "tdl.to(default_device())\n",
        "test_eq(tdl.after_batch.fs[0].a.device, default_device())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD13blkg5TeL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76789633-fab3-4edc-8bb4-8a2d8e8881e8"
      },
      "source": [
        "default_device()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uay1SPozTnfe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3d7d1f8-6305-4286-c975-340e28114c36"
      },
      "source": [
        "tdl.after_batch.fs[0].a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XmP7lrDUOlf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "36f5ad7a-5db4-4f56-944d-244fcfd89c77"
      },
      "source": [
        "show_doc(default_device)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<h4 id=\"default_device\" class=\"doc_header\"><code>default_device</code><a href=\"https://github.com/fastai/fastai2/tree/master/fastai2/torch_core.py#L185\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>default_device</code>(**`use_cuda`**=*`-1`*)\n\nReturn or set default device; `use_cuda`: None - CUDA if available; True - error if not availabe; False - CPU",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2QJh6MGZxu3",
        "colab_type": "text"
      },
      "source": [
        "### Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWcpXZiQZxu4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "outputId": "76463652-fbe1-444c-d857-5aaa19ae844c"
      },
      "source": [
        "show_doc(TfmdDL.one_batch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<h4 id=\"DataLoader.one_batch\" class=\"doc_header\"><code>DataLoader.one_batch</code><a href=\"https://github.com/fastai/fastai2/tree/master/fastai2/data/load.py#L130\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>DataLoader.one_batch</code>()\n\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6rWQ6ClZxu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfm = NegTfm()\n",
        "tdl = TfmdDL(start, after_batch=tfm, bs=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd_dryLNZxu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b = tdl.one_batch()\n",
        "test_eq(tensor([0,-1,-2,-3]), b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrsDiHGpZxu9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "c923a91d-68cf-4c38-d172-fd0cddd643c5"
      },
      "source": [
        "show_doc(TfmdDL.decode)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<h4 id=\"TfmdDL.decode\" class=\"doc_header\"><code>TfmdDL.decode</code><a href=\"__main__.py#L44\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>TfmdDL.decode</code>(**`b`**)\n\nDecode `b` using `tfms`",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GThkPIoUZxu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_eq(tdl.decode(b), tensor(0,1,2,3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZmD-qZoZxvB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "8596a5c5-1367-4666-ec06-7db670b8f9f5"
      },
      "source": [
        "show_doc(TfmdDL.decode_batch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<h4 id=\"TfmdDL.decode_batch\" class=\"doc_header\"><code>TfmdDL.decode_batch</code><a href=\"__main__.py#L45\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>TfmdDL.decode_batch</code>(**`b`**, **`max_n`**=*`9`*, **`full`**=*`True`*)\n\nDecode `b` entirely",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpJu342PZxvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_eq(tdl.decode_batch(b), [0,1,2,3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_C7vCzxZxvE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "5993d6dc-a026-46a9-fec7-b21c62a32467"
      },
      "source": [
        "show_doc(TfmdDL.show_batch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<h4 id=\"TfmdDL.show_batch\" class=\"doc_header\"><code>TfmdDL.show_batch</code><a href=\"__main__.py#L60\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>TfmdDL.show_batch</code>(**`b`**=*`None`*, **`max_n`**=*`9`*, **`ctxs`**=*`None`*, **`show`**=*`True`*, **`unique`**=*`False`*, **\\*\\*`kwargs`**)\n\nShow `b` (defaults to `one_batch`), a list of lists of pipeline outputs (i.e. output of a [`DataLoader`](/data.load#DataLoader))",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1Z4TXd1ZxvG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "6ff438e4-8c2b-49d2-951d-27e64878af50"
      },
      "source": [
        "show_doc(TfmdDL.to)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<h4 id=\"TfmdDL.to\" class=\"doc_header\"><code>TfmdDL.to</code><a href=\"__main__.py#L83\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>TfmdDL.to</code>(**`device`**)\n\nPut self and its transforms state on `device`",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp-IBrjEZxvI",
        "colab_type": "text"
      },
      "source": [
        "## DataLoaders -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL6zcyqSZxvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# export\n",
        "@docs\n",
        "class DataLoaders(GetAttr):\n",
        "    \"Basic wrapper around several `DataLoader`s.\"\n",
        "    _default='train'\n",
        "    def __init__(self, *loaders, path='.', device=None):\n",
        "        self.loaders,self.path = list(loaders),Path(path)\n",
        "        self.device = device\n",
        "\n",
        "    def __getitem__(self, i): return self.loaders[i]\n",
        "    def new_empty(self):\n",
        "        loaders = [dl.new(dl.dataset.new_empty()) for dl in self.loaders]\n",
        "        return type(self)(*loaders, path=self.path, device=self.device)\n",
        "    \n",
        "    def _set(i, self, v): self.loaders[i] = v\n",
        "    train   ,valid    = add_props(lambda i,x: x[i], _set)\n",
        "    train_ds,valid_ds = add_props(lambda i,x: x[i].dataset)\n",
        "\n",
        "    @property\n",
        "    def device(self): return self._device\n",
        "\n",
        "    @device.setter\n",
        "    def device(self, d):\n",
        "        for dl in self.loaders: dl.to(d)\n",
        "        self._device = d\n",
        "\n",
        "    def to(self, device): \n",
        "        self.device = device\n",
        "        return self\n",
        "    \n",
        "    def cuda(self): return self.to(device=default_device())\n",
        "    def cpu(self):  return self.to(device=torch.device('cpu'))\n",
        "    \n",
        "    @classmethod\n",
        "    def from_dsets(cls, *ds, path='.',  bs=64, device=None, dl_type=TfmdDL, **kwargs):\n",
        "        default = (True,) + (False,) * (len(ds)-1)\n",
        "        defaults = {'shuffle': default, 'drop_last': default}\n",
        "        for nm in _batch_tfms: \n",
        "            if nm in kwargs: kwargs[nm] = Pipeline(kwargs[nm])\n",
        "        kwargs = merge(defaults, {k: tuplify(v, match=ds) for k,v in kwargs.items()})\n",
        "        kwargs = [{k: v[i] for k,v in kwargs.items()} for i in range_of(ds)]\n",
        "        return cls(*[dl_type(d, bs=bs, **k) for d,k in zip(ds, kwargs)], path=path, device=device)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dblock(cls, dblock, source, path='.',  bs=64, val_bs=None, shuffle_train=True, device=None, **kwargs):\n",
        "        return dblock.dataloaders(source, path=path, bs=bs, val_bs=val_bs, shuffle_train=shuffle_train, device=device, **kwargs)\n",
        "\n",
        "    _docs=dict(__getitem__=\"Retrieve `DataLoader` at `i` (`0` is training, `1` is validation)\",\n",
        "               train=\"Training `DataLoader`\",\n",
        "               valid=\"Validation `DataLoader`\",\n",
        "               train_ds=\"Training `Dataset`\",\n",
        "               valid_ds=\"Validation `Dataset`\",\n",
        "               to=\"Use `device`\",\n",
        "               cuda=\"Use the gpu if available\",\n",
        "               cpu=\"Use the cpu\",\n",
        "               new_empty=\"Create a new empty version of `self` with the same transforms\",\n",
        "               from_dblock=\"Create a dataloaders from a given `dblock`\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UVLSaqyZxvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dls = DataLoaders(tdl,tdl)\n",
        "x = dls.train.one_batch()\n",
        "x2 = first(tdl)\n",
        "test_eq(x,x2)\n",
        "x2 = dls.one_batch()\n",
        "test_eq(x,x2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlAo7GMgZxvM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hide\n",
        "#test assignment works\n",
        "dls.train = dls.train.new(bs=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh_OqZkqZxvO",
        "colab_type": "text"
      },
      "source": [
        "### Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKEM9RT3ZxvO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40c0bb96-6a91-4637-c5bc-d2e7184b51a0"
      },
      "source": [
        "show_doc(DataLoaders.__getitem__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<h4 id=\"DataLoaders.__getitem__\" class=\"doc_header\"><code>DataLoaders.__getitem__</code><a href=\"__main__.py#L10\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>DataLoaders.__getitem__</code>(**`i`**)\n\nRetrieve [`DataLoader`](/data.load#DataLoader) at `i` (`0` is training, `1` is validation)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGW6N5kXZxvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x2 = dls[0].one_batch()\n",
        "test_eq(x,x2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5x6E-r8ZxvS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c152d8-3cce-4f67-f597-e5e6eadca9b2"
      },
      "source": [
        "show_doc(DataLoaders.train, name=\"DataLoaders.train\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<h4 id=\"DataLoaders.train\" class=\"doc_header\"><code>DataLoaders.train</code><a href=\"__main__.py#L16\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\nTraining [`DataLoader`](/data.load#DataLoader)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zISPL5LZxvU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2cec3b2-f54c-42c6-a863-995d5865aa26"
      },
      "source": [
        "show_doc(DataLoaders.valid, name=\"DataLoaders.valid\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<h4 id=\"DataLoaders.valid\" class=\"doc_header\"><code>DataLoaders.valid</code><a href=\"__main__.py#L16\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\nValidation [`DataLoader`](/data.load#DataLoader)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG-NmZYUZxvW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2790aa0d-65aa-4f4e-c408-751bfd2243bd"
      },
      "source": [
        "show_doc(DataLoaders.train_ds, name=\"DataLoaders.train_ds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<h4 id=\"DataLoaders.train_ds\" class=\"doc_header\"><code>DataLoaders.train_ds</code><a href=\"__main__.py#L17\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\nTraining `Dataset`",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "varZAqQcZxvY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d81c132-24f0-4d6a-cc8b-189d437641d9"
      },
      "source": [
        "show_doc(DataLoaders.valid_ds, name=\"DataLoaders.valid_ds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<h4 id=\"DataLoaders.valid_ds\" class=\"doc_header\"><code>DataLoaders.valid_ds</code><a href=\"__main__.py#L17\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\nValidation `Dataset`",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y05l0nHZxva",
        "colab_type": "text"
      },
      "source": [
        "## TfmdLists -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAT5TyrjZxva",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "class FilteredBase:\n",
        "    \"Base class for lists with subsets\"\n",
        "    _dl_type,_dbunch_type = TfmdDL,DataLoaders\n",
        "    def __init__(self, *args, dl_type=None, **kwargs):\n",
        "        if dl_type is not None: self._dl_type = dl_type\n",
        "        self.dataloaders = delegates(self._dl_type.__init__)(self.dataloaders)\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    @property\n",
        "    def n_subsets(self): return len(self.splits)\n",
        "    def _new(self, items, **kwargs): return super()._new(items, splits=self.splits, **kwargs)\n",
        "    def subset(self): raise NotImplemented\n",
        "\n",
        "    def dataloaders(self, bs=64, val_bs=None, shuffle_train=True, n=None, path='.', dl_type=None, dl_kwargs=None, \n",
        "                    device=None, **kwargs):\n",
        "        if device is None: device=default_device()\n",
        "        if dl_kwargs is None: dl_kwargs = [{}] * self.n_subsets\n",
        "        if dl_type is None: dl_type = self._dl_type\n",
        "        drop_last = kwargs.pop('drop_last', shuffle_train)\n",
        "        dl = dl_type(self.subset(0), bs=bs, shuffle=shuffle_train, drop_last=drop_last, n=n, device=device,\n",
        "                     **merge(kwargs, dl_kwargs[0]))\n",
        "        dls = [dl] + [dl.new(self.subset(i), bs=(bs if val_bs is None else val_bs), shuffle=False, drop_last=False, \n",
        "                             n=None, **dl_kwargs[i]) for i in range(1, self.n_subsets)]\n",
        "        return self._dbunch_type(*dls, path=path, device=device)\n",
        "\n",
        "FilteredBase.train,FilteredBase.valid = add_props(lambda i,x: x.subset(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GcL6EWHZxvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "class TfmdLists(FilteredBase, L, GetAttr):\n",
        "    \"A `Pipeline` of `tfms` applied to a collection of `items`\"\n",
        "    _default='tfms'\n",
        "    def __init__(self, items, tfms, use_list=None, do_setup=True, split_idx=None, train_setup=True,\n",
        "                 splits=None, types=None, verbose=False, dl_type=None):\n",
        "        super().__init__(items, use_list=use_list)\n",
        "        if dl_type is not None: self._dl_type = dl_type\n",
        "        self.splits = L([slice(None),[]] if splits is None else splits).map(mask2idxs)\n",
        "        if isinstance(tfms,TfmdLists): tfms = tfms.tfms\n",
        "        if isinstance(tfms,Pipeline): do_setup=False\n",
        "        self.tfms = Pipeline(tfms, split_idx=split_idx)\n",
        "        store_attr(self, 'types,split_idx')\n",
        "        if do_setup: \n",
        "            pv(f\"Setting up {self.tfms}\", verbose)\n",
        "            self.setup(train_setup=train_setup)\n",
        "\n",
        "    def _new(self, items, split_idx=None, **kwargs): \n",
        "        split_idx = ifnone(split_idx,self.split_idx)\n",
        "        return super()._new(items, tfms=self.tfms, do_setup=False, types=self.types, split_idx=split_idx, **kwargs)\n",
        "    def subset(self, i): return self._new(self._get(self.splits[i]), split_idx=i)\n",
        "    def _after_item(self, o): return self.tfms(o)\n",
        "    def __repr__(self): return f\"{self.__class__.__name__}: {self.items}\\ntfms - {self.tfms.fs}\"\n",
        "    def __iter__(self): return (self[i] for i in range(len(self)))\n",
        "    def show(self, o, **kwargs): return self.tfms.show(o, **kwargs)\n",
        "    def decode(self, o, **kwargs): return self.tfms.decode(o, **kwargs)\n",
        "    def __call__(self, o, **kwargs): return self.tfms.__call__(o, **kwargs)\n",
        "    def overlapping_splits(self): return L(Counter(self.splits.concat()).values()).filter(gt(1))\n",
        "    def new_empty(self): return self._new([])\n",
        "\n",
        "    def setup(self, train_setup=True):\n",
        "        self.tfms.setup(self, train_setup)\n",
        "        if len(self) != 0:\n",
        "            x = super().__getitem__(0) if self.splits is None else super().__getitem__(self.splits[0])[0]\n",
        "            self.types = []\n",
        "            for f in self.tfms.fs:\n",
        "                self.types.append(getattr(f, 'input_types', type(x)))\n",
        "                x = f(x)\n",
        "            self.types.append(type(x))\n",
        "        types = L(t if is_listy(t) else [t] for t in self.types).concat().unique()\n",
        "        self.pretty_types = '\\n'.join([f'  - {t}' for t in types])\n",
        "\n",
        "    def infer_idx(self, x):\n",
        "        idx = 0\n",
        "        for t in self.types:\n",
        "            if isinstance(x, t): break\n",
        "            idx += 1\n",
        "        types = L(t if is_listy(t) else [t] for t in self.types).concat().unique()\n",
        "        pretty_types = '\\n'.join([f'  - {t}' for t in types])\n",
        "        assert idx < len(self.types), f\"Expected an input of type in \\n{pretty_types}\\n but got {type(x)}\"\n",
        "        return idx\n",
        "\n",
        "    def infer(self, x):\n",
        "        return compose_tfms(x, tfms=self.tfms.fs[self.infer_idx(x):], split_idx=self.split_idx)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        res = super().__getitem__(idx)\n",
        "        if self._after_item is None: return res\n",
        "        return self._after_item(res) if is_indexer(idx) else res.map(self._after_item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71csmYTFZxve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "add_docs(TfmdLists,\n",
        "         setup=\"Transform setup with self\",\n",
        "         decode=\"From `Pipeline\",\n",
        "         show=\"From `Pipeline\",\n",
        "         overlapping_splits=\"All splits that are in more than one split\",\n",
        "         subset=\"New `TfmdLists` with same tfms that only includes items in `i`th split\",\n",
        "         infer_idx=\"Finds the index where `self.tfms` can be applied to `x`, depending on the type of `x`\",\n",
        "         infer=\"Apply `self.tfms` to `x` starting at the right tfm depending on the type of `x`\",\n",
        "         new_empty=\"A new version of `self` but with no items\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K-Rj1TzZxvg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#exports\n",
        "def decode_at(o, idx):\n",
        "    \"Decoded item at `idx`\"\n",
        "    return o.decode(o[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVYtY0xdZxvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#exports\n",
        "def show_at(o, idx, **kwargs):\n",
        "    \"Show item at `idx`\",\n",
        "    return o.show(o[idx], **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK_NJIZnZxvk",
        "colab_type": "text"
      },
      "source": [
        "A `TfmdLists` combines a collection of object with a `Pipeline`. `tfms` can either be a `Pipeline` or a list of transforms, in which case, it will wrap them in a `Pipeline`. `use_list` is passed along to `L` with the `items` and `split_idx` are passed to each transform of the `Pipeline`. `do_setup` indicates if the `Pipeline.setup` method should be called during initialization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDim-veWZxvk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class _IntFloatTfm(Transform):\n",
        "    def encodes(self, o):  return TitledInt(o)\n",
        "    def decodes(self, o):  return TitledFloat(o)\n",
        "int2f_tfm=_IntFloatTfm()\n",
        "\n",
        "def _neg(o): return -o\n",
        "neg_tfm = Transform(_neg, _neg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKeaheRqZxvn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0fbc1b97-90d9-422a-d148-277d0aedd422"
      },
      "source": [
        "items = L([1.,2.,3.]); tfms = [neg_tfm, int2f_tfm]\n",
        "tl = TfmdLists(items, tfms=tfms)\n",
        "test_eq_type(tl[0], TitledInt(-1))\n",
        "test_eq_type(tl[1], TitledInt(-2))\n",
        "test_eq_type(tl.decode(tl[2]), TitledFloat(3.))\n",
        "test_stdout(lambda: show_at(tl, 2), '-3')\n",
        "test_eq(tl.types, [float, float, TitledInt])\n",
        "tl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfmdLists: [1.0, 2.0, 3.0]\n",
              "tfms - (#2) [_neg: (object,object) -> _neg (object,object) -> _neg,_IntFloatTfm: (object,object) -> encodes (object,object) -> decodes]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHM6mmT0Zxvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add splits to TfmdLists\n",
        "splits = [[0,2],[1]]\n",
        "tl = TfmdLists(items, tfms=tfms, splits=splits)\n",
        "test_eq(tl.n_subsets, 2)\n",
        "test_eq(tl.train, tl.subset(0))\n",
        "test_eq(tl.valid, tl.subset(1))\n",
        "test_eq(tl.train.items, items[splits[0]])\n",
        "test_eq(tl.valid.items, items[splits[1]])\n",
        "test_eq(tl.train.tfms.split_idx, 0)\n",
        "test_eq(tl.valid.tfms.split_idx, 1)\n",
        "test_eq(tl.train.new_empty().split_idx, 0)\n",
        "test_eq(tl.valid.new_empty().split_idx, 1)\n",
        "test_eq_type(tl.splits, L(splits))\n",
        "assert not tl.overlapping_splits()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSZXtcG8Zxvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(dict(a=[1,2,3],b=[2,3,4]))\n",
        "tl = TfmdLists(df, lambda o: o.a+1, splits=[[0],[1,2]])\n",
        "test_eq(tl[1,2], [3,4])\n",
        "tr = tl.subset(0)\n",
        "test_eq(tr[:], [2])\n",
        "val = tl.subset(1)\n",
        "test_eq(val[:], [3,4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXcuK4PbZxvu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dff8908f-8ec4-4eea-ced9-59f9867a09c1"
      },
      "source": [
        "class _B(Transform):\n",
        "    def __init__(self): self.m = 0\n",
        "    def encodes(self, o): return o+self.m\n",
        "    def decodes(self, o): return o-self.m\n",
        "    def setups(self, items): \n",
        "        print(items)\n",
        "        self.m = tensor(items).float().mean().item()\n",
        "\n",
        "# test for setup, which updates `self.m`\n",
        "tl = TfmdLists(items, _B())\n",
        "test_eq(tl.m, 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TfmdLists: [1.0, 2.0, 3.0]\n",
            "tfms - (#0) []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fDczlzqZxvw",
        "colab_type": "text"
      },
      "source": [
        "Here's how we can use `TfmdLists.setup` to implement a simple category list, getting labels from a mock file list:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUeChEyLZxvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class _Cat(Transform):\n",
        "    order = 1\n",
        "    def encodes(self, o):    return int(self.o2i[o])\n",
        "    def decodes(self, o):    return TitledStr(self.vocab[o])\n",
        "    def setups(self, items): self.vocab,self.o2i = uniqueify(L(items), sort=True, bidir=True)\n",
        "tcat = _Cat()\n",
        "\n",
        "def _lbl(o): return TitledStr(o.split('_')[0])\n",
        "\n",
        "# Check that tfms are sorted by `order` & `_lbl` is called first\n",
        "fns = ['dog_0.jpg','cat_0.jpg','cat_2.jpg','cat_1.jpg','dog_1.jpg']\n",
        "tl = TfmdLists(fns, [tcat,_lbl])\n",
        "exp_voc = ['cat','dog']\n",
        "test_eq(tcat.vocab, exp_voc)\n",
        "test_eq(tl.tfms.vocab, exp_voc)\n",
        "test_eq(tl.vocab, exp_voc)\n",
        "test_eq(tl, (1,0,0,0,1))\n",
        "test_eq([tl.decode(o) for o in tl], ('dog','cat','cat','cat','dog'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWVrfcmBZxvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Check only the training set is taken into account for setup\n",
        "tl = TfmdLists(fns, [tcat,_lbl], splits=[[0,4], [1,2,3]])\n",
        "test_eq(tcat.vocab, ['dog'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5M4ho9PZxv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfm = NegTfm(split_idx=1)\n",
        "tds = TfmdLists(start, A())\n",
        "tdl = TfmdDL(tds, after_batch=tfm, bs=4)\n",
        "x = tdl.one_batch()\n",
        "test_eq(x, torch.arange(4))\n",
        "tds.split_idx = 1\n",
        "x = tdl.one_batch()\n",
        "test_eq(x, -torch.arange(4))\n",
        "tds.split_idx = 0\n",
        "x = tdl.one_batch()\n",
        "test_eq(x, torch.arange(4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_k6a6GD2Zxv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tds = TfmdLists(start, A())\n",
        "tdl = TfmdDL(tds, after_batch=NegTfm(), bs=4)\n",
        "test_eq(tdl.dataset[0], start[0])\n",
        "test_eq(len(tdl), (len(tds)-1)//4+1)\n",
        "test_eq(tdl.bs, 4)\n",
        "test_stdout(tdl.show_batch, '0\\n1\\n2\\n3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7mONcbOZxv3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "c0a5ddd1-5179-4639-f370-d04dd450d593"
      },
      "source": [
        "show_doc(TfmdLists.subset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<h4 id=\"TfmdLists.subset\" class=\"doc_header\"><code>TfmdLists.subset</code><a href=\"__main__.py#L21\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>TfmdLists.subset</code>(**`i`**)\n\nNew [`TfmdLists`](/data.core#TfmdLists) with same tfms that only includes items in `i`th split",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mBVzsW3Zxv4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "2f13ac39-9d49-4a61-8646-a93799180c9c"
      },
      "source": [
        "show_doc(TfmdLists.infer_idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<h4 id=\"TfmdLists.infer_idx\" class=\"doc_header\"><code>TfmdLists.infer_idx</code><a href=\"__main__.py#L43\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>TfmdLists.infer_idx</code>(**`x`**)\n\nFinds the index where `self.tfms` can be applied to `x`, depending on the type of `x`",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29ca_AkDZxv6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "ebcd08fb-c527-4cd6-b731-e1ba6da31624"
      },
      "source": [
        "show_doc(TfmdLists.infer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<h4 id=\"TfmdLists.infer\" class=\"doc_header\"><code>TfmdLists.infer</code><a href=\"__main__.py#L53\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>TfmdLists.infer</code>(**`x`**)\n\nApply `self.tfms` to `x` starting at the right tfm depending on the type of `x`",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr74Y80QZxv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mult(x): return x*2\n",
        "mult.order = 2\n",
        "\n",
        "fns = ['dog_0.jpg','cat_0.jpg','cat_2.jpg','cat_1.jpg','dog_1.jpg']\n",
        "tl = TfmdLists(fns, [_lbl,_Cat(),mult])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW-xdGfAZxwB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_eq(tl.infer_idx('dog_45.jpg'), 0)\n",
        "test_eq(tl.infer('dog_45.jpg'), 2)\n",
        "\n",
        "test_eq(tl.infer_idx(4), 2)\n",
        "test_eq(tl.infer(4), 8)\n",
        "\n",
        "test_fail(lambda: tl.infer_idx(2.0))\n",
        "test_fail(lambda: tl.infer(2.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6iAyUnuZxwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hide\n",
        "#Test input_types works on a Transform\n",
        "cat = _Cat()\n",
        "cat.input_types = (str, float)\n",
        "tl = TfmdLists(fns, [_lbl,cat,mult])\n",
        "test_eq(tl.infer_idx(2.0), 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS4XbayuZxwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hide\n",
        "#Test type annotations work on a function\n",
        "def mult(x:(int,float)): return x*2\n",
        "mult.order = 2\n",
        "tl = TfmdLists(fns, [_lbl,_Cat(),mult])\n",
        "test_eq(tl.infer_idx(2.0), 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVWbovd_ZxwG",
        "colab_type": "text"
      },
      "source": [
        "## Datasets -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRJJfDO0ZxwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "@docs\n",
        "@delegates(TfmdLists)\n",
        "class Datasets(FilteredBase):\n",
        "    \"A dataset that creates a tuple from each `tfms`, passed thru `item_tfms`\"\n",
        "    def __init__(self, items=None, tfms=None, tls=None, n_inp=None, dl_type=None, **kwargs):\n",
        "        super().__init__(dl_type=dl_type)\n",
        "        self.tls = L(tls if tls else [TfmdLists(items, t, **kwargs) for t in L(ifnone(tfms,[None]))])\n",
        "        self.n_inp = ifnone(n_inp, max(1, len(self.tls)-1))\n",
        "\n",
        "    def __getitem__(self, it):\n",
        "        res = tuple([tl[it] for tl in self.tls])\n",
        "        return res if is_indexer(it) else list(zip(*res))\n",
        "\n",
        "    def __getattr__(self,k): return gather_attrs(self, k, 'tls')\n",
        "    def __dir__(self): return super().__dir__() + gather_attr_names(self, 'tls')\n",
        "    def __len__(self): return len(self.tls[0])\n",
        "    def __iter__(self): return (self[i] for i in range(len(self)))\n",
        "    def __repr__(self): return coll_repr(self)\n",
        "    def decode(self, o, full=True): return tuple(tl.decode(o_, full=full) for o_,tl in zip(o,tuplify(self.tls, match=o)))\n",
        "    def subset(self, i): return type(self)(tls=L(tl.subset(i) for tl in self.tls), n_inp=self.n_inp)\n",
        "    def _new(self, items, *args, **kwargs): return super()._new(items, tfms=self.tfms, do_setup=False, **kwargs)\n",
        "    def overlapping_splits(self): return self.tls[0].overlapping_splits()\n",
        "    def new_empty(self): return type(self)(tls=[tl.new_empty() for tl in self.tls], n_inp=self.n_inp)\n",
        "    @property\n",
        "    def splits(self): return self.tls[0].splits\n",
        "    @property\n",
        "    def split_idx(self): return self.tls[0].tfms.split_idx\n",
        "    @property\n",
        "    def items(self): return self.tls[0].items\n",
        "    @items.setter\n",
        "    def items(self, v):\n",
        "        for tl in self.tls: tl.items = v\n",
        "\n",
        "    def show(self, o, ctx=None, **kwargs):\n",
        "        for o_,tl in zip(o,self.tls): ctx = tl.show(o_, ctx=ctx, **kwargs)\n",
        "        return ctx\n",
        "\n",
        "    @contextmanager\n",
        "    def set_split_idx(self, i):\n",
        "        old_split_idx = self.split_idx\n",
        "        for tl in self.tls: tl.tfms.split_idx = i\n",
        "        try: yield self\n",
        "        finally:\n",
        "            for tl in self.tls: tl.tfms.split_idx = old_split_idx\n",
        "\n",
        "    _docs=dict(\n",
        "        decode=\"Compose `decode` of all `tuple_tfms` then all `tfms` on `i`\",\n",
        "        show=\"Show item `o` in `ctx`\",\n",
        "        dataloaders=\"Get a `DataLoaders`\",\n",
        "        overlapping_splits=\"All splits that are in more than one split\",\n",
        "        subset=\"New `Datasets` that only includes subset `i`\",\n",
        "        new_empty=\"Create a new empty version of the `self`, keeping only the transforms\",\n",
        "        set_split_idx=\"Contextmanager to use the same `Datasets` with another `split_idx`\"\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ7SeuNuZxwI",
        "colab_type": "text"
      },
      "source": [
        "A `Datasets` creates a tuple from `items` (typically input,target) by applying to them each list of `Transform` (or `Pipeline`) in `tfms`. Note that if `tfms` contains only one list of `tfms`, the items given by `Datasets` will be tuples of one element. \n",
        "\n",
        "`n_inp` is the number of elements in the tuples that should be considered part of the input and will default to 1 if `tfms` consists of one set of transforms, `len(tfms)-1` otherwise. In most cases, the number of elements in the tuples spit out by `Datasets` will be 2 (for input,target) but it can happen that there is 3 (Siamese networks or tabular data) in which case we need to be able to determine when the inputs end and the targets begin."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bS6iU4fZxwJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7eb11043-15f9-437f-b27b-dfb9730096b7"
      },
      "source": [
        "items = [1,2,3,4]\n",
        "dsets = Datasets(items, [[neg_tfm,int2f_tfm], [add(1)]])\n",
        "t = dsets[0]\n",
        "test_eq(t, (-1,2))\n",
        "test_eq(dsets[0,1,2], [(-1,2),(-2,3),(-3,4)])\n",
        "test_eq(dsets.n_inp, 1)\n",
        "dsets.decode(t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtR7FvS_ZxwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Norm(Transform):\n",
        "    def encodes(self, o): return (o-self.m)/self.s\n",
        "    def decodes(self, o): return (o*self.s)+self.m\n",
        "    def setups(self, items):\n",
        "        its = tensor(items).float()\n",
        "        self.m,self.s = its.mean(),its.std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdvX6FXnZxwM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "items = [1,2,3,4]\n",
        "nrm = Norm()\n",
        "dsets = Datasets(items, [[neg_tfm,int2f_tfm], [neg_tfm,nrm]])\n",
        "\n",
        "x,y = zip(*dsets)\n",
        "test_close(tensor(y).mean(), 0)\n",
        "test_close(tensor(y).std(), 1)\n",
        "test_eq(x, (-1,-2,-3,-4,))\n",
        "test_eq(nrm.m, -2.5)\n",
        "test_stdout(lambda:show_at(dsets, 1), '-2')\n",
        "\n",
        "test_eq(dsets.m, nrm.m)\n",
        "test_eq(dsets.norm.m, nrm.m)\n",
        "test_eq(dsets.train.norm.m, nrm.m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBHCGO6UZxwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hide\n",
        "#Check filtering is properly applied\n",
        "class B(Transform):\n",
        "    def encodes(self, x)->None:  return int(x+1)\n",
        "    def decodes(self, x):        return TitledInt(x-1)\n",
        "add1 = B(split_idx=1)\n",
        "\n",
        "dsets = Datasets(items, [neg_tfm, [neg_tfm,int2f_tfm,add1]], splits=[[3],[0,1,2]])\n",
        "test_eq(dsets[1], [-2,-2])\n",
        "test_eq(dsets.valid[1], [-2,-1])\n",
        "test_eq(dsets.valid[[1,1]], [[-2,-1], [-2,-1]])\n",
        "test_eq(dsets.train[0], [-4,-4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIBYGsDEZxwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_fns = ['dog_0.jpg','cat_0.jpg','cat_2.jpg','cat_1.jpg','kid_1.jpg']\n",
        "tcat = _Cat()\n",
        "dsets = Datasets(test_fns, [[tcat,_lbl]], splits=[[0,1,2], [3,4]])\n",
        "test_eq(tcat.vocab, ['cat','dog'])\n",
        "test_eq(dsets.train, [(1,),(0,),(0,)])\n",
        "test_eq(dsets.valid[0], (0,))\n",
        "test_stdout(lambda: show_at(dsets.train, 0), \"dog\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihHIz6TvZxwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inp = [0,1,2,3,4]\n",
        "dsets = Datasets(inp, tfms=[None])\n",
        "\n",
        "test_eq(*dsets[2], 2)          # Retrieve one item (subset 0 is the default)\n",
        "test_eq(dsets[1,2], [(1,),(2,)])    # Retrieve two items by index\n",
        "mask = [True,False,False,True,False]\n",
        "test_eq(dsets[mask], [(0,),(3,)])   # Retrieve two items by mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F0VdnyvZxwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inp = pd.DataFrame(dict(a=[5,1,2,3,4]))\n",
        "dsets = Datasets(inp, tfms=attrgetter('a')).subset(0)\n",
        "test_eq(*dsets[2], 2)          # Retrieve one item (subset 0 is the default)\n",
        "test_eq(dsets[1,2], [(1,),(2,)])    # Retrieve two items by index\n",
        "mask = [True,False,False,True,False]\n",
        "test_eq(dsets[mask], [(5,),(3,)])   # Retrieve two items by mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0oaHgXZZxwT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test n_inp\n",
        "inp = [0,1,2,3,4]\n",
        "dsets = Datasets(inp, tfms=[None])\n",
        "test_eq(dsets.n_inp, 1)\n",
        "dsets = Datasets(inp, tfms=[[None],[None],[None]])\n",
        "test_eq(dsets.n_inp, 2)\n",
        "dsets = Datasets(inp, tfms=[[None],[None],[None]], n_inp=1)\n",
        "test_eq(dsets.n_inp, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABzjFOKgZxwU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f5042914-98b7-4cf0-9e72-063d74c18e1a"
      },
      "source": [
        "# splits can be indices\n",
        "dsets = Datasets(range(5), tfms=[None], splits=[tensor([0,2]), [1,3,4]])\n",
        "\n",
        "test_eq(dsets.subset(0), [(0,),(2,)])\n",
        "test_eq(dsets.train, [(0,),(2,)])       # Subset 0 is aliased to `train`\n",
        "test_eq(dsets.subset(1), [(1,),(3,),(4,)])\n",
        "test_eq(dsets.valid, [(1,),(3,),(4,)])     # Subset 1 is aliased to `valid`\n",
        "test_eq(*dsets.valid[2], 4)\n",
        "#assert '[(1,),(3,),(4,)]' in str(dsets) and '[(0,),(2,)]' in str(dsets)\n",
        "dsets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#5) [(0,),(1,),(2,),(3,),(4,)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1q8OG3eZxwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# splits can be boolean masks (they don't have to cover all items, but must be disjoint)\n",
        "splits = [[False,True,True,False,True], [True,False,False,False,False]]\n",
        "dsets = Datasets(range(5), tfms=[None], splits=splits)\n",
        "\n",
        "test_eq(dsets.train, [(1,),(2,),(4,)])\n",
        "test_eq(dsets.valid, [(0,)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "500KQAuUZxwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# apply transforms to all items\n",
        "tfm = [[lambda x: x*2,lambda x: x+1]]\n",
        "splits = [[1,2],[0,3,4]]\n",
        "dsets = Datasets(range(5), tfm, splits=splits)\n",
        "test_eq(dsets.train,[(3,),(5,)])\n",
        "test_eq(dsets.valid,[(1,),(7,),(9,)])\n",
        "test_eq(dsets.train[False,True], [(5,)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHeQHnl_ZxwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# only transform subset 1\n",
        "class _Tfm(Transform):\n",
        "    split_idx=1\n",
        "    def encodes(self, x): return x*2\n",
        "    def decodes(self, x): return TitledStr(x//2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBUjyysYZxwY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11500cde-3f0c-4934-bc1b-b416e497d492"
      },
      "source": [
        "dsets = Datasets(range(5), [_Tfm()], splits=[[1,2],[0,3,4]])\n",
        "test_eq(dsets.train,[(1,),(2,)])\n",
        "test_eq(dsets.valid,[(0,),(6,),(8,)])\n",
        "test_eq(dsets.train[False,True], [(2,)])\n",
        "dsets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#5) [(0,),(1,),(2,),(3,),(4,)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vpl7Vlm-Zxwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#A context manager to change the split_idx and apply the validation transform on the training set\n",
        "ds = dsets.train\n",
        "with ds.set_split_idx(1):\n",
        "    test_eq(ds,[(2,),(4,)])\n",
        "test_eq(dsets.train,[(1,),(2,)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtpjsoxWZxwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hide\n",
        "#Test Datasets pickles\n",
        "dsrc1 = pickle.loads(pickle.dumps(dsets))\n",
        "test_eq(dsets.train, dsrc1.train)\n",
        "test_eq(dsets.valid, dsrc1.valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVaBHTo6Zxwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dsets = Datasets(range(5), [_Tfm(),noop], splits=[[1,2],[0,3,4]])\n",
        "test_eq(dsets.train,[(1,1),(2,2)])\n",
        "test_eq(dsets.valid,[(0,0),(6,3),(8,4)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-1y4a7RZxwe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = torch.arange(0,50)\n",
        "tds = Datasets(start, [A()])\n",
        "tdl = TfmdDL(tds, after_item=NegTfm(), bs=4)\n",
        "b = tdl.one_batch()\n",
        "test_eq(tdl.decode_batch(b), ((0,),(1,),(2,),(3,)))\n",
        "test_stdout(tdl.show_batch, \"0\\n1\\n2\\n3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_g9YdKoZxwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# only transform subset 1\n",
        "class _Tfm(Transform):\n",
        "    split_idx=1\n",
        "    def encodes(self, x): return x*2\n",
        "\n",
        "dsets = Datasets(range(8), [None], splits=[[1,2,5,7],[0,3,4,6]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYwZJMshZxwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# only transform subset 1\n",
        "class _Tfm(Transform):\n",
        "    split_idx=1\n",
        "    def encodes(self, x): return x*2\n",
        "\n",
        "dsets = Datasets(range(8), [None], splits=[[1,2,5,7],[0,3,4,6]])\n",
        "dls = dsets.dataloaders(bs=4, after_batch=_Tfm(), shuffle_train=False, device=torch.device('cpu'))\n",
        "test_eq(dls.train, [(tensor([1,2,5, 7]),)])\n",
        "test_eq(dls.valid, [(tensor([0,6,8,12]),)])\n",
        "test_eq(dls.n_inp, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7EokSEnZxwi",
        "colab_type": "text"
      },
      "source": [
        "### Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iFtL7U4Zxwi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "items = [1,2,3,4]\n",
        "dsets = Datasets(items, [[neg_tfm,int2f_tfm]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJClLPh0Zxwk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "ad1372cd-a53f-4b87-e044-52e11f1c3b8e"
      },
      "source": [
        "#hide_input\n",
        "_dsrc = Datasets([1,2])\n",
        "show_doc(_dsrc.dataloaders, name=\"Datasets.dataloaders\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<h4 id=\"Datasets.dataloaders\" class=\"doc_header\"><code>Datasets.dataloaders</code><a href=\"__main__.py#L15\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>Datasets.dataloaders</code>(**`bs`**=*`64`*, **`val_bs`**=*`None`*, **`shuffle_train`**=*`True`*, **`n`**=*`None`*, **`path`**=*`'.'`*, **`dl_type`**=*`None`*, **`dl_kwargs`**=*`None`*, **`device`**=*`None`*, **`shuffle`**=*`False`*, **`num_workers`**=*`None`*, **`verbose`**=*`False`*, **`do_setup`**=*`True`*, **`pin_memory`**=*`False`*, **`timeout`**=*`0`*, **`batch_size`**=*`None`*, **`drop_last`**=*`False`*, **`indexed`**=*`None`*, **`wif`**=*`None`*, **`before_iter`**=*`None`*, **`after_item`**=*`None`*, **`before_batch`**=*`None`*, **`after_batch`**=*`None`*, **`after_iter`**=*`None`*, **`create_batches`**=*`None`*, **`create_item`**=*`None`*, **`create_batch`**=*`None`*, **`retain`**=*`None`*, **`get_idxs`**=*`None`*, **`sample`**=*`None`*, **`shuffle_fn`**=*`None`*, **`do_batch`**=*`None`*)\n\nGet a [`DataLoaders`](/data.core#DataLoaders)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOiQSmThZxwm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "66636e8e-9bdc-43b0-d5ea-a28d1299c670"
      },
      "source": [
        "show_doc(Datasets.decode)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<h4 id=\"Datasets.decode\" class=\"doc_header\"><code>Datasets.decode</code><a href=\"__main__.py#L20\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>Datasets.decode</code>(**`o`**, **`full`**=*`True`*)\n\nCompose `decode` of all `tuple_tfms` then all `tfms` on `i`",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ujcf-U_UZxwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_eq(*dsets[0], -1)\n",
        "test_eq(*dsets.decode((-1,)), 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko32b5c_Zxwo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "e550cc74-5084-4fef-b893-360f35d135a5"
      },
      "source": [
        "show_doc(Datasets.show)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<h4 id=\"Datasets.show\" class=\"doc_header\"><code>Datasets.show</code><a href=\"__main__.py#L35\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>Datasets.show</code>(**`o`**, **`ctx`**=*`None`*, **\\*\\*`kwargs`**)\n\nShow item `o` in `ctx`",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLeBbv8sZxwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_stdout(lambda:dsets.show(dsets[1]), '-2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPdkwiMtZxwr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "33bb473e-0d09-4ff2-8f64-8e9e08bf11d0"
      },
      "source": [
        "show_doc(Datasets.new_empty)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "<h4 id=\"Datasets.new_empty\" class=\"doc_header\"><code>Datasets.new_empty</code><a href=\"__main__.py#L24\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>Datasets.new_empty</code>()\n\nCreate a new empty version of the `self`, keeping only the transforms",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwoCMLruZxwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "items = [1,2,3,4]\n",
        "nrm = Norm()\n",
        "dsets = Datasets(items, [[neg_tfm,int2f_tfm], [neg_tfm]])\n",
        "empty = dsets.new_empty()\n",
        "test_eq(empty.items, [])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh4VhkLzZxwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hide\n",
        "#test it works for dataframes too\n",
        "df = pd.DataFrame({'a':[1,2,3,4,5], 'b':[6,7,8,9,10]})\n",
        "dsets = Datasets(df, [[attrgetter('a')], [attrgetter('b')]])\n",
        "empty = dsets.new_empty()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YTAXu0OZxwv",
        "colab_type": "text"
      },
      "source": [
        "## Add test set for inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGW2JNGBZxwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# only transform subset 1\n",
        "class _Tfm1(Transform):\n",
        "    split_idx=0\n",
        "    def encodes(self, x): return x*3\n",
        "\n",
        "dsets = Datasets(range(8), [[_Tfm(),_Tfm1()]], splits=[[1,2,5,7],[0,3,4,6]])\n",
        "test_eq(dsets.train, [(3,),(6,),(15,),(21,)])\n",
        "test_eq(dsets.valid, [(0,),(6,),(8,),(12,)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61Vl0fukZxwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "def test_set(dsets, test_items, rm_tfms=None, with_labels=False):\n",
        "    \"Create a test set from `test_items` using validation transforms of `dsets`\"\n",
        "    if isinstance(dsets, Datasets):\n",
        "        tls = dsets.tls if with_labels else dsets.tls[:dsets.n_inp]\n",
        "        test_tls = [tl._new(test_items, split_idx=1) for tl in tls]\n",
        "        if rm_tfms is None: rm_tfms = [tl.infer_idx(get_first(test_items)) for tl in test_tls]\n",
        "        else:               rm_tfms = tuplify(rm_tfms, match=test_tls)\n",
        "        for i,j in enumerate(rm_tfms): test_tls[i].tfms.fs = test_tls[i].tfms.fs[j:]\n",
        "        return Datasets(tls=test_tls)\n",
        "    elif isinstance(dsets, TfmdLists):\n",
        "        test_tl = dsets._new(test_items, split_idx=1)\n",
        "        if rm_tfms is None: rm_tfms = dsets.infer_idx(get_first(test_items))\n",
        "        test_tl.tfms.fs = test_tl.tfms.fs[rm_tfms:]\n",
        "        return test_tl\n",
        "    else: raise Exception(f\"This method requires using the fastai library to assemble your data. Expected a `Datasets` or a `TfmdLists` but got {dsets.__class__.__name__}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imaOfoowZxwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class _Tfm1(Transform):\n",
        "    split_idx=0\n",
        "    def encodes(self, x): return x*3\n",
        "\n",
        "dsets = Datasets(range(8), [[_Tfm(),_Tfm1()]], splits=[[1,2,5,7],[0,3,4,6]])\n",
        "test_eq(dsets.train, [(3,),(6,),(15,),(21,)])\n",
        "test_eq(dsets.valid, [(0,),(6,),(8,),(12,)])\n",
        "\n",
        "#Tranform of the validation set are applied\n",
        "tst = test_set(dsets, [1,2,3])\n",
        "test_eq(tst, [(2,),(4,),(6,)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OeI2UWlZxwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hide\n",
        "#Test with different types\n",
        "tfm = _Tfm1()\n",
        "tfm.split_idx,tfm.order = None,2\n",
        "dsets = Datasets(['dog', 'cat', 'cat', 'dog'], [[_Cat(),tfm]])\n",
        "\n",
        "#With strings\n",
        "test_eq(test_set(dsets, ['dog', 'cat', 'cat']), [(3,), (0,), (0,)])\n",
        "#With ints\n",
        "test_eq(test_set(dsets, [1,2]), [(3,), (6,)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo-9p1pTZxw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hide\n",
        "#Test with various input lengths\n",
        "dsets = Datasets(range(8), [[_Tfm(),_Tfm1()],[_Tfm(),_Tfm1()],[_Tfm(),_Tfm1()]], splits=[[1,2,5,7],[0,3,4,6]])\n",
        "tst = test_set(dsets, [1,2,3])\n",
        "test_eq(tst, [(2,2),(4,4),(6,6)])\n",
        "\n",
        "dsets = Datasets(range(8), [[_Tfm(),_Tfm1()],[_Tfm(),_Tfm1()],[_Tfm(),_Tfm1()]], splits=[[1,2,5,7],[0,3,4,6]], n_inp=1)\n",
        "tst = test_set(dsets, [1,2,3])\n",
        "test_eq(tst, [(2,),(4,),(6,)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCXDRKEOZxw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hide\n",
        "#Test with rm_tfms\n",
        "dsets = Datasets(range(8), [[_Tfm(),_Tfm()]], splits=[[1,2,5,7],[0,3,4,6]])\n",
        "tst = test_set(dsets, [1,2,3])\n",
        "test_eq(tst, [(4,),(8,),(12,)])\n",
        "\n",
        "dsets = Datasets(range(8), [[_Tfm(),_Tfm()]], splits=[[1,2,5,7],[0,3,4,6]])\n",
        "tst = test_set(dsets, [1,2,3], rm_tfms=1)\n",
        "test_eq(tst, [(2,),(4,),(6,)])\n",
        "\n",
        "dsets = Datasets(range(8), [[_Tfm(),_Tfm()], [_Tfm(),_Tfm()]], splits=[[1,2,5,7],[0,3,4,6]], n_inp=2)\n",
        "tst = test_set(dsets, [1,2,3], rm_tfms=(1,0))\n",
        "test_eq(tst, [(2,4),(4,8),(6,12)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbA7nioNZxw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "@delegates(TfmdDL.__init__)\n",
        "@patch\n",
        "def test_dl(self:DataLoaders, test_items, rm_type_tfms=None, with_labels=False, **kwargs):\n",
        "    \"Create a test dataloader from `test_items` using validation transforms of `dls`\"\n",
        "    test_ds = test_set(self.valid_ds, test_items, rm_tfms=rm_type_tfms, with_labels=with_labels\n",
        "                      ) if isinstance(self.valid_ds, (Datasets, TfmdLists)) else test_items\n",
        "    return self.valid.new(test_ds, **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr0YhHsmZxw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dsets = Datasets(range(8), [[_Tfm(),_Tfm1()]], splits=[[1,2,5,7],[0,3,4,6]])\n",
        "dls = dsets.dataloaders(bs=4, device=torch.device('cpu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWl3Pm1nZxw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dsets = Datasets(range(8), [[_Tfm(),_Tfm1()]], splits=[[1,2,5,7],[0,3,4,6]])\n",
        "dls = dsets.dataloaders(bs=4, device=torch.device('cpu'))\n",
        "tst_dl = dls.test_dl([2,3,4,5])\n",
        "test_eq(tst_dl._n_inp, 1)\n",
        "test_eq(list(tst_dl), [(tensor([ 4,  6,  8, 10]),)])\n",
        "#Test you can change transforms\n",
        "tst_dl = dls.test_dl([2,3,4,5], after_item=add1)\n",
        "test_eq(list(tst_dl), [(tensor([ 5,  7,  9, 11]),)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f5wVP0QZxw6",
        "colab_type": "text"
      },
      "source": [
        "## Export -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmR_fcmvZxw7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fc5b0c31-aa36-4c8e-eb73-f62a72fafb45"
      },
      "source": [
        "#hide\n",
        "from nbdev.export import notebook2script\n",
        "notebook2script()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converted 00_torch_core.ipynb.\n",
            "Converted 01_layers.ipynb.\n",
            "Converted 02_data.load.ipynb.\n",
            "Converted 03_data.core.ipynb.\n",
            "Converted 04_data.external.ipynb.\n",
            "Converted 05_data.transforms.ipynb.\n",
            "Converted 06_data.block.ipynb.\n",
            "Converted 07_vision.core.ipynb.\n",
            "Converted 08_vision.data.ipynb.\n",
            "Converted 09_vision.augment.ipynb.\n",
            "Converted 09b_vision.utils.ipynb.\n",
            "Converted 09c_vision.widgets.ipynb.\n",
            "Converted 10_tutorial.pets.ipynb.\n",
            "Converted 11_vision.models.xresnet.ipynb.\n",
            "Converted 12_optimizer.ipynb.\n",
            "Converted 13_callback.core.ipynb.\n",
            "Converted 13a_learner.ipynb.\n",
            "Converted 13b_metrics.ipynb.\n",
            "Converted 14_callback.schedule.ipynb.\n",
            "Converted 14a_callback.data.ipynb.\n",
            "Converted 15_callback.hook.ipynb.\n",
            "Converted 15a_vision.models.unet.ipynb.\n",
            "Converted 16_callback.progress.ipynb.\n",
            "Converted 17_callback.tracker.ipynb.\n",
            "Converted 18_callback.fp16.ipynb.\n",
            "Converted 18a_callback.training.ipynb.\n",
            "Converted 19_callback.mixup.ipynb.\n",
            "Converted 20_interpret.ipynb.\n",
            "Converted 20a_distributed.ipynb.\n",
            "Converted 21_vision.learner.ipynb.\n",
            "Converted 22_tutorial.imagenette.ipynb.\n",
            "Converted 23_tutorial.vision.ipynb.\n",
            "Converted 24_tutorial.siamese.ipynb.\n",
            "Converted 24_vision.gan.ipynb.\n",
            "Converted 30_text.core.ipynb.\n",
            "Converted 31_text.data.ipynb.\n",
            "Converted 32_text.models.awdlstm.ipynb.\n",
            "Converted 33_text.models.core.ipynb.\n",
            "Converted 34_callback.rnn.ipynb.\n",
            "Converted 35_tutorial.wikitext.ipynb.\n",
            "Converted 36_text.models.qrnn.ipynb.\n",
            "Converted 37_text.learner.ipynb.\n",
            "Converted 38_tutorial.text.ipynb.\n",
            "Converted 39_tutorial.transformers.ipynb.\n",
            "Converted 40_tabular.core.ipynb.\n",
            "Converted 41_tabular.data.ipynb.\n",
            "Converted 42_tabular.model.ipynb.\n",
            "Converted 43_tabular.learner.ipynb.\n",
            "Converted 44_tutorial.tabular.ipynb.\n",
            "Converted 45_collab.ipynb.\n",
            "Converted 46_tutorial.collab.ipynb.\n",
            "Converted 50_tutorial.datablock.ipynb.\n",
            "Converted 60_medical.imaging.ipynb.\n",
            "Converted 61_tutorial.medical_imaging.ipynb.\n",
            "Converted 65_medical.text.ipynb.\n",
            "Converted 70_callback.wandb.ipynb.\n",
            "Converted 71_callback.tensorboard.ipynb.\n",
            "Converted 72_callback.neptune.ipynb.\n",
            "Converted 73_callback.captum.ipynb.\n",
            "Converted 74_callback.cutmix.ipynb.\n",
            "Converted 97_test_utils.ipynb.\n",
            "Converted 99_pytorch_doc.ipynb.\n",
            "Converted index.ipynb.\n",
            "Converted tutorial.ipynb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp0_JlwjZxw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}