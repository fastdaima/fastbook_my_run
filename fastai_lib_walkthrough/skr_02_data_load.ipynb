{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "jupytext": {
      "split_at_heading": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "skr_02_data.load.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS1F0uli514v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#default_exp data.load"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhkoL9ttAoWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# do something like this - copy over from fastai2 walkthru \n",
        "# !nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJaXNc4iAr5G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "3256ff8f-b019-4c83-dec3-f6e631adda05"
      },
      "source": [
        "# make your Google drive accessible \n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/\"\n",
        "base_dir = root_dir + 'fastai2_library/course-v4/'\n",
        "\n",
        "# navigate to the notebooks directory for dl2\n",
        "import os\n",
        "os.chdir(base_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH9G1bctAsLQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d02aa1d4-4974-4e01-9f77-694cb7829428"
      },
      "source": [
        "!pwd\n",
        "# cd to base_dir if above os.chdir does not work using below command\n",
        "# %cd \"/content/gdrive/My Drive/fastai/course-v4/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/fastai2_library/course-v4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHLgPy8PAx4Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "13d4b1fa-4066-408f-9bd2-1ceaddae5aa1"
      },
      "source": [
        "!pip install -q -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 194kB 3.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 4.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 4.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 10.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 40kB 3.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 7.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 4.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 5.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.7MB 20.2MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc3n6OgLAyKq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1c87fc9b-b661-4184-9ea7-5dc9c6d9845d"
      },
      "source": [
        "%cd nbs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/fastai2_library/course-v4/nbs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ymo6WCZh5143",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "from fastai2.torch_basics import *\n",
        "\n",
        "from torch.utils.data.dataloader import _MultiProcessingDataLoaderIter,_SingleProcessDataLoaderIter,_DatasetKind\n",
        "_loaders = (_MultiProcessingDataLoaderIter,_SingleProcessDataLoaderIter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSgSd3Wy5147",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nbdev.showdoc import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QanjFi-K514-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bs = 4\n",
        "letters = list(string.ascii_lowercase)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc-3x06R515B",
        "colab_type": "text"
      },
      "source": [
        "## DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJuP9E_I515D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "def _wif(worker_id):\n",
        "    set_num_threads(1)\n",
        "    # call PyTorch get_worker_info function\n",
        "    # tells us which dataset this process is working with\n",
        "    # info.dataset.d\n",
        "    info = get_worker_info()\n",
        "    ds = info.dataset.d\n",
        "    # tells us how many workers are there in info.num_workers\n",
        "    # id of this worker into that list of workers avail in info.id\n",
        "    # Use this info to parallelize things automatically \n",
        "    ds.nw,ds.offs = info.num_workers,info.id\n",
        "    # each worker gets a separate random seed to prevent horrible things\n",
        "    # from happening with random seeds being duplicated\n",
        "    set_seed(info.seed)\n",
        "    ds.wif()\n",
        "\n",
        "class _FakeLoader:\n",
        "    # PyTorch expects an object with this list with expected values, used\n",
        "    # below in DataLoader __iter__\n",
        "    _IterableDataset_len_called,_auto_collation,collate_fn,drop_last,dataset_kind,_dataset_kind,_index_sampler = (\n",
        "        None,False,noops,False,_DatasetKind.Iterable,_DatasetKind.Iterable,Inf.count)\n",
        "    def __init__(self, d, pin_memory, num_workers, timeout):\n",
        "        # Note PyTorch (1.2+ versions?) added worker_init_fn \n",
        "        # which is a callback in PyTorch dataloader infrastructure which is \n",
        "        # called everytime a NEW PROCESS is fired off.\n",
        "        # fastai has a worker information function _wif which does \n",
        "        # things mentioned above in _wif\n",
        "        self.dataset,self.default,self.worker_init_fn = self,d,_wif\n",
        "        store_attr(self, 'd,pin_memory,num_workers,timeout')\n",
        "\n",
        "    # Calling this is what passes it back to fastai dataloader\n",
        "    def __iter__(self): return iter(self.d.create_batches(self.d.sample()))\n",
        "\n",
        "    @property\n",
        "    def multiprocessing_context(self): return (None,multiprocessing)[self.num_workers>0]\n",
        "\n",
        "    @contextmanager\n",
        "    def no_multiproc(self):\n",
        "        old_nw = self.num_workers\n",
        "        try:\n",
        "            self.num_workers = 0\n",
        "            yield self.d\n",
        "        finally: self.num_workers = old_nw\n",
        "\n",
        "_collate_types = (ndarray, Tensor, typing.Mapping, str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txuKHlT5515H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "def fa_collate(t):\n",
        "    b = t[0]\n",
        "    # Pytorch by default calls default_collate to take all items in your datasets\n",
        "    # turn them into a batch if you have a batch size or use default_convert\n",
        "    # fastai_collate handles a few types that PyTorch collate does not handle\n",
        "    # does not change behavior but more things should work correctly\n",
        "    # used in create_batch\n",
        "    return (default_collate(t) if isinstance(b, _collate_types)\n",
        "            else type(t[0])([fa_collate(s) for s in zip(*t)]) if isinstance(b, Sequence)\n",
        "            else default_collate(t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEg1Rkt_515L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#e.g. x is int, y is tuple\n",
        "t = [(1,(2,3)),(1,(2,3))]\n",
        "test_eq(fa_collate(t), default_collate(t))\n",
        "test_eq(L(fa_collate(t)).map(type), [Tensor,tuple])\n",
        "\n",
        "t = [(1,(2,(3,4))),(1,(2,(3,4)))]\n",
        "test_eq(fa_collate(t), default_collate(t))\n",
        "test_eq(L(fa_collate(t)).map(type), [Tensor,tuple])\n",
        "test_eq(L(fa_collate(t)[1]).map(type), [Tensor,tuple])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFm8uT0-515O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "def fa_convert(t):\n",
        "    # handles a few types that PyTorch convert does not handle\n",
        "    # does not change behavior but more things should work correctly\n",
        "    # used in create_batch\n",
        "    return (default_convert(t) if isinstance(t, _collate_types)\n",
        "            else type(t)([fa_convert(s) for s in t]) if isinstance(t, Sequence)\n",
        "            else default_convert(t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQy-8Mef515R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t0 = array([1,2])\n",
        "t = [t0,(t0,t0)]\n",
        "\n",
        "test_eq(fa_convert(t), default_convert(t))\n",
        "test_eq(L(fa_convert(t)).map(type), [Tensor,tuple])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymjg6dtr515W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "class SkipItemException(Exception): pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phXIqi7ymQ7w",
        "colab_type": "text"
      },
      "source": [
        "A replacement for PyTorch dataloader. Why replace? Biggest reason: hooks to do what Jeremy wanted to do not available. PyTorch dataloader code (Jeremy thinks) is hard to understand, and pieces are tightly coupled and have assumptions about each other. Hard to add things to it or fix issues with it.  \n",
        "\n",
        "However PyTorch dataloader is based on well-tested rock solid, fast, multi-processing system and fastai still wanted to use that. \n",
        "\n",
        "That mp system is pulled out in PyTorch into _MultiProcessingDataLoaderIter and that is used without any rewrite. But to use it fastai had to write a bit of \"slightly ugly code\". Specifically code in class _FakeLoader. This exists so we can sneak our way into the PyTorch DataLoading system. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j08nCxqVHlHT",
        "colab_type": "text"
      },
      "source": [
        "Overriding a method such as create_item can be done two ways (see below how there are two ways RandDL functionality is implemented). One is inheritance where you get to benefit from state as you get that through inheritance. The other is just passing in create_item as another function defined by us. \n",
        "\n",
        "Create item if you look in DataLoader code is in the _methods\n",
        "list of methods. _methods is a special name and the _methods is going to be looked at by funcs_kwargs decorator. That decorator says the kwargs in this __init__ are not unknown but are actually the list provided by _methods. The decorator automatically grabs any kwargs with these names and it is going to replace these methods that we have with the thing that is passed to that kwarg.\n",
        "\n",
        "Ofcourse, kwargs are bad for discoverability and Tab Completion and documentation coz we don't know what we can pass to kwargs but fastai fixes those problems. So Dataloader(Shift+Tab) works to reveal all the methods  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TNuR3hL515Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "@log_args(but='dataset,wif,create_batch,create_batches,create_item,retain,get_idxs,sample,shuffle_fn,do_batch')\n",
        "@funcs_kwargs\n",
        "class DataLoader(GetAttr):\n",
        "    # Whole bunch of callbacks we can define and use with DataLoader\n",
        "    # eg after_iter runs after all the iterations. before_iter runs before\n",
        "    # all the iterations, \n",
        "\n",
        "    # do_item to get item, do_batch to create batches - calls before batch,  \n",
        "\n",
        "    # Can replace any of these callbacks.\n",
        "    # The _noop_methods currently default to noops or no operations\n",
        "    # We can just use them as callbacks. \n",
        "    _noop_methods = 'wif before_iter after_item before_batch after_batch after_iter'.split()\n",
        "    for o in _noop_methods:\n",
        "        exec(f\"def {o}(self, x=None, *args, **kwargs): return x\")\n",
        "    _methods = _noop_methods + 'create_batches create_item create_batch retain \\\n",
        "        get_idxs sample shuffle_fn do_batch create_batch'.split()\n",
        "    _default = 'dataset'\n",
        "    def __init__(self, dataset=None, bs=None, num_workers=0, pin_memory=False, timeout=0, batch_size=None,\n",
        "                 shuffle=False, drop_last=False, indexed=None, n=None, device=None, **kwargs):\n",
        "        if batch_size is not None: bs = batch_size # PyTorch compatibility\n",
        "        assert not (bs is None and drop_last)\n",
        "        if indexed is None: indexed = dataset is not None and hasattr(dataset,'__getitem__')\n",
        "        if n is None:\n",
        "            try: n = len(dataset)\n",
        "            except TypeError: pass\n",
        "        store_attr(self, 'dataset,bs,shuffle,drop_last,indexed,n,pin_memory,timeout,device')\n",
        "        # to parallelize things automatically - see in _wif code above, setting\n",
        "        # offset and number of workers and then use it below in the sample \n",
        "        # to ensure each process handles contiguous set of things but different\n",
        "        # from each other process. See sample code below.\n",
        "        self.rng,self.nw,self.offs = random.Random(random.randint(0,2**32-1)),1,0\n",
        "        self.fake_l = _FakeLoader(self, pin_memory, num_workers, timeout)\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.n is None: raise TypeError\n",
        "        if self.bs is None: return self.n\n",
        "        return self.n//self.bs + (0 if self.drop_last or self.n%self.bs==0 else 1)\n",
        "\n",
        "    def get_idxs(self):\n",
        "        # if index=True, idxs is infinite count of integers\n",
        "        # if NOT indexed idxs returns infinite list of Nones\n",
        "        # so when you shuffle you just get infinite list of Nones\n",
        "        # in shuffled order\n",
        "\n",
        "        # Inf class helps to create two types of Infinite Lists\n",
        "        # Inf.count and Inf.nones. Covered later but pretty important to use \n",
        "        # functional programming like paradigm \n",
        "        idxs = Inf.count if self.indexed else Inf.nones\n",
        "        if self.n is not None: idxs = list(itertools.islice(idxs, self.n))\n",
        "        if self.shuffle: idxs = self.shuffle_fn(idxs)\n",
        "        return idxs\n",
        "\n",
        "    def sample(self):\n",
        "        idxs = self.get_idxs()\n",
        "        # below is to ensure that each process handles a contiguous set of things\n",
        "        # which is different to each other process.\n",
        "        return (b for i,b in enumerate(idxs) if i//(self.bs or 1)%self.nw==self.offs)\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.randomize()\n",
        "        self.before_iter()\n",
        "        # ugly code below needed for the PyTorch mp dataloader\n",
        "        # create object of type _FakeLoader which is self.fake_l in init\n",
        "        # Needed coz PyTorch assumes that it is working with an object\n",
        "        # which has the list of things specified in _FakeLoader in it\n",
        "        # So needed to create object with those things with expected values\n",
        "        # in _FakeLoader. Calling __iter__ on that object is where we pass it\n",
        "        # back to our DataLoader. This results in tying our DataLoader to\n",
        "        # some private things in PyTorch like _auto_collation etc on list in\n",
        "        # _FakeLoader\n",
        "        for b in _loaders[self.fake_l.num_workers==0](self.fake_l):\n",
        "            if self.device is not None: b = to_device(b, self.device)\n",
        "            # this after_batch is called finally after getting \n",
        "            # done with do_batch lazily hence yield\n",
        "            yield self.after_batch(b)\n",
        "        self.after_iter()\n",
        "        if hasattr(self, 'it'): delattr(self, 'it')\n",
        "\n",
        "    def create_batches(self, samps):\n",
        "        self.it = iter(self.dataset) if self.dataset is not None else None\n",
        "        # go through everything in the sampler - samps\n",
        "        # map do_item over it\n",
        "        res = filter(lambda o:o is not None, map(self.do_item, samps))\n",
        "        # then chunkify which creates batches out of our list (done lazily)\n",
        "        # and then call do_batch - since lazily use yield/yield from\n",
        "        yield from map(self.do_batch, self.chunkify(res))\n",
        "\n",
        "    def new(self, dataset=None, cls=None, **kwargs):\n",
        "        if dataset is None: dataset = self.dataset\n",
        "        if cls is None: cls = type(self)\n",
        "        cur_kwargs = dict(dataset=dataset, num_workers=self.fake_l.num_workers, pin_memory=self.pin_memory, timeout=self.timeout,\n",
        "                          bs=self.bs, shuffle=self.shuffle, drop_last=self.drop_last, indexed=self.indexed, device=self.device)\n",
        "        for n in self._methods: cur_kwargs[n] = getattr(self, n)\n",
        "        return cls(**merge(cur_kwargs, kwargs))\n",
        "\n",
        "    @property\n",
        "    def prebatched(self): return self.bs is None\n",
        "    def do_item(self, s):\n",
        "        # calls create_item then calls after_item\n",
        "        try: return self.after_item(self.create_item(s))\n",
        "        except SkipItemException: return None\n",
        "    def chunkify(self, b): return b if self.prebatched else chunked(b, self.bs, self.drop_last)\n",
        "    def shuffle_fn(self, idxs): return self.rng.sample(idxs, len(idxs))\n",
        "    def randomize(self): self.rng = random.Random(self.rng.randint(0,2**32-1))\n",
        "    def retain(self, res, b):  return retain_types(res, b[0] if is_listy(b) else b)\n",
        "    def create_item(self, s):  return next(self.it) if s is None else self.dataset[s]\n",
        "    # by default uses PyTorch default_collate and default_convert modified here\n",
        "    # to appropriately use fa_collate, fa_convert depending on whether your batch\n",
        "    # size is None or not. You could replace these as is done in fastai language\n",
        "    # model DataLoader and simplifies that DataLoader considerably by inheriting\n",
        "    # from this DataLoader and using some of these callbacks. \n",
        "    def create_batch(self, b): return (fa_collate,fa_convert)[self.prebatched](b)\n",
        "    # calls before batch, then creates batch, the self.retain is what retains type\n",
        "    # then finally up above you see it calls after_batch in __iter__\n",
        "    def do_batch(self, b): return self.retain(self.create_batch(self.before_batch(b)), b)\n",
        "    def to(self, device): self.device = device\n",
        "    def one_batch(self):\n",
        "        if self.n is not None and len(self)==0: raise ValueError(f'This DataLoader does not contain any batches')\n",
        "        with self.fake_l.no_multiproc(): res = first(self)\n",
        "        if hasattr(self, 'it'): delattr(self, 'it')\n",
        "        return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJMTVvw6wXz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Signature: chunked(it, cs, drop_last=False)\n",
        "Source:   \n",
        "def chunked(it, cs, drop_last=False):\n",
        "    if not isinstance(it, Iterator): it = iter(it)\n",
        "    while True:\n",
        "        res = list(itertools.islice(it, cs))\n",
        "        if res and (len(res)==cs or not drop_last): yield res\n",
        "        if len(res)<cs: return\n",
        "File:      /usr/local/lib/python3.6/dist-packages/fastcore/utils.py\n",
        "'''\n",
        "chunked??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtrmwwDS515c",
        "colab_type": "text"
      },
      "source": [
        "Override `item` and use the default infinite sampler to get a stream of unknown length (`stop()` when you want to stop the stream)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meLvd6yI6bKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Signature: stop(e=<class 'StopIteration'>)\n",
        "Source:   \n",
        "def stop(e=StopIteration):\n",
        "    \"Raises exception `e` (by default `StopException`) even if in an expression\"\n",
        "    raise e\n",
        "File:      /usr/local/lib/python3.6/dist-packages/fastcore/utils.py\n",
        "'''\n",
        "stop??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT6j6C9r515d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "629bbcdb-e09d-4a25-c394-636bf1af632c"
      },
      "source": [
        "# Here is an example subclass of DataLoader that overrides create_item\n",
        "# Normally create_item grabs the i'th item of the dataset assuming that we have some\n",
        "# sample that we want. \n",
        "\n",
        "# Here we override it to return some random number \n",
        "# stop raises a StopIteration exception\n",
        "# In python Generators/Iterators raise this exception to indicate Done.\n",
        "\n",
        "# Can use this for something reading Network Input Stream for eg. until it gets\n",
        "# some STOP code. Can create STREAMING DATALOADERs in this way\n",
        " \n",
        "class RandDL(DataLoader):\n",
        "    def create_item(self, s):\n",
        "        r = random.random()\n",
        "        return r if r<0.95 else stop()\n",
        "\n",
        "L(RandDL())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#10) [0.12202089395945215,0.2734062294718411,0.40389921044850496,0.08418430973218571,0.8724985568657233,0.4350469849794909,0.8034439090204336,0.13666965035052892,0.6575673348847532,0.40757546504289943]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbn8Cxhj515h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b7abfd44-3b5f-4f2b-bc11-8bf1230f5095"
      },
      "source": [
        "L(RandDL(bs=4, drop_last=True)).map(len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#12) [4,4,4,4,4,4,4,4,4,4...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ILXCdtT7-va",
        "colab_type": "text"
      },
      "source": [
        "When num_workers=4 this creates 4 streaming data loaders. You end up with more batches than the 0 numworkers version above because you have more streaming workers and they are all doin the RandDL job totally independently. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6RfkAdN515o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d50e8d0f-46e0-42ce-a6d7-5bc40389161b"
      },
      "source": [
        "dl = RandDL(bs=4, num_workers=4, drop_last=True)\n",
        "L(dl).map(len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#9) [4,4,4,4,4,4,4,4,4]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMf_lndT515r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_eq(dl.fake_l.num_workers, 4)\n",
        "with dl.fake_l.no_multiproc(): \n",
        "    test_eq(dl.fake_l.num_workers, 0)\n",
        "    L(dl).map(len)\n",
        "test_eq(dl.fake_l.num_workers, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqbIhNPg515v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d81a71f5-1092-4444-870c-d68b4673f2db"
      },
      "source": [
        "# Alternate way to create RandDL \n",
        "# Create a _rand_item function and pass that into DataLoader as its create_item\n",
        "# Does exactly same thing. \n",
        "\n",
        "# Anywhere you add callbacks or customizability through inheritance you can also \n",
        "# do it by passing in a parameter with the name of the method you wish to override.\n",
        "# Use this a lot if you wish to change some simple function that you wish to change\n",
        "# overriding is more work and do not need to understand oo and inheritance etc\n",
        "# Important limitation of doing it this way - you dont get to use any state\n",
        "# because you do not get passed in self. If you care about state then you need\n",
        "# to use first method of inheritance, if you dont then use this way to do it.\n",
        "\n",
        "def _rand_item(s):\n",
        "    r = random.random()\n",
        "    return r if r<0.95 else stop()\n",
        "\n",
        "L(DataLoader(create_item=_rand_item))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#3) [0.7845215025498629,0.4475754830394927,0.10627255598425778]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpKxgQjd515y",
        "colab_type": "text"
      },
      "source": [
        "If you don't set `bs`, then `dataset` is assumed to provide an iterator or a `__getitem__` that returns a batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-bdZQ6c9HYY",
        "colab_type": "text"
      },
      "source": [
        "If you do not provide bs, then fastai dataloader does not do any batching. Built into PyTorch 1.2 dataloader as well. So idea of bs = None. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXXSm-fX515z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds1 = DataLoader(letters)\n",
        "# No batches created - so listifying DataLoader of letters yields letters back\n",
        "test_eq(L(ds1), letters)\n",
        "test_eq(len(ds1), 26)\n",
        "\n",
        "# DataLoader can be shuffled and test_shuffled checks that the two arguments\n",
        "# contains all same elements in different orders\n",
        "test_shuffled(L(DataLoader(letters, shuffle=True)), letters)\n",
        "\n",
        "# Possible for Dataset to NOT have __getitem__ ie dataset is some kind of \n",
        "# infinite stream like a network, or from a GINORMOUS file system, \n",
        "# support for NON-INDEXABLE or ITERABLE datasets\n",
        "\n",
        "# By default fastai DataLoader checks whether what is passed in like letters\n",
        "# has a __getitem__ and if it DOES NOT, it will treat as ITERABLE, if it DOES\n",
        "# will treat as INDEXED. Can be overridden with indexed=False implying ITERABLE\n",
        "# return same thing as before but is doing so by CALLING NEXT rather than\n",
        "# __getitem__. Useful for GINORMOUS Datasets, \n",
        "ds1 = DataLoader(letters, indexed=False)\n",
        "test_eq(L(ds1), letters)\n",
        "test_eq(len(ds1), 26)\n",
        "\n",
        "t2 = L(tensor([0,1,2]),tensor([3,4,5]))\n",
        "ds2 = DataLoader(t2)\n",
        "test_eq_type(L(ds2), t2)\n",
        "\n",
        "# Notice DataLoader converts the arrays to Tensors\n",
        "t3 = L(array([0,1,2]),array([3,4,5]))\n",
        "ds3 = DataLoader(t3)\n",
        "test_eq_type(L(ds3), t3.map(tensor))\n",
        "\n",
        "ds4 = DataLoader(t3, create_batch=noop, after_iter=lambda: setattr(t3, 'f', 1))\n",
        "test_eq_type(L(ds4), t3)\n",
        "test_eq(t3.f, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2whis-D5156",
        "colab_type": "text"
      },
      "source": [
        "If you do set `bs`, then `dataset` is assumed to provide an iterator or a `__getitem__` that returns a single item of a batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQoqiqqr5157",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Joins each batch together with no separator, then join each set of batches with a\n",
        "# space between them and do that for two epoch ie two go arounds of all batches.\n",
        "def twoepochs(d): return ' '.join(''.join(list(o)) for _ in range(2) for o in d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRakQIxEogPR",
        "colab_type": "text"
      },
      "source": [
        "Can use DataLoader like a PyTorch dataloader. Recall that Dataset is anything that has a length and can index into so letters, a list of letters defined previously is a dataset. num_workers is how many multiprocessing workers to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7su69nI515-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds1 = DataLoader(letters, bs=4, drop_last=True, num_workers=0)\n",
        "test_eq(twoepochs(ds1), 'abcd efgh ijkl mnop qrst uvwx abcd efgh ijkl mnop qrst uvwx')\n",
        "\n",
        "ds1 = DataLoader(letters,4,num_workers=2)\n",
        "test_eq(twoepochs(ds1), 'abcd efgh ijkl mnop qrst uvwx yz abcd efgh ijkl mnop qrst uvwx yz')\n",
        "\n",
        "# can pass in things that can be turned into Tensors like ints in range and like\n",
        "# PyTorch dataloader it will turn those into batches of Tensors. \n",
        "# Note use of test_eq_type checks value and types are same.\n",
        "\n",
        "ds1 = DataLoader(range(12), bs=4, num_workers=3)\n",
        "test_eq_type(L(ds1), L(tensor([0,1,2,3]),tensor([4,5,6,7]),tensor([8,9,10,11])))\n",
        "\n",
        "# HOOKS THAT YOU CAN ADD IN TO FASTAI DataLoader\n",
        "\n",
        "# One hook is after_iter. It will run at the end of each iteration. \n",
        "# Here t3 is just some tensor and it will set t3.f to something so that\n",
        "# after you run this t3.f has value 2. So you can add code that runs after an \n",
        "# iteration \n",
        "ds1 = DataLoader([str(i) for i in range(11)], bs=4, after_iter=lambda: setattr(t3, 'f', 2))\n",
        "test_eq_type(L(ds1), L(['0','1','2','3'],['4','5','6','7'],['8','9','10']))\n",
        "test_eq(t3.f, 2)\n",
        "\n",
        "# Can pass not just a Dataset but a generator to a dataloader and it will work \n",
        "# fine as well. \n",
        "it = iter(DataLoader(map(noop,range(20)), bs=4, num_workers=1))\n",
        "test_eq_type([next(it) for _ in range(3)], [tensor([0,1,2,3]),tensor([4,5,6,7]),tensor([8,9,10,11])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KctRbjx516C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "488ea042-242a-4b0c-f800-be7836333b56"
      },
      "source": [
        "# When you have multiple workers then ensuring everything processed\n",
        "# comes back in the correct order \n",
        "\n",
        "# Here a dataloader that simply returns an item from a list. Added a random\n",
        "# bit of sleep each time. \n",
        "\n",
        "# The tests check that even if you have multiple workers that we get back\n",
        "# the results in the correct order and %time tests indicate that as you increase\n",
        "# num_workers the tests do run quickly.\n",
        "class SleepyDL(list):\n",
        "    def __getitem__(self,i):\n",
        "        time.sleep(random.random()/50)\n",
        "        return super().__getitem__(i)\n",
        "\n",
        "t = SleepyDL(letters)\n",
        "\n",
        "%time test_eq(DataLoader(t, num_workers=0), letters)\n",
        "%time test_eq(DataLoader(t, num_workers=2), letters)\n",
        "%time test_eq(DataLoader(t, num_workers=4), letters)\n",
        "\n",
        "dl = DataLoader(t, shuffle=True, num_workers=1)\n",
        "test_shuffled(L(dl), letters)\n",
        "test_shuffled(L(dl), L(dl))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 6.14 ms, sys: 163 µs, total: 6.31 ms\n",
            "Wall time: 269 ms\n",
            "CPU times: user 9.34 ms, sys: 17.8 ms, total: 27.1 ms\n",
            "Wall time: 180 ms\n",
            "CPU times: user 16.2 ms, sys: 26.1 ms, total: 42.3 ms\n",
            "Wall time: 125 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6g_vWU4516I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d4accac1-dd82-4a81-c63f-7c6270c49292"
      },
      "source": [
        "# Example of using only __iter__ vs __getitem__ \n",
        "# So only able to iterate, \n",
        "\n",
        "# Example of working with iterative only Q with variable latency (could be \n",
        "# something streaming over a Network),\n",
        "\n",
        "# Test checks that we get back the right thing and sleepy Q only has __iter__\n",
        "# not __getitem__ so no indexing so no guarantee of what order things will\n",
        "# come back in so we end up with something that is shuffled.\n",
        "\n",
        "# Also note that if index is False, shuffle does NOT do anything.\n",
        "# Implementation in DataLoader (see above) if index is False you just\n",
        "# get Infinite list of Nones in shuffled order so it does not matter.\n",
        "class SleepyQueue():\n",
        "    \"Simulate a queue with varying latency\"\n",
        "    def __init__(self, q): self.q=q\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            time.sleep(random.random()/100)\n",
        "            try: yield self.q.get_nowait()\n",
        "            except queues.Empty: return\n",
        "\n",
        "q = Queue()\n",
        "for o in range(30): q.put(o)\n",
        "it = SleepyQueue(q)\n",
        "\n",
        "%time test_shuffled(L(DataLoader(it, num_workers=4)), range(30))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 14.2 ms, sys: 28.6 ms, total: 42.8 ms\n",
            "Wall time: 132 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVPx3qOE516M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class A(TensorBase): pass\n",
        "\n",
        "for nw in (0,2):\n",
        "    t = A(tensor([1,2]))\n",
        "    dl = DataLoader([t,t,t,t,t,t,t,t], bs=4, num_workers=nw)\n",
        "    b = first(dl)\n",
        "    test_eq(type(b), A)\n",
        "\n",
        "    t = (A(tensor([1,2])),)\n",
        "    dl = DataLoader([t,t,t,t,t,t,t,t], bs=4, num_workers=nw)\n",
        "    b = first(dl)\n",
        "    test_eq(type(b[0]), A)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HNSMLYI516P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class A(TensorBase): pass\n",
        "t = A(tensor(1,2))\n",
        "\n",
        "tdl = DataLoader([t,t,t,t,t,t,t,t], bs=4, num_workers=2, after_batch=to_device)\n",
        "b = first(tdl)\n",
        "test_eq(type(b), A)\n",
        "\n",
        "# Unknown attributes are delegated to `dataset`\n",
        "test_eq(tdl.pop(), tensor(1,2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCVtwIpe516T",
        "colab_type": "text"
      },
      "source": [
        "## Export -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgzJRyxk516T",
        "colab_type": "code",
        "colab": {},
        "outputId": "38ecdbb3-e91f-453a-f665-f25a1559bbc8"
      },
      "source": [
        "#hide\n",
        "from nbdev.export import notebook2script\n",
        "notebook2script()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converted 00_torch_core.ipynb.\n",
            "Converted 01_layers.ipynb.\n",
            "Converted 02_data.load.ipynb.\n",
            "Converted 03_data.core.ipynb.\n",
            "Converted 04_data.external.ipynb.\n",
            "Converted 05_data.transforms.ipynb.\n",
            "Converted 06_data.block.ipynb.\n",
            "Converted 07_vision.core.ipynb.\n",
            "Converted 08_vision.data.ipynb.\n",
            "Converted 09_vision.augment.ipynb.\n",
            "Converted 09b_vision.utils.ipynb.\n",
            "Converted 09c_vision.widgets.ipynb.\n",
            "Converted 10_tutorial.pets.ipynb.\n",
            "Converted 11_vision.models.xresnet.ipynb.\n",
            "Converted 12_optimizer.ipynb.\n",
            "Converted 13_callback.core.ipynb.\n",
            "Converted 13a_learner.ipynb.\n",
            "Converted 13b_metrics.ipynb.\n",
            "Converted 14_callback.schedule.ipynb.\n",
            "Converted 14a_callback.data.ipynb.\n",
            "Converted 15_callback.hook.ipynb.\n",
            "Converted 15a_vision.models.unet.ipynb.\n",
            "Converted 16_callback.progress.ipynb.\n",
            "Converted 17_callback.tracker.ipynb.\n",
            "Converted 18_callback.fp16.ipynb.\n",
            "Converted 18a_callback.training.ipynb.\n",
            "Converted 19_callback.mixup.ipynb.\n",
            "Converted 20_interpret.ipynb.\n",
            "Converted 20a_distributed.ipynb.\n",
            "Converted 21_vision.learner.ipynb.\n",
            "Converted 22_tutorial.imagenette.ipynb.\n",
            "Converted 23_tutorial.vision.ipynb.\n",
            "Converted 24_tutorial.siamese.ipynb.\n",
            "Converted 24_vision.gan.ipynb.\n",
            "Converted 30_text.core.ipynb.\n",
            "Converted 31_text.data.ipynb.\n",
            "Converted 32_text.models.awdlstm.ipynb.\n",
            "Converted 33_text.models.core.ipynb.\n",
            "Converted 34_callback.rnn.ipynb.\n",
            "Converted 35_tutorial.wikitext.ipynb.\n",
            "Converted 36_text.models.qrnn.ipynb.\n",
            "Converted 37_text.learner.ipynb.\n",
            "Converted 38_tutorial.text.ipynb.\n",
            "Converted 39_tutorial.transformers.ipynb.\n",
            "Converted 40_tabular.core.ipynb.\n",
            "Converted 41_tabular.data.ipynb.\n",
            "Converted 42_tabular.model.ipynb.\n",
            "Converted 43_tabular.learner.ipynb.\n",
            "Converted 44_tutorial.tabular.ipynb.\n",
            "Converted 45_collab.ipynb.\n",
            "Converted 46_tutorial.collab.ipynb.\n",
            "Converted 50_tutorial.datablock.ipynb.\n",
            "Converted 60_medical.imaging.ipynb.\n",
            "Converted 61_tutorial.medical_imaging.ipynb.\n",
            "Converted 65_medical.text.ipynb.\n",
            "Converted 70_callback.wandb.ipynb.\n",
            "Converted 71_callback.tensorboard.ipynb.\n",
            "Converted 72_callback.neptune.ipynb.\n",
            "Converted 73_callback.captum.ipynb.\n",
            "Converted 74_callback.cutmix.ipynb.\n",
            "Converted 97_test_utils.ipynb.\n",
            "Converted 99_pytorch_doc.ipynb.\n",
            "Converted index.ipynb.\n",
            "Converted tutorial.ipynb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMG1_yVd516b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}